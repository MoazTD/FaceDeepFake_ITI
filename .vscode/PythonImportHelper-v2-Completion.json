[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "checkpoint_sequential",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "PReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "AvgPool1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ConvTranspose1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "AvgPool1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ConvTranspose1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "AvgPool1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ConvTranspose1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "mxnet",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mxnet",
        "description": "mxnet",
        "detail": "mxnet",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "mxnet",
        "description": "mxnet",
        "isExtraImport": true,
        "detail": "mxnet",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "sklearn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn",
        "description": "sklearn",
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "scipy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy",
        "description": "scipy",
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "signal",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "signal",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "PCA",
        "importPath": "sklearn.decomposition",
        "description": "sklearn.decomposition",
        "isExtraImport": true,
        "detail": "sklearn.decomposition",
        "documentation": {}
    },
    {
        "label": "KFold",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "cpu_count",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Manager",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Process",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "cpu_count",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Manager",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Queue",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Process",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "cpu_count",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "freeze_support",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "sample_colours_from_colourmap",
        "importPath": "menpo.visualize.viewmatplotlib",
        "description": "menpo.visualize.viewmatplotlib",
        "isExtraImport": true,
        "detail": "menpo.visualize.viewmatplotlib",
        "documentation": {}
    },
    {
        "label": "sample_colours_from_colourmap",
        "importPath": "menpo.visualize.viewmatplotlib",
        "description": "menpo.visualize.viewmatplotlib",
        "isExtraImport": true,
        "detail": "menpo.visualize.viewmatplotlib",
        "documentation": {}
    },
    {
        "label": "prettytable",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "prettytable",
        "description": "prettytable",
        "detail": "prettytable",
        "documentation": {}
    },
    {
        "label": "PrettyTable",
        "importPath": "prettytable",
        "description": "prettytable",
        "isExtraImport": true,
        "detail": "prettytable",
        "documentation": {}
    },
    {
        "label": "PrettyTable",
        "importPath": "prettytable",
        "description": "prettytable",
        "isExtraImport": true,
        "detail": "prettytable",
        "documentation": {}
    },
    {
        "label": "roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "auc",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "auc",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "verification",
        "importPath": "eval",
        "description": "eval",
        "isExtraImport": true,
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "importPath": "utils.utils_logging",
        "description": "utils.utils_logging",
        "isExtraImport": true,
        "detail": "utils.utils_logging",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "importPath": "utils.utils_logging",
        "description": "utils.utils_logging",
        "isExtraImport": true,
        "detail": "utils.utils_logging",
        "documentation": {}
    },
    {
        "label": "init_logging",
        "importPath": "utils.utils_logging",
        "description": "utils.utils_logging",
        "isExtraImport": true,
        "detail": "utils.utils_logging",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "join",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "ceil",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "cos",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sin",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "atan2",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "asin",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "randint",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "numbers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numbers",
        "description": "numbers",
        "detail": "numbers",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "queue",
        "description": "queue",
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "ImageFolder",
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "isExtraImport": true,
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "utils.utils_distributed_sampler",
        "description": "utils.utils_distributed_sampler",
        "isExtraImport": true,
        "detail": "utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "get_dist_info",
        "importPath": "utils.utils_distributed_sampler",
        "description": "utils.utils_distributed_sampler",
        "isExtraImport": true,
        "detail": "utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "worker_init_fn",
        "importPath": "utils.utils_distributed_sampler",
        "description": "utils.utils_distributed_sampler",
        "isExtraImport": true,
        "detail": "utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "setup_seed",
        "importPath": "utils.utils_distributed_sampler",
        "description": "utils.utils_distributed_sampler",
        "isExtraImport": true,
        "detail": "utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "timeit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timeit",
        "description": "timeit",
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "skimage",
        "description": "skimage",
        "isExtraImport": true,
        "detail": "skimage",
        "documentation": {}
    },
    {
        "label": "io",
        "importPath": "skimage",
        "description": "skimage",
        "isExtraImport": true,
        "detail": "skimage",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "backbones",
        "description": "backbones",
        "isExtraImport": true,
        "detail": "backbones",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "backbones",
        "description": "backbones",
        "isExtraImport": true,
        "detail": "backbones",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "backbones",
        "description": "backbones",
        "isExtraImport": true,
        "detail": "backbones",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "backbones",
        "description": "backbones",
        "isExtraImport": true,
        "detail": "backbones",
        "documentation": {}
    },
    {
        "label": "pathlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pathlib",
        "description": "pathlib",
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "get_model_complexity_info",
        "importPath": "ptflops",
        "description": "ptflops",
        "isExtraImport": true,
        "detail": "ptflops",
        "documentation": {}
    },
    {
        "label": "_LRScheduler",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "SGD",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "lr_scheduler",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxruntime",
        "description": "onnxruntime",
        "detail": "onnxruntime",
        "documentation": {}
    },
    {
        "label": "onnx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnx",
        "description": "onnx",
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "numpy_helper",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "get_image",
        "importPath": "insightface.data",
        "description": "insightface.data",
        "isExtraImport": true,
        "detail": "insightface.data",
        "documentation": {}
    },
    {
        "label": "skimage.transform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "skimage.transform",
        "description": "skimage.transform",
        "detail": "skimage.transform",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "ArcFaceORT",
        "importPath": "onnx_helper",
        "description": "onnx_helper",
        "isExtraImport": true,
        "detail": "onnx_helper",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "linear",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "conv1d",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "conv2d",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "CombinedMarginLoss",
        "importPath": "losses",
        "description": "losses",
        "isExtraImport": true,
        "detail": "losses",
        "documentation": {}
    },
    {
        "label": "PolynomialLRWarmup",
        "importPath": "lr_scheduler",
        "description": "lr_scheduler",
        "isExtraImport": true,
        "detail": "lr_scheduler",
        "documentation": {}
    },
    {
        "label": "PartialFC_V2",
        "importPath": "partial_fc_v2",
        "description": "partial_fc_v2",
        "isExtraImport": true,
        "detail": "partial_fc_v2",
        "documentation": {}
    },
    {
        "label": "CallBackLogging",
        "importPath": "utils.utils_callbacks",
        "description": "utils.utils_callbacks",
        "isExtraImport": true,
        "detail": "utils.utils_callbacks",
        "documentation": {}
    },
    {
        "label": "CallBackVerification",
        "importPath": "utils.utils_callbacks",
        "description": "utils.utils_callbacks",
        "isExtraImport": true,
        "detail": "utils.utils_callbacks",
        "documentation": {}
    },
    {
        "label": "get_config",
        "importPath": "utils.utils_config",
        "description": "utils.utils_config",
        "isExtraImport": true,
        "detail": "utils.utils_config",
        "documentation": {}
    },
    {
        "label": "fp16_compress_hook",
        "importPath": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "description": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "isExtraImport": true,
        "detail": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "scipy.io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.io",
        "description": "scipy.io",
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "loadmat",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "savemat",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "loadmat",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "transferBFM09",
        "importPath": "util.load_mats",
        "description": "util.load_mats",
        "isExtraImport": true,
        "detail": "util.load_mats",
        "documentation": {}
    },
    {
        "label": "util",
        "importPath": "util",
        "description": "util",
        "isExtraImport": true,
        "detail": "util",
        "documentation": {}
    },
    {
        "label": "MeshRenderer",
        "importPath": "util.nvdiffrast",
        "description": "util.nvdiffrast",
        "isExtraImport": true,
        "detail": "util.nvdiffrast",
        "documentation": {}
    },
    {
        "label": "estimate_norm_torch",
        "importPath": "util.preprocess",
        "description": "util.preprocess",
        "isExtraImport": true,
        "detail": "util.preprocess",
        "documentation": {}
    },
    {
        "label": "trimesh",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "trimesh",
        "description": "trimesh",
        "detail": "trimesh",
        "documentation": {}
    },
    {
        "label": "warp_affine",
        "importPath": "kornia.geometry",
        "description": "kornia.geometry",
        "isExtraImport": true,
        "detail": "kornia.geometry",
        "documentation": {}
    },
    {
        "label": "warp_affine",
        "importPath": "kornia.geometry",
        "description": "kornia.geometry",
        "isExtraImport": true,
        "detail": "kornia.geometry",
        "documentation": {}
    },
    {
        "label": "from core",
        "importPath": "core3d.models import networks  # Use absolute",
        "description": "core3d.models import networks  # Use absolute",
        "isExtraImport": true,
        "detail": "core3d.models import networks  # Use absolute",
        "documentation": {}
    },
    {
        "label": "insightface",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "insightface",
        "description": "insightface",
        "detail": "insightface",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "importPath": "DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "DDFA_V2.FaceBoxes.FaceBoxes",
        "isExtraImport": true,
        "detail": "DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "importPath": "DDFA_V2.TDDFA",
        "description": "DDFA_V2.TDDFA",
        "isExtraImport": true,
        "detail": "DDFA_V2.TDDFA",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "DDFA_V2.utils.render",
        "description": "DDFA_V2.utils.render",
        "isExtraImport": true,
        "detail": "DDFA_V2.utils.render",
        "documentation": {}
    },
    {
        "label": "depth",
        "importPath": "DDFA_V2.utils.depth",
        "description": "DDFA_V2.utils.depth",
        "isExtraImport": true,
        "detail": "DDFA_V2.utils.depth",
        "documentation": {}
    },
    {
        "label": "pncc",
        "importPath": "DDFA_V2.utils.pncc",
        "description": "DDFA_V2.utils.pncc",
        "isExtraImport": true,
        "detail": "DDFA_V2.utils.pncc",
        "documentation": {}
    },
    {
        "label": "ser_to_obj",
        "importPath": "DDFA_V2.utils.serialization",
        "description": "DDFA_V2.utils.serialization",
        "isExtraImport": true,
        "detail": "DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageDraw",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageDraw",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "_load",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_load",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_numpy_to_cuda",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_numpy_to_tensor",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_load",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_dump",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_load",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_load",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "_load",
        "importPath": "utils.io",
        "description": "utils.io",
        "isExtraImport": true,
        "detail": "utils.io",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "Extension",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "Extension",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "cythonize",
        "importPath": "Cython.Build",
        "description": "Cython.Build",
        "isExtraImport": true,
        "detail": "Cython.Build",
        "documentation": {}
    },
    {
        "label": "cythonize",
        "importPath": "Cython.Build",
        "description": "Cython.Build",
        "isExtraImport": true,
        "detail": "Cython.Build",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "distutils.core",
        "description": "distutils.core",
        "isExtraImport": true,
        "detail": "distutils.core",
        "documentation": {}
    },
    {
        "label": "Extension",
        "importPath": "distutils.extension",
        "description": "distutils.extension",
        "isExtraImport": true,
        "detail": "distutils.extension",
        "documentation": {}
    },
    {
        "label": "build_ext",
        "importPath": "Cython.Distutils",
        "description": "Cython.Distutils",
        "isExtraImport": true,
        "detail": "Cython.Distutils",
        "documentation": {}
    },
    {
        "label": "build_ext",
        "importPath": "Cython.Distutils",
        "description": "Cython.Distutils",
        "isExtraImport": true,
        "detail": "Cython.Distutils",
        "documentation": {}
    },
    {
        "label": "cpu_nms",
        "importPath": "FaceBoxes.utils.nms.cpu_nms",
        "description": "FaceBoxes.utils.nms.cpu_nms",
        "isExtraImport": true,
        "detail": "FaceBoxes.utils.nms.cpu_nms",
        "documentation": {}
    },
    {
        "label": "cpu_soft_nms",
        "importPath": "FaceBoxes.utils.nms.cpu_nms",
        "description": "FaceBoxes.utils.nms.cpu_nms",
        "isExtraImport": true,
        "detail": "FaceBoxes.utils.nms.cpu_nms",
        "documentation": {}
    },
    {
        "label": "product",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "Sim3DR_Cython",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "Sim3DR_Cython",
        "description": "Sim3DR_Cython",
        "detail": "Sim3DR_Cython",
        "documentation": {}
    },
    {
        "label": "rasterize",
        "importPath": "Sim3DR",
        "description": "Sim3DR",
        "isExtraImport": true,
        "detail": "Sim3DR",
        "documentation": {}
    },
    {
        "label": "rasterize",
        "importPath": "Sim3DR",
        "description": "Sim3DR",
        "isExtraImport": true,
        "detail": "Sim3DR",
        "documentation": {}
    },
    {
        "label": "RenderPipeline",
        "importPath": "Sim3DR",
        "description": "Sim3DR",
        "isExtraImport": true,
        "detail": "Sim3DR",
        "documentation": {}
    },
    {
        "label": "rasterize",
        "importPath": "Sim3DR",
        "description": "Sim3DR",
        "isExtraImport": true,
        "detail": "Sim3DR",
        "documentation": {}
    },
    {
        "label": "RenderPipeline",
        "importPath": "Sim3DR",
        "description": "Sim3DR",
        "isExtraImport": true,
        "detail": "Sim3DR",
        "documentation": {}
    },
    {
        "label": "plot_image",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "plot_image",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "plot_image",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "plot_image",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "plot_image",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "draw_landmarks",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "get_suffix",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "cv_draw_landmark",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "get_suffix",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "cv_draw_landmark",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "get_suffix",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "cv_draw_landmark",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "draw_landmarks",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "get_suffix",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "crop_img",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "parse_roi_box_from_bbox",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "parse_roi_box_from_landmark",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "crop_img",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "parse_roi_box_from_bbox",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "parse_roi_box_from_landmark",
        "importPath": "utils.functions",
        "description": "utils.functions",
        "isExtraImport": true,
        "detail": "utils.functions",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "models",
        "description": "models",
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "_to_ctype",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "_to_ctype",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "_parse_param",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "similar_transform",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "ToTensorGjz",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "NormalizeGjz",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "_parse_param",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "similar_transform",
        "importPath": "utils.tddfa_util",
        "description": "utils.tddfa_util",
        "isExtraImport": true,
        "detail": "utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "ctypes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ctypes",
        "description": "ctypes",
        "detail": "ctypes",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "importPath": "FaceBoxes",
        "description": "FaceBoxes",
        "isExtraImport": true,
        "detail": "FaceBoxes",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "importPath": "FaceBoxes",
        "description": "FaceBoxes",
        "isExtraImport": true,
        "detail": "FaceBoxes",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "importPath": "FaceBoxes",
        "description": "FaceBoxes",
        "isExtraImport": true,
        "detail": "FaceBoxes",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "importPath": "FaceBoxes",
        "description": "FaceBoxes",
        "isExtraImport": true,
        "detail": "FaceBoxes",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "importPath": "FaceBoxes",
        "description": "FaceBoxes",
        "isExtraImport": true,
        "detail": "FaceBoxes",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "importPath": "FaceBoxes",
        "description": "FaceBoxes",
        "isExtraImport": true,
        "detail": "FaceBoxes",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "importPath": "TDDFA",
        "description": "TDDFA",
        "isExtraImport": true,
        "detail": "TDDFA",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "importPath": "TDDFA",
        "description": "TDDFA",
        "isExtraImport": true,
        "detail": "TDDFA",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "importPath": "TDDFA",
        "description": "TDDFA",
        "isExtraImport": true,
        "detail": "TDDFA",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "importPath": "TDDFA",
        "description": "TDDFA",
        "isExtraImport": true,
        "detail": "TDDFA",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "importPath": "TDDFA",
        "description": "TDDFA",
        "isExtraImport": true,
        "detail": "TDDFA",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "importPath": "TDDFA",
        "description": "TDDFA",
        "isExtraImport": true,
        "detail": "TDDFA",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "utils.render",
        "description": "utils.render",
        "isExtraImport": true,
        "detail": "utils.render",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "utils.render",
        "description": "utils.render",
        "isExtraImport": true,
        "detail": "utils.render",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "utils.render",
        "description": "utils.render",
        "isExtraImport": true,
        "detail": "utils.render",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "utils.render",
        "description": "utils.render",
        "isExtraImport": true,
        "detail": "utils.render",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "utils.render",
        "description": "utils.render",
        "isExtraImport": true,
        "detail": "utils.render",
        "documentation": {}
    },
    {
        "label": "depth",
        "importPath": "utils.depth",
        "description": "utils.depth",
        "isExtraImport": true,
        "detail": "utils.depth",
        "documentation": {}
    },
    {
        "label": "depth",
        "importPath": "utils.depth",
        "description": "utils.depth",
        "isExtraImport": true,
        "detail": "utils.depth",
        "documentation": {}
    },
    {
        "label": "pncc",
        "importPath": "utils.pncc",
        "description": "utils.pncc",
        "isExtraImport": true,
        "detail": "utils.pncc",
        "documentation": {}
    },
    {
        "label": "pncc",
        "importPath": "utils.pncc",
        "description": "utils.pncc",
        "isExtraImport": true,
        "detail": "utils.pncc",
        "documentation": {}
    },
    {
        "label": "uv_tex",
        "importPath": "utils.uv",
        "description": "utils.uv",
        "isExtraImport": true,
        "detail": "utils.uv",
        "documentation": {}
    },
    {
        "label": "uv_tex",
        "importPath": "utils.uv",
        "description": "utils.uv",
        "isExtraImport": true,
        "detail": "utils.uv",
        "documentation": {}
    },
    {
        "label": "viz_pose",
        "importPath": "utils.pose",
        "description": "utils.pose",
        "isExtraImport": true,
        "detail": "utils.pose",
        "documentation": {}
    },
    {
        "label": "viz_pose",
        "importPath": "utils.pose",
        "description": "utils.pose",
        "isExtraImport": true,
        "detail": "utils.pose",
        "documentation": {}
    },
    {
        "label": "ser_to_ply",
        "importPath": "utils.serialization",
        "description": "utils.serialization",
        "isExtraImport": true,
        "detail": "utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_obj",
        "importPath": "utils.serialization",
        "description": "utils.serialization",
        "isExtraImport": true,
        "detail": "utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_ply",
        "importPath": "utils.serialization",
        "description": "utils.serialization",
        "isExtraImport": true,
        "detail": "utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_obj",
        "importPath": "utils.serialization",
        "description": "utils.serialization",
        "isExtraImport": true,
        "detail": "utils.serialization",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "utils.render_ctypes",
        "description": "utils.render_ctypes",
        "isExtraImport": true,
        "detail": "utils.render_ctypes",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Popen",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "gradio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gradio",
        "description": "gradio",
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "FaceBoxes.utils.timer",
        "description": "FaceBoxes.utils.timer",
        "isExtraImport": true,
        "detail": "FaceBoxes.utils.timer",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.backends.cudnn",
        "description": "torch.backends.cudnn",
        "detail": "torch.backends.cudnn",
        "documentation": {}
    },
    {
        "label": "BFMModel",
        "importPath": "bfm",
        "description": "bfm",
        "isExtraImport": true,
        "detail": "bfm",
        "documentation": {}
    },
    {
        "label": "convert_to_onnx",
        "importPath": "utils.onnx",
        "description": "utils.onnx",
        "isExtraImport": true,
        "detail": "utils.onnx",
        "documentation": {}
    },
    {
        "label": "BFMModel",
        "importPath": "bfm.bfm",
        "description": "bfm.bfm",
        "isExtraImport": true,
        "detail": "bfm.bfm",
        "documentation": {}
    },
    {
        "label": "convert_bfm_to_onnx",
        "importPath": "bfm.bfm_onnx",
        "description": "bfm.bfm_onnx",
        "isExtraImport": true,
        "detail": "bfm.bfm_onnx",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "zipfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zipfile",
        "description": "zipfile",
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSplitter",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QScrollArea",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QRadioButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGridLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QButtonGroup",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialogButtonBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QRadioButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGraphicsView",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGraphicsScene",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialogButtonBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGridLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QRadioButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QStackedWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTabWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFormLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGridLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSplitter",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizeGrip",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMenuBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMenu",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QScrollArea",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QStyleOptionButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGraphicsView",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGraphicsScene",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialogButtonBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QRadioButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QStackedWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTabWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFormLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGridLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSplitter",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizeGrip",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMenuBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMenu",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QScrollArea",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QStyleOptionButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QListWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGridLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDoubleSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTabWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTableWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTableWidgetItem",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHeaderView",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSplitter",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QScrollArea",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialogButtonBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFormLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSplashScreen",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QButtonGroup",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QRadioButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QStackedWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTabWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFormLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGridLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSplitter",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizeGrip",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMenuBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMenu",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QScrollArea",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QStyleOptionButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QRadioButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRect",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThreadPool",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRunnable",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Slot",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutex",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QWaitCondition",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEvent",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPropertyAnimation",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEasingCurve",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QUrl",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutex",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutexLocker",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QCoreApplication",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDate",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDateTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QLocale",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMetaObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPoint",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRect",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEvent",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPropertyAnimation",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEasingCurve",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QUrl",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutex",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutexLocker",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QCoreApplication",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDate",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDateTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QLocale",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMetaObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPoint",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRect",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPropertyAnimation",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEasingCurve",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRect",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QUrl",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEvent",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPropertyAnimation",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEasingCurve",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QUrl",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutex",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutexLocker",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QCoreApplication",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDate",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDateTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QLocale",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMetaObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPoint",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRect",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEvent",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Signal",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPropertyAnimation",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QEasingCurve",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QUrl",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutex",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMutexLocker",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QCoreApplication",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDate",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QDateTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QLocale",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QMetaObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPoint",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRect",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTime",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QCoreApplication",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPainter",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPen",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPen",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPainter",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QAction",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QConicalGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QCursor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFontDatabase",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QKeySequence",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QLinearGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QRadialGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QTransform",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPen",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPainter",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QAction",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QConicalGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QCursor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFontDatabase",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QKeySequence",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QLinearGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QRadialGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QTransform",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPainter",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QLinearGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QDragEnterEvent",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QDropEvent",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QMovie",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPainter",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QAction",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QConicalGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QCursor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFontDatabase",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QKeySequence",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QLinearGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QRadialGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QTransform",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QAction",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QConicalGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QCursor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QFontDatabase",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QKeySequence",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QLinearGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QRadialGradient",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QTransform",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "ui",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ui",
        "description": "ui",
        "detail": "ui",
        "documentation": {}
    },
    {
        "label": "enhancement",
        "importPath": "ui",
        "description": "ui",
        "isExtraImport": true,
        "detail": "ui",
        "documentation": {}
    },
    {
        "label": "apply_theme",
        "importPath": "ui.themes",
        "description": "ui.themes",
        "isExtraImport": true,
        "detail": "ui.themes",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "FaceDetector",
        "importPath": "core.face_detector",
        "description": "core.face_detector",
        "isExtraImport": true,
        "detail": "core.face_detector",
        "documentation": {}
    },
    {
        "label": "FaceDetector",
        "importPath": "core.face_detector",
        "description": "core.face_detector",
        "isExtraImport": true,
        "detail": "core.face_detector",
        "documentation": {}
    },
    {
        "label": "FaceDetector",
        "importPath": "core.face_detector",
        "description": "core.face_detector",
        "isExtraImport": true,
        "detail": "core.face_detector",
        "documentation": {}
    },
    {
        "label": "FaceDetector",
        "importPath": "core.face_detector",
        "description": "core.face_detector",
        "isExtraImport": true,
        "detail": "core.face_detector",
        "documentation": {}
    },
    {
        "label": "FaceSwapperModel",
        "importPath": "core.face_swapper",
        "description": "core.face_swapper",
        "isExtraImport": true,
        "detail": "core.face_swapper",
        "documentation": {}
    },
    {
        "label": "video_utils",
        "importPath": "core",
        "description": "core",
        "isExtraImport": true,
        "detail": "core",
        "documentation": {}
    },
    {
        "label": "processors",
        "importPath": "core",
        "description": "core",
        "isExtraImport": true,
        "detail": "core",
        "documentation": {}
    },
    {
        "label": "video_utils",
        "importPath": "core",
        "description": "core",
        "isExtraImport": true,
        "detail": "core",
        "documentation": {}
    },
    {
        "label": "processors",
        "importPath": "core",
        "description": "core",
        "isExtraImport": true,
        "detail": "core",
        "documentation": {}
    },
    {
        "label": "FaceAnalysis",
        "importPath": "insightface.app",
        "description": "insightface.app",
        "isExtraImport": true,
        "detail": "insightface.app",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "FaceSwapController",
        "importPath": "core.controller",
        "description": "core.controller",
        "isExtraImport": true,
        "detail": "core.controller",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "locale",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "locale",
        "description": "locale",
        "detail": "locale",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "pyworld",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyworld",
        "description": "pyworld",
        "detail": "pyworld",
        "documentation": {}
    },
    {
        "label": "F0Predictor",
        "importPath": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "description": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "documentation": {}
    },
    {
        "label": "F0Predictor",
        "importPath": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "description": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "documentation": {}
    },
    {
        "label": "F0Predictor",
        "importPath": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "description": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "documentation": {}
    },
    {
        "label": "parselmouth",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "parselmouth",
        "description": "parselmouth",
        "detail": "parselmouth",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "commons",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "modules",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "commons",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "modules",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "attentions",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "commons",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "modules",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "commons",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "modules",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "commons",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "commons",
        "importPath": "infer.lib.infer_pack",
        "description": "infer.lib.infer_pack",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "importPath": "infer.lib.infer_pack.modules",
        "description": "infer.lib.infer_pack.modules",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "importPath": "infer.lib.infer_pack.modules",
        "description": "infer.lib.infer_pack.modules",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "remove_weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "spectral_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "remove_weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "spectral_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "remove_weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "importPath": "infer.lib.infer_pack.commons",
        "description": "infer.lib.infer_pack.commons",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "infer.lib.infer_pack.commons",
        "description": "infer.lib.infer_pack.commons",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "importPath": "infer.lib.infer_pack.commons",
        "description": "infer.lib.infer_pack.commons",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "infer.lib.infer_pack.commons",
        "description": "infer.lib.infer_pack.commons",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "importPath": "infer.lib.infer_pack.commons",
        "description": "infer.lib.infer_pack.commons",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "infer.lib.infer_pack.commons",
        "description": "infer.lib.infer_pack.commons",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "infer.lib.infer_pack.attentions_onnx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "infer.lib.infer_pack.attentions_onnx",
        "description": "infer.lib.infer_pack.attentions_onnx",
        "detail": "infer.lib.infer_pack.attentions_onnx",
        "documentation": {}
    },
    {
        "label": "piecewise_rational_quadratic_transform",
        "importPath": "infer.lib.infer_pack.transforms",
        "description": "infer.lib.infer_pack.transforms",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "librosa",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa",
        "description": "librosa",
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "load_model_ensemble_and_task",
        "importPath": "fairseq.checkpoint_utils",
        "description": "fairseq.checkpoint_utils",
        "isExtraImport": true,
        "detail": "fairseq.checkpoint_utils",
        "documentation": {}
    },
    {
        "label": "index_put",
        "importPath": "fairseq.utils",
        "description": "fairseq.utils",
        "isExtraImport": true,
        "detail": "fairseq.utils",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "spectrogram_torch",
        "importPath": "infer.lib.train.mel_processing",
        "description": "infer.lib.train.mel_processing",
        "isExtraImport": true,
        "detail": "infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "mel_spectrogram_torch",
        "importPath": "infer.lib.train.mel_processing",
        "description": "infer.lib.train.mel_processing",
        "isExtraImport": true,
        "detail": "infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "spec_to_mel_torch",
        "importPath": "infer.lib.train.mel_processing",
        "description": "infer.lib.train.mel_processing",
        "isExtraImport": true,
        "detail": "infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "load_filepaths_and_text",
        "importPath": "infer.lib.train.utils",
        "description": "infer.lib.train.utils",
        "isExtraImport": true,
        "detail": "infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "load_wav_to_torch",
        "importPath": "infer.lib.train.utils",
        "description": "infer.lib.train.utils",
        "isExtraImport": true,
        "detail": "infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "mel",
        "importPath": "librosa.filters",
        "description": "librosa.filters",
        "isExtraImport": true,
        "detail": "librosa.filters",
        "documentation": {}
    },
    {
        "label": "mel",
        "importPath": "librosa.filters",
        "description": "librosa.filters",
        "isExtraImport": true,
        "detail": "librosa.filters",
        "documentation": {}
    },
    {
        "label": "I18nAuto",
        "importPath": "i18n.i18n",
        "description": "i18n.i18n",
        "isExtraImport": true,
        "detail": "i18n.i18n",
        "documentation": {}
    },
    {
        "label": "I18nAuto",
        "importPath": "i18n.i18n",
        "description": "i18n.i18n",
        "isExtraImport": true,
        "detail": "i18n.i18n",
        "documentation": {}
    },
    {
        "label": "I18nAuto",
        "importPath": "i18n.i18n",
        "description": "i18n.i18n",
        "isExtraImport": true,
        "detail": "i18n.i18n",
        "documentation": {}
    },
    {
        "label": "read",
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "isExtraImport": true,
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "layers",
        "description": "layers",
        "detail": "layers",
        "documentation": {}
    },
    {
        "label": "platform,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform.",
        "description": "platform.",
        "detail": "platform.",
        "documentation": {}
    },
    {
        "label": "ffmpeg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ffmpeg",
        "description": "ffmpeg",
        "detail": "ffmpeg",
        "documentation": {}
    },
    {
        "label": "av",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "av",
        "description": "av",
        "detail": "av",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "jit",
        "importPath": "infer.lib",
        "description": "infer.lib",
        "isExtraImport": true,
        "detail": "infer.lib",
        "documentation": {}
    },
    {
        "label": "jit",
        "importPath": "infer.lib",
        "description": "infer.lib",
        "isExtraImport": true,
        "detail": "infer.lib",
        "documentation": {}
    },
    {
        "label": "jit",
        "importPath": "infer.lib",
        "description": "infer.lib",
        "isExtraImport": true,
        "detail": "infer.lib",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "librosa.util",
        "description": "librosa.util",
        "isExtraImport": true,
        "detail": "librosa.util",
        "documentation": {}
    },
    {
        "label": "pad_center",
        "importPath": "librosa.util",
        "description": "librosa.util",
        "isExtraImport": true,
        "detail": "librosa.util",
        "documentation": {}
    },
    {
        "label": "tiny",
        "importPath": "librosa.util",
        "description": "librosa.util",
        "isExtraImport": true,
        "detail": "librosa.util",
        "documentation": {}
    },
    {
        "label": "scipy.signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "get_window",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "get_synthesizer",
        "importPath": "infer.lib.jit.get_synthesizer",
        "description": "infer.lib.jit.get_synthesizer",
        "isExtraImport": true,
        "detail": "infer.lib.jit.get_synthesizer",
        "documentation": {}
    },
    {
        "label": "get_synthesizer",
        "importPath": "infer.lib.jit.get_synthesizer",
        "description": "infer.lib.jit.get_synthesizer",
        "isExtraImport": true,
        "detail": "infer.lib.jit.get_synthesizer",
        "documentation": {}
    },
    {
        "label": "fairseq",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fairseq",
        "description": "fairseq",
        "detail": "fairseq",
        "documentation": {}
    },
    {
        "label": "checkpoint_utils",
        "importPath": "fairseq",
        "description": "fairseq",
        "isExtraImport": true,
        "detail": "fairseq",
        "documentation": {}
    },
    {
        "label": "checkpoint_utils",
        "importPath": "fairseq",
        "description": "fairseq",
        "isExtraImport": true,
        "detail": "fairseq",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faiss",
        "description": "faiss",
        "detail": "faiss",
        "documentation": {}
    },
    {
        "label": "torchcrepe",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchcrepe",
        "description": "torchcrepe",
        "detail": "torchcrepe",
        "documentation": {}
    },
    {
        "label": "torchaudio.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchaudio.transforms",
        "description": "torchaudio.transforms",
        "detail": "torchaudio.transforms",
        "documentation": {}
    },
    {
        "label": "Resample",
        "importPath": "torchaudio.transforms",
        "description": "torchaudio.transforms",
        "isExtraImport": true,
        "detail": "torchaudio.transforms",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "configs.config",
        "description": "configs.config",
        "isExtraImport": true,
        "detail": "configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "configs.config",
        "description": "configs.config",
        "isExtraImport": true,
        "detail": "configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "configs.config",
        "description": "configs.config",
        "isExtraImport": true,
        "detail": "configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "configs.config",
        "description": "configs.config",
        "isExtraImport": true,
        "detail": "configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "configs.config",
        "description": "configs.config",
        "isExtraImport": true,
        "detail": "configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "configs.config",
        "description": "configs.config",
        "isExtraImport": true,
        "detail": "configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "configs.config",
        "description": "configs.config",
        "isExtraImport": true,
        "detail": "configs.config",
        "documentation": {}
    },
    {
        "label": "intel_extension_for_pytorch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "intel_extension_for_pytorch",
        "description": "intel_extension_for_pytorch",
        "detail": "intel_extension_for_pytorch",
        "documentation": {}
    },
    {
        "label": "intel_extension_for_pytorch._C",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "intel_extension_for_pytorch._C",
        "description": "intel_extension_for_pytorch._C",
        "detail": "intel_extension_for_pytorch._C",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "onnxsim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxsim",
        "description": "onnxsim",
        "detail": "onnxsim",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMsNSFsidM",
        "importPath": "infer.lib.infer_pack.models_onnx",
        "description": "infer.lib.infer_pack.models_onnx",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMsNSFsidM",
        "importPath": "infer.lib.infer_pack.models_onnx",
        "description": "infer.lib.infer_pack.models_onnx",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "importPath": "infer.lib.audio",
        "description": "infer.lib.audio",
        "isExtraImport": true,
        "detail": "infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "importPath": "infer.lib.audio",
        "description": "infer.lib.audio",
        "isExtraImport": true,
        "detail": "infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "importPath": "infer.lib.audio",
        "description": "infer.lib.audio",
        "isExtraImport": true,
        "detail": "infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "importPath": "infer.lib.audio",
        "description": "infer.lib.audio",
        "isExtraImport": true,
        "detail": "infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "importPath": "infer.lib.audio",
        "description": "infer.lib.audio",
        "isExtraImport": true,
        "detail": "infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "wav2",
        "importPath": "infer.lib.audio",
        "description": "infer.lib.audio",
        "isExtraImport": true,
        "detail": "infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "torch_directml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch_directml",
        "description": "torch_directml",
        "detail": "torch_directml",
        "documentation": {}
    },
    {
        "label": "Slicer",
        "importPath": "infer.lib.slicer2",
        "description": "infer.lib.slicer2",
        "isExtraImport": true,
        "detail": "infer.lib.slicer2",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "infer.lib.train",
        "description": "infer.lib.train",
        "isExtraImport": true,
        "detail": "infer.lib.train",
        "documentation": {}
    },
    {
        "label": "torch.multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.multiprocessing",
        "description": "torch.multiprocessing",
        "detail": "torch.multiprocessing",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "DistributedBucketSampler",
        "importPath": "infer.lib.train.data_utils",
        "description": "infer.lib.train.data_utils",
        "isExtraImport": true,
        "detail": "infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "TextAudioCollate",
        "importPath": "infer.lib.train.data_utils",
        "description": "infer.lib.train.data_utils",
        "isExtraImport": true,
        "detail": "infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "TextAudioCollateMultiNSFsid",
        "importPath": "infer.lib.train.data_utils",
        "description": "infer.lib.train.data_utils",
        "isExtraImport": true,
        "detail": "infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "TextAudioLoader",
        "importPath": "infer.lib.train.data_utils",
        "description": "infer.lib.train.data_utils",
        "isExtraImport": true,
        "detail": "infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "TextAudioLoaderMultiNSFsid",
        "importPath": "infer.lib.train.data_utils",
        "description": "infer.lib.train.data_utils",
        "isExtraImport": true,
        "detail": "infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "discriminator_loss",
        "importPath": "infer.lib.train.losses",
        "description": "infer.lib.train.losses",
        "isExtraImport": true,
        "detail": "infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "feature_loss",
        "importPath": "infer.lib.train.losses",
        "description": "infer.lib.train.losses",
        "isExtraImport": true,
        "detail": "infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "generator_loss",
        "importPath": "infer.lib.train.losses",
        "description": "infer.lib.train.losses",
        "isExtraImport": true,
        "detail": "infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "kl_loss",
        "importPath": "infer.lib.train.losses",
        "description": "infer.lib.train.losses",
        "isExtraImport": true,
        "detail": "infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "savee",
        "importPath": "infer.lib.train.process_ckpt",
        "description": "infer.lib.train.process_ckpt",
        "isExtraImport": true,
        "detail": "infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "change_info",
        "importPath": "infer.lib.train.process_ckpt",
        "description": "infer.lib.train.process_ckpt",
        "isExtraImport": true,
        "detail": "infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "extract_small_model",
        "importPath": "infer.lib.train.process_ckpt",
        "description": "infer.lib.train.process_ckpt",
        "isExtraImport": true,
        "detail": "infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "merge",
        "importPath": "infer.lib.train.process_ckpt",
        "description": "infer.lib.train.process_ckpt",
        "isExtraImport": true,
        "detail": "infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "show_info",
        "importPath": "infer.lib.train.process_ckpt",
        "description": "infer.lib.train.process_ckpt",
        "isExtraImport": true,
        "detail": "infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "MDXNetDereverb",
        "importPath": "infer.modules.uvr5.mdxnet",
        "description": "infer.modules.uvr5.mdxnet",
        "isExtraImport": true,
        "detail": "infer.modules.uvr5.mdxnet",
        "documentation": {}
    },
    {
        "label": "AudioPre",
        "importPath": "infer.modules.uvr5.vr",
        "description": "infer.modules.uvr5.vr",
        "isExtraImport": true,
        "detail": "infer.modules.uvr5.vr",
        "documentation": {}
    },
    {
        "label": "AudioPreDeEcho",
        "importPath": "infer.modules.uvr5.vr",
        "description": "infer.modules.uvr5.vr",
        "isExtraImport": true,
        "detail": "infer.modules.uvr5.vr",
        "documentation": {}
    },
    {
        "label": "nets_61968KB",
        "importPath": "infer.lib.uvr5_pack.lib_v5",
        "description": "infer.lib.uvr5_pack.lib_v5",
        "isExtraImport": true,
        "detail": "infer.lib.uvr5_pack.lib_v5",
        "documentation": {}
    },
    {
        "label": "spec_utils",
        "importPath": "infer.lib.uvr5_pack.lib_v5",
        "description": "infer.lib.uvr5_pack.lib_v5",
        "isExtraImport": true,
        "detail": "infer.lib.uvr5_pack.lib_v5",
        "documentation": {}
    },
    {
        "label": "ModelParameters",
        "importPath": "infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "infer.lib.uvr5_pack.lib_v5.model_param_init",
        "isExtraImport": true,
        "detail": "infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "CascadedNet",
        "importPath": "infer.lib.uvr5_pack.lib_v5.nets_new",
        "description": "infer.lib.uvr5_pack.lib_v5.nets_new",
        "isExtraImport": true,
        "detail": "infer.lib.uvr5_pack.lib_v5.nets_new",
        "documentation": {}
    },
    {
        "label": "inference",
        "importPath": "infer.lib.uvr5_pack.utils",
        "description": "infer.lib.uvr5_pack.utils",
        "isExtraImport": true,
        "detail": "infer.lib.uvr5_pack.utils",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs256NSFsid",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs256NSFsid_nono",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs768NSFsid",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs768NSFsid_nono",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs256NSFsid",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs256NSFsid",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs256NSFsid_nono",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs768NSFsid",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs768NSFsid_nono",
        "importPath": "infer.lib.infer_pack.models",
        "description": "infer.lib.infer_pack.models",
        "isExtraImport": true,
        "detail": "infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "infer.modules.vc.pipeline",
        "description": "infer.modules.vc.pipeline",
        "isExtraImport": true,
        "detail": "infer.modules.vc.pipeline",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "infer.modules.vc.utils",
        "description": "infer.modules.vc.utils",
        "isExtraImport": true,
        "detail": "infer.modules.vc.utils",
        "documentation": {}
    },
    {
        "label": "MiniBatchKMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "MiniBatchKMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdb",
        "description": "pdb",
        "detail": "pdb",
        "documentation": {}
    },
    {
        "label": "STFT",
        "importPath": "infer.lib.rmvpe",
        "description": "infer.lib.rmvpe",
        "isExtraImport": true,
        "detail": "infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "Number",
        "importPath": "torch.types",
        "description": "torch.types",
        "isExtraImport": true,
        "detail": "torch.types",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "VC",
        "importPath": "infer.modules.vc.modules",
        "description": "infer.modules.vc.modules",
        "isExtraImport": true,
        "detail": "infer.modules.vc.modules",
        "documentation": {}
    },
    {
        "label": "VC",
        "importPath": "infer.modules.vc.modules",
        "description": "infer.modules.vc.modules",
        "isExtraImport": true,
        "detail": "infer.modules.vc.modules",
        "documentation": {}
    },
    {
        "label": "VC",
        "importPath": "infer.modules.vc.modules",
        "description": "infer.modules.vc.modules",
        "isExtraImport": true,
        "detail": "infer.modules.vc.modules",
        "documentation": {}
    },
    {
        "label": "VC",
        "importPath": "infer.modules.vc.modules",
        "description": "infer.modules.vc.modules",
        "isExtraImport": true,
        "detail": "infer.modules.vc.modules",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "sounddevice",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sounddevice",
        "description": "sounddevice",
        "detail": "sounddevice",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "uvicorn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uvicorn",
        "description": "uvicorn",
        "detail": "uvicorn",
        "documentation": {}
    },
    {
        "label": "uvr",
        "importPath": "infer.modules.uvr5.modules",
        "description": "infer.modules.uvr5.modules",
        "isExtraImport": true,
        "detail": "infer.modules.uvr5.modules",
        "documentation": {}
    },
    {
        "label": "torch,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.",
        "description": "torch.",
        "detail": "torch.",
        "documentation": {}
    },
    {
        "label": "euclidean",
        "importPath": "scipy.spatial.distance",
        "description": "scipy.spatial.distance",
        "isExtraImport": true,
        "detail": "scipy.spatial.distance",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "apply_ageing",
        "importPath": "core.apply_age",
        "description": "core.apply_age",
        "isExtraImport": true,
        "detail": "core.apply_age",
        "documentation": {}
    },
    {
        "label": "dlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dlib",
        "description": "dlib",
        "detail": "dlib",
        "documentation": {}
    },
    {
        "label": "QMediaPlayer",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QAudioOutput",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QMediaPlayer",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QAudioOutput",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QMediaPlayer",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QAudioOutput",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QMediaPlayer",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QAudioOutput",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QMediaPlayer",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QAudioOutput",
        "importPath": "PySide6.QtMultimedia",
        "description": "PySide6.QtMultimedia",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "QVideoWidget",
        "importPath": "PySide6.QtMultimediaWidgets",
        "description": "PySide6.QtMultimediaWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimediaWidgets",
        "documentation": {}
    },
    {
        "label": "QVideoWidget",
        "importPath": "PySide6.QtMultimediaWidgets",
        "description": "PySide6.QtMultimediaWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimediaWidgets",
        "documentation": {}
    },
    {
        "label": "QVideoWidget",
        "importPath": "PySide6.QtMultimediaWidgets",
        "description": "PySide6.QtMultimediaWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimediaWidgets",
        "documentation": {}
    },
    {
        "label": "QVideoWidget",
        "importPath": "PySide6.QtMultimediaWidgets",
        "description": "PySide6.QtMultimediaWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimediaWidgets",
        "documentation": {}
    },
    {
        "label": "QVideoWidget",
        "importPath": "PySide6.QtMultimediaWidgets",
        "description": "PySide6.QtMultimediaWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtMultimediaWidgets",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "VoiceConversionWorkflow",
        "importPath": "voice_handler",
        "description": "voice_handler",
        "isExtraImport": true,
        "detail": "voice_handler",
        "documentation": {}
    },
    {
        "label": "RVCConverter",
        "importPath": "voice_handler",
        "description": "voice_handler",
        "isExtraImport": true,
        "detail": "voice_handler",
        "documentation": {}
    },
    {
        "label": "QUiLoader",
        "importPath": "PySide6.QtUiTools",
        "description": "PySide6.QtUiTools",
        "isExtraImport": true,
        "detail": "PySide6.QtUiTools",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "uitls",
        "description": "uitls",
        "isExtraImport": true,
        "detail": "uitls",
        "documentation": {}
    },
    {
        "label": "FaceSwapProcessor",
        "importPath": "core.process",
        "description": "core.process",
        "isExtraImport": true,
        "detail": "core.process",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "ui.uitls",
        "description": "ui.uitls",
        "isExtraImport": true,
        "detail": "ui.uitls",
        "documentation": {}
    },
    {
        "label": "cmds",
        "importPath": "maya",
        "description": "maya",
        "isExtraImport": true,
        "detail": "maya",
        "documentation": {}
    },
    {
        "label": "IBasicBlock",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "class IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 groups=1, base_width=64, dilation=1):\n        super(IBasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05,)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "IResNet",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "class IResNet(nn.Module):\n    fc_scale = 7 * 7\n    def __init__(self,\n                 block, layers, dropout=0, num_features=512, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n        super(IResNet, self).__init__()\n        self.extra_gflops = 0.0\n        self.fp16 = fp16\n        self.inplanes = 64\n        self.dilation = 1",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,\n                     bias=False,\n                     dilation=dilation)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "def conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=1,\n                     stride=stride,\n                     bias=False)\nclass IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "iresnet18",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "def iresnet18(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained,\n                    progress, **kwargs)\ndef iresnet34(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet50(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet100(pretrained=False, progress=True, **kwargs):",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "iresnet34",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "def iresnet34(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet50(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet100(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet100', IBasicBlock, [3, 13, 30, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet200(pretrained=False, progress=True, **kwargs):",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "iresnet50",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "def iresnet50(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet100(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet100', IBasicBlock, [3, 13, 30, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet200(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet200', IBasicBlock, [6, 26, 60, 6], pretrained,\n                    progress, **kwargs)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "iresnet100",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "def iresnet100(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet100', IBasicBlock, [3, 13, 30, 3], pretrained,\n                    progress, **kwargs)\ndef iresnet200(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet200', IBasicBlock, [6, 26, 60, 6], pretrained,\n                    progress, **kwargs)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "iresnet200",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "def iresnet200(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet200', IBasicBlock, [6, 26, 60, 6], pretrained,\n                    progress, **kwargs)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "__all__ = ['iresnet18', 'iresnet34', 'iresnet50', 'iresnet100', 'iresnet200']\nusing_ckpt = False\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "using_ckpt",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "peekOfCode": "using_ckpt = False\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,\n                     bias=False,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet",
        "documentation": {}
    },
    {
        "label": "IBasicBlock",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "peekOfCode": "class IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 groups=1, base_width=64, dilation=1):\n        super(IBasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05, )",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "documentation": {}
    },
    {
        "label": "IResNet",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "peekOfCode": "class IResNet(nn.Module):\n    fc_scale = 7 * 7\n    def __init__(self,\n                 block, layers, dropout=0, num_features=512, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n        super(IResNet, self).__init__()\n        self.fp16 = fp16\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,\n                     bias=False,\n                     dilation=dilation)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "peekOfCode": "def conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=1,\n                     stride=stride,\n                     bias=False)\nclass IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "documentation": {}
    },
    {
        "label": "iresnet2060",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "peekOfCode": "def iresnet2060(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet2060', IBasicBlock, [3, 128, 1024 - 128, 3], pretrained, progress, **kwargs)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "peekOfCode": "__all__ = ['iresnet2060']\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,\n                     bias=False,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.iresnet2060",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "class Flatten(Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\nclass ConvBlock(Module):\n    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n        super(ConvBlock, self).__init__()\n        self.layers = nn.Sequential(\n            Conv2d(in_c, out_c, kernel, groups=groups, stride=stride, padding=padding, bias=False),\n            BatchNorm2d(num_features=out_c),\n            PReLU(num_parameters=out_c)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "class ConvBlock(Module):\n    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n        super(ConvBlock, self).__init__()\n        self.layers = nn.Sequential(\n            Conv2d(in_c, out_c, kernel, groups=groups, stride=stride, padding=padding, bias=False),\n            BatchNorm2d(num_features=out_c),\n            PReLU(num_parameters=out_c)\n        )\n    def forward(self, x):\n        return self.layers(x)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "LinearBlock",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "class LinearBlock(Module):\n    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n        super(LinearBlock, self).__init__()\n        self.layers = nn.Sequential(\n            Conv2d(in_c, out_c, kernel, stride, padding, groups=groups, bias=False),\n            BatchNorm2d(num_features=out_c)\n        )\n    def forward(self, x):\n        return self.layers(x)\nclass DepthWise(Module):",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "DepthWise",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "class DepthWise(Module):\n    def __init__(self, in_c, out_c, residual=False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n        super(DepthWise, self).__init__()\n        self.residual = residual\n        self.layers = nn.Sequential(\n            ConvBlock(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1)),\n            ConvBlock(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride),\n            LinearBlock(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n        )\n    def forward(self, x):",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "Residual",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "class Residual(Module):\n    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n        super(Residual, self).__init__()\n        modules = []\n        for _ in range(num_block):\n            modules.append(DepthWise(c, c, True, kernel, stride, padding, groups))\n        self.layers = Sequential(*modules)\n    def forward(self, x):\n        return self.layers(x)\nclass GDC(Module):",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "GDC",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "class GDC(Module):\n    def __init__(self, embedding_size):\n        super(GDC, self).__init__()\n        self.layers = nn.Sequential(\n            LinearBlock(512, 512, groups=512, kernel=(7, 7), stride=(1, 1), padding=(0, 0)),\n            Flatten(),\n            Linear(512, embedding_size, bias=False),\n            BatchNorm1d(embedding_size))\n    def forward(self, x):\n        return self.layers(x)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "MobileFaceNet",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "class MobileFaceNet(Module):\n    def __init__(self, fp16=False, num_features=512, blocks=(1, 4, 6, 2), scale=2):\n        super(MobileFaceNet, self).__init__()\n        self.scale = scale\n        self.fp16 = fp16\n        self.layers = nn.ModuleList()\n        self.layers.append(\n            ConvBlock(3, 64 * self.scale, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n        if blocks[0] == 1:",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "get_mbf",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "def get_mbf(fp16, num_features, blocks=(1, 4, 6, 2), scale=2):\n    return MobileFaceNet(fp16, num_features, blocks, scale=scale)\ndef get_mbf_large(fp16, num_features, blocks=(2, 8, 12, 4), scale=4):\n    return MobileFaceNet(fp16, num_features, blocks, scale=scale)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "get_mbf_large",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "peekOfCode": "def get_mbf_large(fp16, num_features, blocks=(2, 8, 12, 4), scale=4):\n    return MobileFaceNet(fp16, num_features, blocks, scale=scale)",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.mobilefacenet",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU6, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "documentation": {}
    },
    {
        "label": "VITBatchNorm",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "peekOfCode": "class VITBatchNorm(nn.Module):\n    def __init__(self, num_features):\n        super().__init__()\n        self.num_features = num_features\n        self.bn = nn.BatchNorm1d(num_features=num_features)\n    def forward(self, x):\n        return self.bn(x)\nclass Attention(nn.Module):\n    def __init__(self,\n                 dim: int,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self,\n                 dim: int,\n                 num_heads: int = 8,\n                 qkv_bias: bool = False,\n                 qk_scale: Optional[None] = None,\n                 attn_drop: float = 0.,\n                 proj_drop: float = 0.):\n        super().__init__()\n        self.num_heads = num_heads",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(self,\n                 dim: int,\n                 num_heads: int,\n                 num_patches: int,\n                 mlp_ratio: float = 4.,\n                 qkv_bias: bool = False,\n                 qk_scale: Optional[None] = None,\n                 drop: float = 0.,\n                 attn_drop: float = 0.,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    def __init__(self, img_size=108, patch_size=9, in_channels=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * \\\n            (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "documentation": {}
    },
    {
        "label": "VisionTransformer",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "description": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "peekOfCode": "class VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n    def __init__(self,\n                 img_size: int = 112,\n                 patch_size: int = 16,\n                 in_channels: int = 3,\n                 num_classes: int = 1000,\n                 embed_dim: int = 768,\n                 depth: int = 12,",
        "detail": "alignment3D.core3d.models.arcface_torch.backbones.vit",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.sample_rate = 0.1\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.batch_size = 512 # total_batch_size = batch_size * num_gpus\nconfig.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.lr = 0.1  # batch size is 512\nconfig.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.rec = \"synthetic\"\nconfig.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.num_classes = 30 * 10000\nconfig.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.num_image = 100000\nconfig.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.num_epoch = 30\nconfig.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.warmup_epoch = -1\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "description": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.3millions",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config = edict()\n# Margin Base Softmax\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.save_all_states = False\nconfig.output = \"ms1mv3_arcface_r50\"\nconfig.embedding_size = 512\n# Partial FC\nconfig.sample_rate = 1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.save_all_states = False\nconfig.output = \"ms1mv3_arcface_r50\"\nconfig.embedding_size = 512\n# Partial FC\nconfig.sample_rate = 1\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.save_all_states = False\nconfig.output = \"ms1mv3_arcface_r50\"\nconfig.embedding_size = 512\n# Partial FC\nconfig.sample_rate = 1\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = False\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.resume = False\nconfig.save_all_states = False\nconfig.output = \"ms1mv3_arcface_r50\"\nconfig.embedding_size = 512\n# Partial FC\nconfig.sample_rate = 1\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = False\nconfig.batch_size = 128\n# For SGD ",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.save_all_states",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.save_all_states = False\nconfig.output = \"ms1mv3_arcface_r50\"\nconfig.embedding_size = 512\n# Partial FC\nconfig.sample_rate = 1\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = False\nconfig.batch_size = 128\n# For SGD \nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.output = \"ms1mv3_arcface_r50\"\nconfig.embedding_size = 512\n# Partial FC\nconfig.sample_rate = 1\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = False\nconfig.batch_size = 128\n# For SGD \nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.embedding_size = 512\n# Partial FC\nconfig.sample_rate = 1\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = False\nconfig.batch_size = 128\n# For SGD \nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.momentum = 0.9",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.sample_rate = 1\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = False\nconfig.batch_size = 128\n# For SGD \nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\n# For AdamW",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.interclass_filtering_threshold = 0\nconfig.fp16 = False\nconfig.batch_size = 128\n# For SGD \nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\n# For AdamW\n# config.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.fp16 = False\nconfig.batch_size = 128\n# For SGD \nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\n# For AdamW\n# config.optimizer = \"adamw\"\n# config.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.batch_size = 128\n# For SGD \nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\n# For AdamW\n# config.optimizer = \"adamw\"\n# config.lr = 0.001\n# config.weight_decay = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\n# For AdamW\n# config.optimizer = \"adamw\"\n# config.lr = 0.001\n# config.weight_decay = 0.1\nconfig.verbose = 2000\nconfig.frequent = 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.lr = 0.1\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\n# For AdamW\n# config.optimizer = \"adamw\"\n# config.lr = 0.001\n# config.weight_decay = 0.1\nconfig.verbose = 2000\nconfig.frequent = 10\n# For Large Sacle Dataset, such as WebFace42M",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\n# For AdamW\n# config.optimizer = \"adamw\"\n# config.lr = 0.001\n# config.weight_decay = 0.1\nconfig.verbose = 2000\nconfig.frequent = 10\n# For Large Sacle Dataset, such as WebFace42M\nconfig.dali = False ",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.weight_decay = 5e-4\n# For AdamW\n# config.optimizer = \"adamw\"\n# config.lr = 0.001\n# config.weight_decay = 0.1\nconfig.verbose = 2000\nconfig.frequent = 10\n# For Large Sacle Dataset, such as WebFace42M\nconfig.dali = False \nconfig.dali_aug = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.verbose = 2000\nconfig.frequent = 10\n# For Large Sacle Dataset, such as WebFace42M\nconfig.dali = False \nconfig.dali_aug = False\n# Gradient ACC\nconfig.gradient_acc = 1\n# setup seed\nconfig.seed = 2048\n# dataload numworkers",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.frequent",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.frequent = 10\n# For Large Sacle Dataset, such as WebFace42M\nconfig.dali = False \nconfig.dali_aug = False\n# Gradient ACC\nconfig.gradient_acc = 1\n# setup seed\nconfig.seed = 2048\n# dataload numworkers\nconfig.num_workers = 2",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.dali = False \nconfig.dali_aug = False\n# Gradient ACC\nconfig.gradient_acc = 1\n# setup seed\nconfig.seed = 2048\n# dataload numworkers\nconfig.num_workers = 2\n# WandB Logger\nconfig.wandb_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.dali_aug",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.dali_aug = False\n# Gradient ACC\nconfig.gradient_acc = 1\n# setup seed\nconfig.seed = 2048\n# dataload numworkers\nconfig.num_workers = 2\n# WandB Logger\nconfig.wandb_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nconfig.suffix_run_name = None",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.gradient_acc",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.gradient_acc = 1\n# setup seed\nconfig.seed = 2048\n# dataload numworkers\nconfig.num_workers = 2\n# WandB Logger\nconfig.wandb_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nconfig.suffix_run_name = None\nconfig.using_wandb = False\nconfig.wandb_entity = \"entity\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.seed",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.seed = 2048\n# dataload numworkers\nconfig.num_workers = 2\n# WandB Logger\nconfig.wandb_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nconfig.suffix_run_name = None\nconfig.using_wandb = False\nconfig.wandb_entity = \"entity\"\nconfig.wandb_project = \"project\"\nconfig.wandb_log_all = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.num_workers",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.num_workers = 2\n# WandB Logger\nconfig.wandb_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nconfig.suffix_run_name = None\nconfig.using_wandb = False\nconfig.wandb_entity = \"entity\"\nconfig.wandb_project = \"project\"\nconfig.wandb_log_all = True\nconfig.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.wandb_key",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.wandb_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nconfig.suffix_run_name = None\nconfig.using_wandb = False\nconfig.wandb_entity = \"entity\"\nconfig.wandb_project = \"project\"\nconfig.wandb_log_all = True\nconfig.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.suffix_run_name",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.suffix_run_name = None\nconfig.using_wandb = False\nconfig.wandb_entity = \"entity\"\nconfig.wandb_project = \"project\"\nconfig.wandb_log_all = True\nconfig.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.using_wandb",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.using_wandb = False\nconfig.wandb_entity = \"entity\"\nconfig.wandb_project = \"project\"\nconfig.wandb_log_all = True\nconfig.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.wandb_entity",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.wandb_entity = \"entity\"\nconfig.wandb_project = \"project\"\nconfig.wandb_log_all = True\nconfig.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.wandb_project",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.wandb_project = \"project\"\nconfig.wandb_log_all = True\nconfig.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.wandb_log_all",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.wandb_log_all = True\nconfig.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.save_artifacts",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.save_artifacts = False\nconfig.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config.wandb_resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.base",
        "description": "alignment3D.core3d.models.arcface_torch.configs.base",
        "peekOfCode": "config.wandb_resume = False # resume wandb run: Only if the you wand t resume the last run that it was interrupted",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.base",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_mbf",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.rec = \"/train_tmp/glint360k\"\nconfig.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.num_classes = 360232\nconfig.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.num_image = 17091657\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.glint360k_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.num_image = 5822653\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_mbf",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.rec = \"/train_tmp/faces_emore\"\nconfig.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.num_classes = 85742\nconfig.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.num_image = 5822653\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv2_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.num_image = 5179510\nconfig.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_mbf",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.lr = 0.02\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.rec = \"/train_tmp/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.num_image = 5179510\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "description": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.ms1mv3_r50_onegpu",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M_Conflict\"\nconfig.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.num_classes = 1017970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_conflict_r50_pfc03_filter04",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.1\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.sample_rate = 0.1\nconfig.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.interclass_filtering_threshold = 0.4\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_pfc01_filter04_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M_FLIP40\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_flip_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_mbf",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.interclass_filtering_threshold",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.interclass_filtering_threshold = 0\nconfig.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.optimizer = \"sgd\"\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace12M\"\nconfig.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.num_classes = 617970\nconfig.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.num_image = 12720066\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf12m_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.sample_rate = 0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc0008_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.warmup_epoch = 2\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_mbf_bs8k",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.batch_size = 256\nconfig.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.lr = 0.3\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.warmup_epoch = 1\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.batch_size = 512\nconfig.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.lr = 0.6\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.warmup_epoch = 4\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_16gpus_r50_bs8k",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_32gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.batch_size = 512\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.warmup_epoch = 2\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_8gpus_r50_bs4k",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.lr = 0.2\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_16gpus",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.sample_rate = 0.2\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.verbose = 10000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_r100_32gpus",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_h\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.5\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_h\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.5\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.network = \"vit_h\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.5\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.5\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.5\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.5\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.sample_rate = 0.5\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.weight_decay = 0.1\nconfig.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.batch_size = 768\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.dali = True\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.num_epoch = 16\nconfig.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 8\nconfig.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.val_targets = []\nconfig.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config.dali_aug",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "peekOfCode": "config.dali_aug = True",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc02_vit_h",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r18\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r18\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.network = \"r18\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r18",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r200\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r200\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.network = \"r200\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r200",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.lr = 0.4\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "peekOfCode": "config.val_targets = [\"lfw\", \"cfp_fp\", \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_32gpu_r50",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_b_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_b_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.network = \"vit_b_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_l_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_l_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.network = \"vit_l_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_l",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_s_dp005_mask_0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_s_dp005_mask_0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.network = \"vit_s_dp005_mask_0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_s",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_t_dp005_mask0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_t_dp005_mask0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.network = \"vit_t_dp005_mask0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.weight_decay = 0.1\nconfig.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.batch_size = 384\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_64gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_b_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_b_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.network = \"vit_b_dp005_mask_005\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.weight_decay = 0.1\nconfig.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.batch_size = 256\nconfig.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.gradient_acc",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.gradient_acc = 12 # total batchsize is 256 * 12\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_b",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_t_dp005_mask0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"vit_t_dp005_mask0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.network = \"vit_t_dp005_mask0\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.sample_rate = 0.3\nconfig.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.fp16 = True\nconfig.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.weight_decay = 0.1\nconfig.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.batch_size = 512\nconfig.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.optimizer",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.optimizer = \"adamw\"\nconfig.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.lr = 0.001\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace42M\"\nconfig.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.num_classes = 2059906\nconfig.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.num_image = 42474557\nconfig.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.num_epoch = 40\nconfig.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.warmup_epoch = config.num_epoch // 10\nconfig.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "peekOfCode": "config.val_targets = []",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf42m_pfc03_40epoch_8gpu_vit_t",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.network = \"mbf\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.weight_decay = 1e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_mbf",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.network = \"r100\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r100",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config = edict()\nconfig.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.margin_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.margin_list = (1.0, 0.0, 0.4)\nconfig.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.network",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.network = \"r50\"\nconfig.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.resume",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.resume = False\nconfig.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.output",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.output = None\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.embedding_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.sample_rate",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.fp16",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.momentum",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.weight_decay",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.lr",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.lr = 0.1\nconfig.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.verbose",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.verbose = 2000\nconfig.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.dali",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.dali = False\nconfig.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.rec",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.rec = \"/train_tmp/WebFace4M\"\nconfig.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.num_classes",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.num_classes = 205990\nconfig.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.num_image",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.num_image = 4235242\nconfig.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.num_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.num_epoch = 20\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.warmup_epoch",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "config.val_targets",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "description": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "peekOfCode": "config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]",
        "detail": "alignment3D.core3d.models.arcface_torch.configs.wf4m_r50",
        "documentation": {}
    },
    {
        "label": "LFold",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "class LFold:\n    def __init__(self, n_splits=2, shuffle=False):\n        self.n_splits = n_splits\n        if self.n_splits > 1:\n            self.k_fold = KFold(n_splits=n_splits, shuffle=shuffle)\n    def split(self, indices):\n        if self.n_splits > 1:\n            return self.k_fold.split(indices)\n        else:\n            return [(indices, indices)]",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "calculate_roc",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def calculate_roc(thresholds,\n                  embeddings1,\n                  embeddings2,\n                  actual_issame,\n                  nrof_folds=10,\n                  pca=0):\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "calculate_accuracy",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(\n        np.logical_and(np.logical_not(predict_issame),\n                       np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "calculate_val",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def calculate_val(thresholds,\n                  embeddings1,\n                  embeddings2,\n                  actual_issame,\n                  far_target,\n                  nrof_folds=10):\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "calculate_val_far",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def calculate_val_far(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(\n        np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    # print(true_accept, false_accept)\n    # print(n_same, n_diff)\n    val = float(true_accept) / float(n_same)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = calculate_roc(thresholds,\n                                       embeddings1,\n                                       embeddings2,\n                                       np.asarray(actual_issame),\n                                       nrof_folds=nrof_folds,",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "load_bin",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def load_bin(path, image_size):\n    try:\n        with open(path, 'rb') as f:\n            bins, issame_list = pickle.load(f)  # py2\n    except UnicodeDecodeError as e:\n        with open(path, 'rb') as f:\n            bins, issame_list = pickle.load(f, encoding='bytes')  # py3\n    data_list = []\n    for flip in [0, 1]:\n        data = torch.empty((len(issame_list) * 2, 3, image_size[0], image_size[1]))",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def test(data_set, backbone, batch_size, nfolds=10):\n    print('testing verification..')\n    data_list = data_set[0]\n    issame_list = data_set[1]\n    embeddings_list = []\n    time_consumed = 0.0\n    for i in range(len(data_list)):\n        data = data_list[i]\n        embeddings = None\n        ba = 0",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "dumpR",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "description": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "peekOfCode": "def dumpR(data_set,\n          backbone,\n          batch_size,\n          name='',\n          data_extra=None,\n          label_shape=None):\n    print('dump verification embedding..')\n    data_list = data_set[0]\n    issame_list = data_set[1]\n    embeddings_list = []",
        "detail": "alignment3D.core3d.models.arcface_torch.eval.verification",
        "documentation": {}
    },
    {
        "label": "read_worker",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "description": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "peekOfCode": "def read_worker(args, q_in):\n    path_imgidx = os.path.join(args.input, \"train.idx\")\n    path_imgrec = os.path.join(args.input, \"train.rec\")\n    imgrec = mx.recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, \"r\")\n    s = imgrec.read_idx(0)\n    header, _ = mx.recordio.unpack(s)\n    assert header.flag > 0\n    imgidx = np.array(range(1, int(header.label[0])))\n    np.random.shuffle(imgidx)\n    for idx in imgidx:",
        "detail": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "documentation": {}
    },
    {
        "label": "write_worker",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "description": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "peekOfCode": "def write_worker(args, q_out):\n    pre_time = time.time()\n    if args.input[-1] == '/':\n        args.input = args.input[:-1]\n    dirname = os.path.dirname(args.input)\n    basename = os.path.basename(args.input)\n    output = os.path.join(dirname, f\"shuffled_{basename}\")\n    os.makedirs(output, exist_ok=True)\n    path_imgidx = os.path.join(output, \"train.idx\")\n    path_imgrec = os.path.join(output, \"train.rec\")",
        "detail": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "description": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "peekOfCode": "def main(args):\n    queue = multiprocessing.Queue(10240)\n    read_process = multiprocessing.Process(target=read_worker, args=(args, queue))\n    read_process.daemon = True\n    read_process.start()\n    write_process = multiprocessing.Process(target=write_worker, args=(args, queue))\n    write_process.start()\n    write_process.join()\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()",
        "detail": "alignment3D.core3d.models.arcface_torch.scripts.shuffle_rec",
        "documentation": {}
    },
    {
        "label": "read_template_pair_list",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "def read_template_pair_list(path):\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    t1 = pairs[:, 0].astype(np.int)\n    t2 = pairs[:, 1].astype(np.int)\n    label = pairs[:, 2].astype(np.int)\n    return t1, t2, label\np1, p2, label = read_template_pair_list(\n    os.path.join('%s/meta' % image_path,\n                 '%s_template_pair_label.txt' % 'ijbc'))\nmethods = []",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "files = [x.strip() for x in files]\nimage_path = \"/train_tmp/IJB_release/IJBC\"\ndef read_template_pair_list(path):\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    t1 = pairs[:, 0].astype(np.int)\n    t2 = pairs[:, 1].astype(np.int)\n    label = pairs[:, 2].astype(np.int)\n    return t1, t2, label\np1, p2, label = read_template_pair_list(\n    os.path.join('%s/meta' % image_path,",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "image_path",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "image_path = \"/train_tmp/IJB_release/IJBC\"\ndef read_template_pair_list(path):\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    t1 = pairs[:, 0].astype(np.int)\n    t2 = pairs[:, 1].astype(np.int)\n    label = pairs[:, 2].astype(np.int)\n    return t1, t2, label\np1, p2, label = read_template_pair_list(\n    os.path.join('%s/meta' % image_path,\n                 '%s_template_pair_label.txt' % 'ijbc'))",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "methods",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "methods = []\nscores = []\nfor file in files:\n    methods.append(file)\n    scores.append(np.load(file))\nmethods = np.array(methods)\nscores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "scores",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "scores = []\nfor file in files:\n    methods.append(file)\n    scores.append(np.load(file))\nmethods = np.array(methods)\nscores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "methods",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "methods = np.array(methods)\nscores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "scores",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "scores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "colours",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "colours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "x_labels",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "x_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr\n    plt.plot(fpr,\n             tpr,",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "tpr_fpr_table",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "tpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr\n    plt.plot(fpr,\n             tpr,\n             color=colours[method],",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "description": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "peekOfCode": "fig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr\n    plt.plot(fpr,\n             tpr,\n             color=colours[method],\n             lw=1,",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.plot",
        "documentation": {}
    },
    {
        "label": "CallBackVerification",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_callbacks",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_callbacks",
        "peekOfCode": "class CallBackVerification(object):\n    def __init__(self, val_targets, rec_prefix, summary_writer=None, image_size=(112, 112), wandb_logger=None):\n        self.rank: int = distributed.get_rank()\n        self.highest_acc: float = 0.0\n        self.highest_acc_list: List[float] = [0.0] * len(val_targets)\n        self.ver_list: List[object] = []\n        self.ver_name_list: List[str] = []\n        if self.rank is 0:\n            self.init_dataset(val_targets=val_targets, data_dir=rec_prefix, image_size=image_size)\n        self.summary_writer = summary_writer",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_callbacks",
        "documentation": {}
    },
    {
        "label": "CallBackLogging",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_callbacks",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_callbacks",
        "peekOfCode": "class CallBackLogging(object):\n    def __init__(self, frequent, total_step, batch_size, start_step=0,writer=None):\n        self.frequent: int = frequent\n        self.rank: int = distributed.get_rank()\n        self.world_size: int = distributed.get_world_size()\n        self.time_start = time.time()\n        self.total_step: int = total_step\n        self.start_step: int = start_step\n        self.batch_size: int = batch_size\n        self.writer = writer",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_callbacks",
        "documentation": {}
    },
    {
        "label": "get_config",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_config",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_config",
        "peekOfCode": "def get_config(config_file):\n    assert config_file.startswith('configs/'), 'config file setting must start with configs/'\n    temp_config_name = osp.basename(config_file)\n    temp_module_name = osp.splitext(temp_config_name)[0]\n    config = importlib.import_module(\"configs.base\")\n    cfg = config.config\n    config = importlib.import_module(\"configs.%s\" % temp_module_name)\n    job_cfg = config.config\n    cfg.update(job_cfg)\n    if cfg.output is None:",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_config",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "peekOfCode": "class DistributedSampler(_DistributedSampler):\n    def __init__(\n        self,\n        dataset,\n        num_replicas=None,  # world_size\n        rank=None,  # local_rank\n        shuffle=True,\n        seed=0,\n    ):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "setup_seed",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "peekOfCode": "def setup_seed(seed, cuda_deterministic=True):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    if cuda_deterministic:  # slower, more reproducible\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    else:  # faster, less reproducible",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "worker_init_fn",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "peekOfCode": "def worker_init_fn(worker_id, num_workers, rank, seed):\n    # The seed of each worker equals to\n    # num_worker * rank + worker_id + user_seed\n    worker_seed = num_workers * rank + worker_id + seed\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\ndef get_dist_info():\n    if dist.is_available() and dist.is_initialized():\n        rank = dist.get_rank()",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "get_dist_info",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "peekOfCode": "def get_dist_info():\n    if dist.is_available() and dist.is_initialized():\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n    return rank, world_size\ndef sync_random_seed(seed=None, device=\"cuda\"):\n    \"\"\"Make sure different ranks share the same seed.",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "sync_random_seed",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "peekOfCode": "def sync_random_seed(seed=None, device=\"cuda\"):\n    \"\"\"Make sure different ranks share the same seed.\n    All workers must call this function, otherwise it will deadlock.\n    This method is generally used in `DistributedSampler`,\n    because the seed should be identical across all processes\n    in the distributed group.\n    In distributed sampling, different ranks should sample non-overlapped\n    data in the dataset. Therefore, this function is used to make sure that\n    each rank shuffles the data indices in the same order based\n    on the same seed. Then different ranks could use different indices",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_distributed_sampler",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_logging",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_logging",
        "peekOfCode": "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n        self.reset()\n    def reset(self):",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_logging",
        "documentation": {}
    },
    {
        "label": "init_logging",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.utils.utils_logging",
        "description": "alignment3D.core3d.models.arcface_torch.utils.utils_logging",
        "peekOfCode": "def init_logging(rank, models_root):\n    if rank == 0:\n        log_root = logging.getLogger()\n        log_root.setLevel(logging.INFO)\n        formatter = logging.Formatter(\"Training: %(asctime)s-%(message)s\")\n        handler_file = logging.FileHandler(os.path.join(models_root, \"training.log\"))\n        handler_stream = logging.StreamHandler(sys.stdout)\n        handler_file.setFormatter(formatter)\n        handler_stream.setFormatter(formatter)\n        log_root.addHandler(handler_file)",
        "detail": "alignment3D.core3d.models.arcface_torch.utils.utils_logging",
        "documentation": {}
    },
    {
        "label": "BackgroundGenerator",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.dataset",
        "description": "alignment3D.core3d.models.arcface_torch.dataset",
        "peekOfCode": "class BackgroundGenerator(threading.Thread):\n    def __init__(self, generator, local_rank, max_prefetch=6):\n        super(BackgroundGenerator, self).__init__()\n        self.queue = Queue.Queue(max_prefetch)\n        self.generator = generator\n        self.local_rank = local_rank\n        self.daemon = True\n        self.start()\n    def run(self):\n        torch.cuda.set_device(self.local_rank)",
        "detail": "alignment3D.core3d.models.arcface_torch.dataset",
        "documentation": {}
    },
    {
        "label": "DataLoaderX",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.dataset",
        "description": "alignment3D.core3d.models.arcface_torch.dataset",
        "peekOfCode": "class DataLoaderX(DataLoader):\n    def __init__(self, local_rank, **kwargs):\n        super(DataLoaderX, self).__init__(**kwargs)\n        self.stream = torch.cuda.Stream(local_rank)\n        self.local_rank = local_rank\n    def __iter__(self):\n        self.iter = super(DataLoaderX, self).__iter__()\n        self.iter = BackgroundGenerator(self.iter, self.local_rank)\n        self.preload()\n        return self",
        "detail": "alignment3D.core3d.models.arcface_torch.dataset",
        "documentation": {}
    },
    {
        "label": "MXFaceDataset",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.dataset",
        "description": "alignment3D.core3d.models.arcface_torch.dataset",
        "peekOfCode": "class MXFaceDataset(Dataset):\n    def __init__(self, root_dir, local_rank):\n        super(MXFaceDataset, self).__init__()\n        self.transform = transforms.Compose(\n            [transforms.ToPILImage(),\n             transforms.RandomHorizontalFlip(),\n             transforms.ToTensor(),\n             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n             ])\n        self.root_dir = root_dir",
        "detail": "alignment3D.core3d.models.arcface_torch.dataset",
        "documentation": {}
    },
    {
        "label": "SyntheticDataset",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.dataset",
        "description": "alignment3D.core3d.models.arcface_torch.dataset",
        "peekOfCode": "class SyntheticDataset(Dataset):\n    def __init__(self):\n        super(SyntheticDataset, self).__init__()\n        img = np.random.randint(0, 255, size=(112, 112, 3), dtype=np.int32)\n        img = np.transpose(img, (2, 0, 1))\n        img = torch.from_numpy(img).squeeze(0).float()\n        img = ((img / 255) - 0.5) / 0.5\n        self.img = img\n        self.label = 1\n    def __getitem__(self, index):",
        "detail": "alignment3D.core3d.models.arcface_torch.dataset",
        "documentation": {}
    },
    {
        "label": "DALIWarper",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.dataset",
        "description": "alignment3D.core3d.models.arcface_torch.dataset",
        "peekOfCode": "class DALIWarper(object):\n    def __init__(self, dali_iter):\n        self.iter = dali_iter\n    def __next__(self):\n        data_dict = self.iter.__next__()[0]\n        tensor_data = data_dict['data'].cuda()\n        tensor_label: torch.Tensor = data_dict['label'].cuda().long()\n        tensor_label.squeeze_()\n        return tensor_data, tensor_label\n    def __iter__(self):",
        "detail": "alignment3D.core3d.models.arcface_torch.dataset",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.dataset",
        "description": "alignment3D.core3d.models.arcface_torch.dataset",
        "peekOfCode": "def get_dataloader(\n    root_dir,\n    local_rank,\n    batch_size,\n    dali = False,\n    dali_aug = False,\n    seed = 2048,\n    num_workers = 2,\n    ) -> Iterable:\n    rec = os.path.join(root_dir, 'train.rec')",
        "detail": "alignment3D.core3d.models.arcface_torch.dataset",
        "documentation": {}
    },
    {
        "label": "dali_data_iter",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.dataset",
        "description": "alignment3D.core3d.models.arcface_torch.dataset",
        "peekOfCode": "def dali_data_iter(\n    batch_size: int, rec_file: str, idx_file: str, num_threads: int,\n    initial_fill=32768, random_shuffle=True,\n    prefetch_queue_depth=1, local_rank=0, name=\"reader\",\n    mean=(127.5, 127.5, 127.5), \n    std=(127.5, 127.5, 127.5),\n    dali_aug=False\n    ):\n    \"\"\"\n    Parameters:",
        "detail": "alignment3D.core3d.models.arcface_torch.dataset",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "class Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)\n        self.image_size = image_size\n        weight = torch.load(prefix)\n        resnet = get_model(args.network, dropout=0, fp16=False).cuda()\n        resnet.load_state_dict(weight)\n        model = torch.nn.DataParallel(resnet)\n        self.model = model\n        self.model.eval()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "divideIntoNstrand",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def divideIntoNstrand(listTemp, n):\n    twoList = [[] for i in range(n)]\n    for i, e in enumerate(listTemp):\n        twoList[i % n].append(e)\n    return twoList\ndef read_template_media_list(path):\n    # ijb_meta = np.loadtxt(path, dtype=str)\n    ijb_meta = pd.read_csv(path, sep=' ', header=None).values\n    templates = ijb_meta[:, 1].astype(np.int)\n    medias = ijb_meta[:, 2].astype(np.int)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "read_template_media_list",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def read_template_media_list(path):\n    # ijb_meta = np.loadtxt(path, dtype=str)\n    ijb_meta = pd.read_csv(path, sep=' ', header=None).values\n    templates = ijb_meta[:, 1].astype(np.int)\n    medias = ijb_meta[:, 2].astype(np.int)\n    return templates, medias\n# In[ ]:\ndef read_template_pair_list(path):\n    # pairs = np.loadtxt(path, dtype=str)\n    pairs = pd.read_csv(path, sep=' ', header=None).values",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "read_template_pair_list",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def read_template_pair_list(path):\n    # pairs = np.loadtxt(path, dtype=str)\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    # print(pairs.shape)\n    # print(pairs[:, 0].astype(np.int))\n    t1 = pairs[:, 0].astype(np.int)\n    t2 = pairs[:, 1].astype(np.int)\n    label = pairs[:, 2].astype(np.int)\n    return t1, t2, label\n# In[ ]:",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "read_image_feature",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def read_image_feature(path):\n    with open(path, 'rb') as fid:\n        img_feats = pickle.load(fid)\n    return img_feats\n# In[ ]:\ndef get_image_feature(img_path, files_list, model_path, epoch, gpu_id):\n    batch_size = args.batch_size\n    data_shape = (3, 112, 112)\n    files = files_list\n    print('files:', len(files))",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "get_image_feature",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def get_image_feature(img_path, files_list, model_path, epoch, gpu_id):\n    batch_size = args.batch_size\n    data_shape = (3, 112, 112)\n    files = files_list\n    print('files:', len(files))\n    rare_size = len(files) % batch_size\n    faceness_scores = []\n    batch = 0\n    img_feats = np.empty((len(files), 1024), dtype=np.float32)\n    batch_data = np.empty((2 * batch_size, 3, 112, 112))",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "image2template_feature",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def image2template_feature(img_feats=None, templates=None, medias=None):\n    # ==========================================================\n    # 1. face image feature l2 normalization. img_feats:[number_image x feats_dim]\n    # 2. compute media feature.\n    # 3. compute template feature.\n    # ==========================================================\n    unique_templates = np.unique(templates)\n    template_feats = np.zeros((len(unique_templates), img_feats.shape[1]))\n    for count_template, uqt in enumerate(unique_templates):\n        (ind_t,) = np.where(templates == uqt)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "verification",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def verification(template_norm_feats=None,\n                 unique_templates=None,\n                 p1=None,\n                 p2=None):\n    # ==========================================================\n    #         Compute set-to-set Similarity Score.\n    # ==========================================================\n    template2id = np.zeros((max(unique_templates) + 1, 1), dtype=int)\n    for count_template, uqt in enumerate(unique_templates):\n        template2id[uqt] = count_template",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "verification2",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def verification2(template_norm_feats=None,\n                  unique_templates=None,\n                  p1=None,\n                  p2=None):\n    template2id = np.zeros((max(unique_templates) + 1, 1), dtype=int)\n    for count_template, uqt in enumerate(unique_templates):\n        template2id[uqt] = count_template\n    score = np.zeros((len(p1),))  # save cosine distance between pairs\n    total_pairs = np.array(range(len(p1)))\n    batchsize = 100000  # small batchsize instead of all pairs in one batch due to the memory limiation",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "read_score",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "def read_score(path):\n    with open(path, 'rb') as fid:\n        img_feats = pickle.load(fid)\n    return img_feats\n# # Step1: Load Meta Data\n# In[ ]:\nassert target == 'IJBC' or target == 'IJBB'\n# =============================================================\n# load image and template relationships for template feature embedding\n# tid --> template id,  mid --> media id",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "parser = argparse.ArgumentParser(description='do ijb test')\n# general\nparser.add_argument('--model-prefix', default='', help='path to load model.')\nparser.add_argument('--image-path', default='', type=str, help='')\nparser.add_argument('--result-dir', default='.', type=str, help='')\nparser.add_argument('--batch-size', default=128, type=int, help='')\nparser.add_argument('--network', default='iresnet50', type=str, help='')\nparser.add_argument('--job', default='insightface', type=str, help='job name')\nparser.add_argument('--target', default='IJBC', type=str, help='target, set to IJBC or IJBB')\nargs = parser.parse_args()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "args = parser.parse_args()\ntarget = args.target\nmodel_path = args.model_prefix\nimage_path = args.image_path\nresult_dir = args.result_dir\ngpu_id = None\nuse_norm_score = True  # if Ture, TestMode(N1)\nuse_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "target",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "target = args.target\nmodel_path = args.model_prefix\nimage_path = args.image_path\nresult_dir = args.result_dir\ngpu_id = None\nuse_norm_score = True  # if Ture, TestMode(N1)\nuse_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "model_path = args.model_prefix\nimage_path = args.image_path\nresult_dir = args.result_dir\ngpu_id = None\nuse_norm_score = True  # if Ture, TestMode(N1)\nuse_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size\nclass Embedding(object):",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "image_path",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "image_path = args.image_path\nresult_dir = args.result_dir\ngpu_id = None\nuse_norm_score = True  # if Ture, TestMode(N1)\nuse_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "result_dir",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "result_dir = args.result_dir\ngpu_id = None\nuse_norm_score = True  # if Ture, TestMode(N1)\nuse_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "gpu_id",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "gpu_id = None\nuse_norm_score = True  # if Ture, TestMode(N1)\nuse_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)\n        self.image_size = image_size",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "use_norm_score",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "use_norm_score = True  # if Ture, TestMode(N1)\nuse_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)\n        self.image_size = image_size\n        weight = torch.load(prefix)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "use_detector_score",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "use_detector_score = True  # if Ture, TestMode(D1)\nuse_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)\n        self.image_size = image_size\n        weight = torch.load(prefix)\n        resnet = get_model(args.network, dropout=0, fp16=False).cuda()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "use_flip_test",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "use_flip_test = True  # if Ture, TestMode(F1)\njob = args.job\nbatch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)\n        self.image_size = image_size\n        weight = torch.load(prefix)\n        resnet = get_model(args.network, dropout=0, fp16=False).cuda()\n        resnet.load_state_dict(weight)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "job",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "job = args.job\nbatch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)\n        self.image_size = image_size\n        weight = torch.load(prefix)\n        resnet = get_model(args.network, dropout=0, fp16=False).cuda()\n        resnet.load_state_dict(weight)\n        model = torch.nn.DataParallel(resnet)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "batch_size = args.batch_size\nclass Embedding(object):\n    def __init__(self, prefix, data_shape, batch_size=1):\n        image_size = (112, 112)\n        self.image_size = image_size\n        weight = torch.load(prefix)\n        resnet = get_model(args.network, dropout=0, fp16=False).cuda()\n        resnet.load_state_dict(weight)\n        model = torch.nn.DataParallel(resnet)\n        self.model = model",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "start",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "start = timeit.default_timer()\ntemplates, medias = read_template_media_list(\n    os.path.join('%s/meta' % image_path,\n                 '%s_face_tid_mid.txt' % target.lower()))\nstop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# In[ ]:\n# =============================================================\n# load template pairs for template-to-template verification\n# tid : template id,  label : 1/0",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "stop",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "stop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# In[ ]:\n# =============================================================\n# load template pairs for template-to-template verification\n# tid : template id,  label : 1/0\n# format:\n#           tid_1 tid_2 label\n# =============================================================\nstart = timeit.default_timer()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "start",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "start = timeit.default_timer()\np1, p2, label = read_template_pair_list(\n    os.path.join('%s/meta' % image_path,\n                 '%s_template_pair_label.txt' % target.lower()))\nstop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# # Step 2: Get Image Features\n# In[ ]:\n# =============================================================\n# load image features",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "stop",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "stop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# # Step 2: Get Image Features\n# In[ ]:\n# =============================================================\n# load image features\n# format:\n#           img_feats: [image_num x feats_dim] (227630, 512)\n# =============================================================\nstart = timeit.default_timer()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "start",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "start = timeit.default_timer()\nimg_path = '%s/loose_crop' % image_path\nimg_list_path = '%s/meta/%s_name_5pts_score.txt' % (image_path, target.lower())\nimg_list = open(img_list_path)\nfiles = img_list.readlines()\n# files_list = divideIntoNstrand(files, rank_size)\nfiles_list = files\n# img_feats\n# for i in range(rank_size):\nimg_feats, faceness_scores = get_image_feature(img_path, files_list,",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "img_path = '%s/loose_crop' % image_path\nimg_list_path = '%s/meta/%s_name_5pts_score.txt' % (image_path, target.lower())\nimg_list = open(img_list_path)\nfiles = img_list.readlines()\n# files_list = divideIntoNstrand(files, rank_size)\nfiles_list = files\n# img_feats\n# for i in range(rank_size):\nimg_feats, faceness_scores = get_image_feature(img_path, files_list,\n                                               model_path, 0, gpu_id)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "img_list_path",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "img_list_path = '%s/meta/%s_name_5pts_score.txt' % (image_path, target.lower())\nimg_list = open(img_list_path)\nfiles = img_list.readlines()\n# files_list = divideIntoNstrand(files, rank_size)\nfiles_list = files\n# img_feats\n# for i in range(rank_size):\nimg_feats, faceness_scores = get_image_feature(img_path, files_list,\n                                               model_path, 0, gpu_id)\nstop = timeit.default_timer()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "img_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "img_list = open(img_list_path)\nfiles = img_list.readlines()\n# files_list = divideIntoNstrand(files, rank_size)\nfiles_list = files\n# img_feats\n# for i in range(rank_size):\nimg_feats, faceness_scores = get_image_feature(img_path, files_list,\n                                               model_path, 0, gpu_id)\nstop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "files = img_list.readlines()\n# files_list = divideIntoNstrand(files, rank_size)\nfiles_list = files\n# img_feats\n# for i in range(rank_size):\nimg_feats, faceness_scores = get_image_feature(img_path, files_list,\n                                               model_path, 0, gpu_id)\nstop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\nprint('Feature Shape: ({} , {}) .'.format(img_feats.shape[0],",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "files_list",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "files_list = files\n# img_feats\n# for i in range(rank_size):\nimg_feats, faceness_scores = get_image_feature(img_path, files_list,\n                                               model_path, 0, gpu_id)\nstop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\nprint('Feature Shape: ({} , {}) .'.format(img_feats.shape[0],\n                                          img_feats.shape[1]))\n# # Step3: Get Template Features",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "stop",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "stop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\nprint('Feature Shape: ({} , {}) .'.format(img_feats.shape[0],\n                                          img_feats.shape[1]))\n# # Step3: Get Template Features\n# In[ ]:\n# =============================================================\n# compute template features from image features.\n# =============================================================\nstart = timeit.default_timer()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "start",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "start = timeit.default_timer()\n# ==========================================================\n# Norm feature before aggregation into template feature?\n# Feature norm from embedding network and faceness score are able to decrease weights for noise samples (not face).\n# ==========================================================\n# 1. FaceScore （Feature Norm）\n# 2. FaceScore （Detector）\nif use_flip_test:\n    # concat --- F1\n    # img_input_feats = img_feats",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "stop",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "stop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# # Step 4: Get Template Similarity Scores\n# In[ ]:\n# =============================================================\n# compute verification scores between template pairs.\n# =============================================================\nstart = timeit.default_timer()\nscore = verification(template_norm_feats, unique_templates, p1, p2)\nstop = timeit.default_timer()",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "start",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "start = timeit.default_timer()\nscore = verification(template_norm_feats, unique_templates, p1, p2)\nstop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# In[ ]:\nsave_path = os.path.join(result_dir, args.job)\n# save_path = result_dir + '/%s_result' % target\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nscore_save_file = os.path.join(save_path, \"%s.npy\" % target.lower())",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "score",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "score = verification(template_norm_feats, unique_templates, p1, p2)\nstop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# In[ ]:\nsave_path = os.path.join(result_dir, args.job)\n# save_path = result_dir + '/%s_result' % target\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nscore_save_file = os.path.join(save_path, \"%s.npy\" % target.lower())\nnp.save(score_save_file, score)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "stop",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "stop = timeit.default_timer()\nprint('Time: %.2f s. ' % (stop - start))\n# In[ ]:\nsave_path = os.path.join(result_dir, args.job)\n# save_path = result_dir + '/%s_result' % target\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nscore_save_file = os.path.join(save_path, \"%s.npy\" % target.lower())\nnp.save(score_save_file, score)\n# # Step 5: Get ROC Curves and TPR@FPR Table",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "save_path",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "save_path = os.path.join(result_dir, args.job)\n# save_path = result_dir + '/%s_result' % target\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nscore_save_file = os.path.join(save_path, \"%s.npy\" % target.lower())\nnp.save(score_save_file, score)\n# # Step 5: Get ROC Curves and TPR@FPR Table\n# In[ ]:\nfiles = [score_save_file]\nmethods = []",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "score_save_file",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "score_save_file = os.path.join(save_path, \"%s.npy\" % target.lower())\nnp.save(score_save_file, score)\n# # Step 5: Get ROC Curves and TPR@FPR Table\n# In[ ]:\nfiles = [score_save_file]\nmethods = []\nscores = []\nfor file in files:\n    methods.append(Path(file).stem)\n    scores.append(np.load(file))",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "files = [score_save_file]\nmethods = []\nscores = []\nfor file in files:\n    methods.append(Path(file).stem)\n    scores.append(np.load(file))\nmethods = np.array(methods)\nscores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "methods",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "methods = []\nscores = []\nfor file in files:\n    methods.append(Path(file).stem)\n    scores.append(np.load(file))\nmethods = np.array(methods)\nscores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "scores",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "scores = []\nfor file in files:\n    methods.append(Path(file).stem)\n    scores.append(np.load(file))\nmethods = np.array(methods)\nscores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "methods",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "methods = np.array(methods)\nscores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "scores",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "scores = dict(zip(methods, scores))\ncolours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "colours",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "colours = dict(\n    zip(methods, sample_colours_from_colourmap(methods.shape[0], 'Set2')))\nx_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "x_labels",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "x_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]\ntpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr\n    plt.plot(fpr,\n             tpr,",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "tpr_fpr_table",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "tpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])\nfig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr\n    plt.plot(fpr,\n             tpr,\n             color=colours[method],",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "peekOfCode": "fig = plt.figure()\nfor method in methods:\n    fpr, tpr, _ = roc_curve(label, scores[method])\n    roc_auc = auc(fpr, tpr)\n    fpr = np.flipud(fpr)\n    tpr = np.flipud(tpr)  # select largest tpr at same fpr\n    plt.plot(fpr,\n             tpr,\n             color=colours[method],\n             lw=1,",
        "detail": "alignment3D.core3d.models.arcface_torch.eval_ijbc",
        "documentation": {}
    },
    {
        "label": "inference",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.inference",
        "description": "alignment3D.core3d.models.arcface_torch.inference",
        "peekOfCode": "def inference(weight, name, img):\n    if img is None:\n        img = np.random.randint(0, 255, size=(112, 112, 3), dtype=np.uint8)\n    else:\n        img = cv2.imread(img)\n        img = cv2.resize(img, (112, 112))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = np.transpose(img, (2, 0, 1))\n    img = torch.from_numpy(img).unsqueeze(0).float()\n    img.div_(255).sub_(0.5).div_(0.5)",
        "detail": "alignment3D.core3d.models.arcface_torch.inference",
        "documentation": {}
    },
    {
        "label": "CombinedMarginLoss",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.losses",
        "description": "alignment3D.core3d.models.arcface_torch.losses",
        "peekOfCode": "class CombinedMarginLoss(torch.nn.Module):\n    def __init__(self, \n                 s, \n                 m1,\n                 m2,\n                 m3,\n                 interclass_filtering_threshold=0):\n        super().__init__()\n        self.s = s\n        self.m1 = m1",
        "detail": "alignment3D.core3d.models.arcface_torch.losses",
        "documentation": {}
    },
    {
        "label": "ArcFace",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.losses",
        "description": "alignment3D.core3d.models.arcface_torch.losses",
        "peekOfCode": "class ArcFace(torch.nn.Module):\n    \"\"\" ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n    \"\"\"\n    def __init__(self, s=64.0, margin=0.5):\n        super(ArcFace, self).__init__()\n        self.s = s\n        self.margin = margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.theta = math.cos(math.pi - margin)",
        "detail": "alignment3D.core3d.models.arcface_torch.losses",
        "documentation": {}
    },
    {
        "label": "CosFace",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.losses",
        "description": "alignment3D.core3d.models.arcface_torch.losses",
        "peekOfCode": "class CosFace(torch.nn.Module):\n    def __init__(self, s=64.0, m=0.40):\n        super(CosFace, self).__init__()\n        self.s = s\n        self.m = m\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n        index = torch.where(labels != -1)[0]\n        target_logit = logits[index, labels[index].view(-1)]\n        final_target_logit = target_logit - self.m\n        logits[index, labels[index].view(-1)] = final_target_logit",
        "detail": "alignment3D.core3d.models.arcface_torch.losses",
        "documentation": {}
    },
    {
        "label": "PolynomialLRWarmup",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.lr_scheduler",
        "description": "alignment3D.core3d.models.arcface_torch.lr_scheduler",
        "peekOfCode": "class PolynomialLRWarmup(_LRScheduler):\n    def __init__(self, optimizer, warmup_iters, total_iters=5, power=1.0, last_epoch=-1, verbose=False):\n        super().__init__(optimizer, last_epoch=last_epoch, verbose=verbose)\n        self.total_iters = total_iters\n        self.power = power\n        self.warmup_iters = warmup_iters\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)",
        "detail": "alignment3D.core3d.models.arcface_torch.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "ArcFaceORT",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_helper",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_helper",
        "peekOfCode": "class ArcFaceORT:\n    def __init__(self, model_path, cpu=False):\n        self.model_path = model_path\n        # providers = None will use available provider, for onnxruntime-gpu it will be \"CUDAExecutionProvider\"\n        self.providers = ['CPUExecutionProvider'] if cpu else None\n    #input_size is (w,h), return error message, return None if success\n    def check(self, track='cfat', test_img = None):\n        #default is cfat\n        max_model_size_mb=1024\n        max_feat_dim=512",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_helper",
        "documentation": {}
    },
    {
        "label": "AlignedDataSet",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "class AlignedDataSet(mx.gluon.data.Dataset):\n    def __init__(self, root, lines, align=True):\n        self.lines = lines\n        self.root = root\n        self.align = align\n    def __len__(self):\n        return len(self.lines)\n    def __getitem__(self, idx):\n        each_line = self.lines[idx]\n        name_lmk_score = each_line.strip().split(' ')",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "extract",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def extract(model_root, dataset):\n    model = ArcFaceORT(model_path=model_root)\n    model.check()\n    feat_mat = np.zeros(shape=(len(dataset), 2 * model.feat_dim))\n    def collate_fn(data):\n        return torch.cat(data, dim=0)\n    data_loader = DataLoader(\n        dataset, batch_size=128, drop_last=False, num_workers=4, collate_fn=collate_fn, )\n    num_iter = 0\n    for batch in data_loader:",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "read_template_media_list",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def read_template_media_list(path):\n    ijb_meta = pd.read_csv(path, sep=' ', header=None).values\n    templates = ijb_meta[:, 1].astype(np.int)\n    medias = ijb_meta[:, 2].astype(np.int)\n    return templates, medias\ndef read_template_pair_list(path):\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    t1 = pairs[:, 0].astype(np.int)\n    t2 = pairs[:, 1].astype(np.int)\n    label = pairs[:, 2].astype(np.int)",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "read_template_pair_list",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def read_template_pair_list(path):\n    pairs = pd.read_csv(path, sep=' ', header=None).values\n    t1 = pairs[:, 0].astype(np.int)\n    t2 = pairs[:, 1].astype(np.int)\n    label = pairs[:, 2].astype(np.int)\n    return t1, t2, label\ndef read_image_feature(path):\n    with open(path, 'rb') as fid:\n        img_feats = pickle.load(fid)\n    return img_feats",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "read_image_feature",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def read_image_feature(path):\n    with open(path, 'rb') as fid:\n        img_feats = pickle.load(fid)\n    return img_feats\ndef image2template_feature(img_feats=None,\n                           templates=None,\n                           medias=None):\n    unique_templates = np.unique(templates)\n    template_feats = np.zeros((len(unique_templates), img_feats.shape[1]))\n    for count_template, uqt in enumerate(unique_templates):",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "image2template_feature",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def image2template_feature(img_feats=None,\n                           templates=None,\n                           medias=None):\n    unique_templates = np.unique(templates)\n    template_feats = np.zeros((len(unique_templates), img_feats.shape[1]))\n    for count_template, uqt in enumerate(unique_templates):\n        (ind_t,) = np.where(templates == uqt)\n        face_norm_feats = img_feats[ind_t]\n        face_medias = medias[ind_t]\n        unique_medias, unique_media_counts = np.unique(face_medias, return_counts=True)",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "verification",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def verification(template_norm_feats=None,\n                 unique_templates=None,\n                 p1=None,\n                 p2=None):\n    template2id = np.zeros((max(unique_templates) + 1, 1), dtype=int)\n    for count_template, uqt in enumerate(unique_templates):\n        template2id[uqt] = count_template\n    score = np.zeros((len(p1),))\n    total_pairs = np.array(range(len(p1)))\n    batchsize = 100000",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "verification2",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def verification2(template_norm_feats=None,\n                  unique_templates=None,\n                  p1=None,\n                  p2=None):\n    template2id = np.zeros((max(unique_templates) + 1, 1), dtype=int)\n    for count_template, uqt in enumerate(unique_templates):\n        template2id[uqt] = count_template\n    score = np.zeros((len(p1),))  # save cosine distance between pairs\n    total_pairs = np.array(range(len(p1)))\n    batchsize = 100000  # small batchsize instead of all pairs in one batch due to the memory limiation",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "def main(args):\n    use_norm_score = True  # if Ture, TestMode(N1)\n    use_detector_score = True  # if Ture, TestMode(D1)\n    use_flip_test = True  # if Ture, TestMode(F1)\n    assert args.target == 'IJBC' or args.target == 'IJBB'\n    start = timeit.default_timer()\n    templates, medias = read_template_media_list(\n        os.path.join('%s/meta' % args.image_path, '%s_face_tid_mid.txt' % args.target.lower()))\n    stop = timeit.default_timer()\n    print('Time: %.2f s. ' % (stop - start))",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "SRC",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "description": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "peekOfCode": "SRC = np.array(\n    [\n        [30.2946, 51.6963],\n        [65.5318, 51.5014],\n        [48.0252, 71.7366],\n        [33.5493, 92.3655],\n        [62.7299, 92.2041]]\n    , dtype=np.float32)\nSRC[:, 0] += 8.0\n@torch.no_grad()",
        "detail": "alignment3D.core3d.models.arcface_torch.onnx_ijbc",
        "documentation": {}
    },
    {
        "label": "PartialFC_V2",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "description": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "peekOfCode": "class PartialFC_V2(torch.nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2203.15565\n    A distributed sparsely updating variant of the FC layer, named Partial FC (PFC).\n    When sample rate less than 1, in each iteration, positive class centers and a random subset of\n    negative class centers are selected to compute the margin-based softmax loss, all class\n    centers are still maintained throughout the whole training process, but only a subset is\n    selected and updated in each iteration.\n    .. note::\n        When sample rate equal to 1, Partial FC is equal to model parallelism(default sample rate is 1).",
        "detail": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "documentation": {}
    },
    {
        "label": "DistCrossEntropyFunc",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "description": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "peekOfCode": "class DistCrossEntropyFunc(torch.autograd.Function):\n    \"\"\"\n    CrossEntropy loss is calculated in parallel, allreduce denominator into single gpu and calculate softmax.\n    Implemented of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n    \"\"\"\n    @staticmethod\n    def forward(ctx, logits: torch.Tensor, label: torch.Tensor):\n        \"\"\" \"\"\"\n        batch_size = logits.size(0)\n        # for numerical stability",
        "detail": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "documentation": {}
    },
    {
        "label": "DistCrossEntropy",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "description": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "peekOfCode": "class DistCrossEntropy(torch.nn.Module):\n    def __init__(self):\n        super(DistCrossEntropy, self).__init__()\n    def forward(self, logit_part, label_part):\n        return DistCrossEntropyFunc.apply(logit_part, label_part)\nclass AllGatherFunc(torch.autograd.Function):\n    \"\"\"AllGather op with gradient backward\"\"\"\n    @staticmethod\n    def forward(ctx, tensor, *gather_list):\n        gather_list = list(gather_list)",
        "detail": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "documentation": {}
    },
    {
        "label": "AllGatherFunc",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "description": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "peekOfCode": "class AllGatherFunc(torch.autograd.Function):\n    \"\"\"AllGather op with gradient backward\"\"\"\n    @staticmethod\n    def forward(ctx, tensor, *gather_list):\n        gather_list = list(gather_list)\n        distributed.all_gather(gather_list, tensor)\n        return tuple(gather_list)\n    @staticmethod\n    def backward(ctx, *grads):\n        grad_list = list(grads)",
        "detail": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "documentation": {}
    },
    {
        "label": "AllGather",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "description": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "peekOfCode": "AllGather = AllGatherFunc.apply",
        "detail": "alignment3D.core3d.models.arcface_torch.partial_fc_v2",
        "documentation": {}
    },
    {
        "label": "convert_onnx",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.torch2onnx",
        "description": "alignment3D.core3d.models.arcface_torch.torch2onnx",
        "peekOfCode": "def convert_onnx(net, path_module, output, opset=11, simplify=False):\n    assert isinstance(net, torch.nn.Module)\n    img = np.random.randint(0, 255, size=(112, 112, 3), dtype=np.int32)\n    img = img.astype(np.float)\n    img = (img / 255. - 0.5) / 0.5  # torch style norm\n    img = img.transpose((2, 0, 1))\n    img = torch.from_numpy(img).unsqueeze(0).float()\n    weight = torch.load(path_module)\n    net.load_state_dict(weight, strict=True)\n    net.eval()",
        "detail": "alignment3D.core3d.models.arcface_torch.torch2onnx",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.arcface_torch.train_v2",
        "description": "alignment3D.core3d.models.arcface_torch.train_v2",
        "peekOfCode": "def main(args):\n    # get config\n    cfg = get_config(args.config)\n    # global control random seed\n    setup_seed(seed=cfg.seed, cuda_deterministic=False)\n    torch.cuda.set_device(local_rank)\n    os.makedirs(cfg.output, exist_ok=True)\n    init_logging(rank, cfg.output)\n    summary_writer = (\n        SummaryWriter(log_dir=os.path.join(cfg.output, \"tensorboard\"))",
        "detail": "alignment3D.core3d.models.arcface_torch.train_v2",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.base_model",
        "description": "alignment3D.core3d.models.base_model",
        "peekOfCode": "class BaseModel(ABC):\n    \"\"\"This class is an abstract base class (ABC) for models.\n    To create a subclass, you need to implement the following five functions:\n        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n        -- <set_input>:                     unpack data from dataset and apply preprocessing.\n        -- <forward>:                       produce intermediate results.\n        -- <optimize_parameters>:           calculate losses, gradients, and update network weights.\n        -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n    \"\"\"\n    def __init__(self, opt):",
        "detail": "alignment3D.core3d.models.base_model",
        "documentation": {}
    },
    {
        "label": "SH",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.bfm",
        "description": "alignment3D.core3d.models.bfm",
        "peekOfCode": "class SH:\n    def __init__(self):\n        self.a = [np.pi, 2 * np.pi / np.sqrt(3.), 2 * np.pi / np.sqrt(8.)]\n        self.c = [1/np.sqrt(4 * np.pi), np.sqrt(3.) / np.sqrt(4 * np.pi), 3 * np.sqrt(5.) / np.sqrt(12 * np.pi)]\nclass ParametricFaceModel:\n    def __init__(self, \n                bfm_folder='./BFM', \n                recenter=True,\n                camera_distance=10.,\n                init_lit=np.array([",
        "detail": "alignment3D.core3d.models.bfm",
        "documentation": {}
    },
    {
        "label": "ParametricFaceModel",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.bfm",
        "description": "alignment3D.core3d.models.bfm",
        "peekOfCode": "class ParametricFaceModel:\n    def __init__(self, \n                bfm_folder='./BFM', \n                recenter=True,\n                camera_distance=10.,\n                init_lit=np.array([\n                    0.8, 0, 0, 0, 0, 0, 0, 0, 0\n                    ]),\n                focal=1015.,\n                center=112.,",
        "detail": "alignment3D.core3d.models.bfm",
        "documentation": {}
    },
    {
        "label": "perspective_projection",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.bfm",
        "description": "alignment3D.core3d.models.bfm",
        "peekOfCode": "def perspective_projection(focal, center):\n    # return p.T (N, 3) @ (3, 3) \n    return np.array([\n        focal, 0, center,\n        0, focal, center,\n        0, 0, 1\n    ]).reshape([3, 3]).astype(np.float32).transpose()\nclass SH:\n    def __init__(self):\n        self.a = [np.pi, 2 * np.pi / np.sqrt(3.), 2 * np.pi / np.sqrt(8.)]",
        "detail": "alignment3D.core3d.models.bfm",
        "documentation": {}
    },
    {
        "label": "FaceReconModel",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.facerecon_model",
        "description": "alignment3D.core3d.models.facerecon_model",
        "peekOfCode": "class FaceReconModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        \"\"\"  Configures options specific for CUT model\n        \"\"\"\n        # net structure and parameters\n        parser.add_argument('--net_recon', type=str, default='resnet50', choices=['resnet18', 'resnet34', 'resnet50'], help='network structure')\n        parser.add_argument('--init_path', type=str, default='checkpoints/init_model/resnet50-0676ba61.pth')\n        parser.add_argument('--use_last_fc', type=util.str2bool, nargs='?', const=True, default=False, help='zero initialize the last fc')\n        parser.add_argument('--bfm_folder', type=str, default='BFM')",
        "detail": "alignment3D.core3d.models.facerecon_model",
        "documentation": {}
    },
    {
        "label": "PerceptualLoss",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.losses",
        "description": "alignment3D.core3d.models.losses",
        "peekOfCode": "class PerceptualLoss(nn.Module):\n    def __init__(self, recog_net, input_size=112):\n        super(PerceptualLoss, self).__init__()\n        self.recog_net = recog_net\n        self.preprocess = lambda x: 2 * x - 1\n        self.input_size=input_size\n    def forward(imageA, imageB, M):\n        \"\"\"\n        1 - cosine distance\n        Parameters:",
        "detail": "alignment3D.core3d.models.losses",
        "documentation": {}
    },
    {
        "label": "resize_n_crop",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.losses",
        "description": "alignment3D.core3d.models.losses",
        "peekOfCode": "def resize_n_crop(image, M, dsize=112):\n    # image: (b, c, h, w)\n    # M   :  (b, 2, 3)\n    return warp_affine(image, M, dsize=(dsize, dsize))\n### perceptual level loss\nclass PerceptualLoss(nn.Module):\n    def __init__(self, recog_net, input_size=112):\n        super(PerceptualLoss, self).__init__()\n        self.recog_net = recog_net\n        self.preprocess = lambda x: 2 * x - 1",
        "detail": "alignment3D.core3d.models.losses",
        "documentation": {}
    },
    {
        "label": "perceptual_loss",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.losses",
        "description": "alignment3D.core3d.models.losses",
        "peekOfCode": "def perceptual_loss(id_featureA, id_featureB):\n    cosine_d = torch.sum(id_featureA * id_featureB, dim=-1)\n        # assert torch.sum((cosine_d > 1).float()) == 0\n    return torch.sum(1 - cosine_d) / cosine_d.shape[0]  \n### image level loss\ndef photo_loss(imageA, imageB, mask, eps=1e-6):\n    \"\"\"\n    l2 norm (with sqrt, to ensure backward stabililty, use eps, otherwise Nan may occur)\n    Parameters:\n        imageA       --torch.tensor (B, 3, H, W), range (0, 1), RGB order ",
        "detail": "alignment3D.core3d.models.losses",
        "documentation": {}
    },
    {
        "label": "photo_loss",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.losses",
        "description": "alignment3D.core3d.models.losses",
        "peekOfCode": "def photo_loss(imageA, imageB, mask, eps=1e-6):\n    \"\"\"\n    l2 norm (with sqrt, to ensure backward stabililty, use eps, otherwise Nan may occur)\n    Parameters:\n        imageA       --torch.tensor (B, 3, H, W), range (0, 1), RGB order \n        imageB       --same as imageA\n    \"\"\"\n    loss = torch.sqrt(eps + torch.sum((imageA - imageB) ** 2, dim=1, keepdims=True)) * mask\n    loss = torch.sum(loss) / torch.max(torch.sum(mask), torch.tensor(1.0).to(mask.device))\n    return loss",
        "detail": "alignment3D.core3d.models.losses",
        "documentation": {}
    },
    {
        "label": "landmark_loss",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.losses",
        "description": "alignment3D.core3d.models.losses",
        "peekOfCode": "def landmark_loss(predict_lm, gt_lm, weight=None):\n    \"\"\"\n    weighted mse loss\n    Parameters:\n        predict_lm    --torch.tensor (B, 68, 2)\n        gt_lm         --torch.tensor (B, 68, 2)\n        weight        --numpy.array (1, 68)\n    \"\"\"\n    if not weight:\n        weight = np.ones([68])",
        "detail": "alignment3D.core3d.models.losses",
        "documentation": {}
    },
    {
        "label": "reg_loss",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.losses",
        "description": "alignment3D.core3d.models.losses",
        "peekOfCode": "def reg_loss(coeffs_dict, opt=None):\n    \"\"\"\n    l2 norm without the sqrt, from yu's implementation (mse)\n    tf.nn.l2_loss https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss\n    Parameters:\n        coeffs_dict     -- a  dict of torch.tensors , keys: id, exp, tex, angle, gamma, trans\n    \"\"\"\n    # coefficient regularization to ensure plausible 3d faces\n    if opt:\n        w_id, w_exp, w_tex = opt.w_id, opt.w_exp, opt.w_tex",
        "detail": "alignment3D.core3d.models.losses",
        "documentation": {}
    },
    {
        "label": "reflectance_loss",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.losses",
        "description": "alignment3D.core3d.models.losses",
        "peekOfCode": "def reflectance_loss(texture, mask):\n    \"\"\"\n    minimize texture variance (mse), albedo regularization to ensure an uniform skin albedo\n    Parameters:\n        texture       --torch.tensor, (B, N, 3)\n        mask          --torch.tensor, (N), 1 or 0\n    \"\"\"\n    mask = mask.reshape([1, mask.shape[0], 1])\n    texture_mean = torch.sum(mask * texture, dim=1, keepdims=True) / torch.sum(mask)\n    loss = torch.sum(((texture - texture_mean) * mask)**2) / (texture.shape[0] * torch.sum(mask))",
        "detail": "alignment3D.core3d.models.losses",
        "documentation": {}
    },
    {
        "label": "ReconNetWrapper",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "class ReconNetWrapper(nn.Module):\n    fc_dim=257\n    def __init__(self, net_recon, use_last_fc=False, init_path=None):\n        super(ReconNetWrapper, self).__init__()\n        self.use_last_fc = use_last_fc\n        if net_recon not in func_dict:\n            return  NotImplementedError('network [%s] is not implemented', net_recon)\n        func, last_dim = func_dict[net_recon]\n        backbone = func(use_last_fc=use_last_fc, num_classes=self.fc_dim)\n        if init_path and os.path.isfile(init_path):",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "RecogNetWrapper",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "class RecogNetWrapper(nn.Module):\n    def __init__(self, net_recog, pretrained_path=None, input_size=112):\n        super(RecogNetWrapper, self).__init__()\n        net = get_model(name=net_recog, fp16=False)\n        if pretrained_path:\n            state_dict = torch.load(pretrained_path, map_location='cpu')\n            net.load_state_dict(state_dict)\n            print(\"loading pretrained net_recog %s from %s\" %(net_recog, pretrained_path))\n        for param in net.parameters():\n            param.requires_grad = False",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "class Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n    expansion: int = 4\n    def __init__(\n        self,\n        inplanes: int,",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "class ResNet(nn.Module):\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 1000,\n        zero_init_residual: bool = False,\n        use_last_fc: bool = False,\n        groups: int = 1,\n        width_per_group: int = 64,",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resize_n_crop",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resize_n_crop(image, M, dsize=112):\n    # image: (b, c, h, w)\n    # M   :  (b, 2, 3)\n    return warp_affine(image, M, dsize=(dsize, dsize))\ndef filter_state_dict(state_dict, remove_name='fc'):\n    new_state_dict = {}\n    for key in state_dict:\n        if remove_name in key:\n            continue\n        new_state_dict[key] = state_dict[key]",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "filter_state_dict",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def filter_state_dict(state_dict, remove_name='fc'):\n    new_state_dict = {}\n    for key in state_dict:\n        if remove_name in key:\n            continue\n        new_state_dict[key] = state_dict[key]\n    return new_state_dict\ndef get_scheduler(optimizer, opt):\n    \"\"\"Return a learning rate scheduler\n    Parameters:",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "get_scheduler",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def get_scheduler(optimizer, opt):\n    \"\"\"Return a learning rate scheduler\n    Parameters:\n        optimizer          -- the optimizer of the network\n        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　\n                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n    See https://pytorch.org/docs/stable/optim.html for more details.\n    \"\"\"\n    if opt.lr_policy == 'linear':",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "define_net_recon",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def define_net_recon(net_recon, use_last_fc=False, init_path=None):\n    return ReconNetWrapper(net_recon, use_last_fc=use_last_fc, init_path=init_path)\ndef define_net_recog(net_recog, pretrained_path=None):\n    net = RecogNetWrapper(net_recog=net_recog, pretrained_path=pretrained_path)\n    net.eval()\n    return net\nclass ReconNetWrapper(nn.Module):\n    fc_dim=257\n    def __init__(self, net_recon, use_last_fc=False, init_path=None):\n        super(ReconNetWrapper, self).__init__()",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "define_net_recog",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def define_net_recog(net_recog, pretrained_path=None):\n    net = RecogNetWrapper(net_recog=net_recog, pretrained_path=pretrained_path)\n    net.eval()\n    return net\nclass ReconNetWrapper(nn.Module):\n    fc_dim=257\n    def __init__(self, net_recon, use_last_fc=False, init_path=None):\n        super(ReconNetWrapper, self).__init__()\n        self.use_last_fc = use_last_fc\n        if net_recon not in func_dict:",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1, bias: bool = False) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def conv1x1(in_planes: int, out_planes: int, stride: int = 1, bias: bool = False) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resnet18",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-18 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\ndef resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resnet34",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-34 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\ndef resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resnet50",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-50 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\ndef resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resnet101",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-101 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n                   **kwargs)\ndef resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resnet152",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-152 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)\ndef resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resnext50_32x4d",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNeXt-50 32x4d model from\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    kwargs['groups'] = 32\n    kwargs['width_per_group'] = 4\n    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "resnext101_32x8d",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNeXt-101 32x8d model from\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    kwargs['groups'] = 32\n    kwargs['width_per_group'] = 8\n    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "wide_resnet50_2",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"Wide ResNet-50-2 model from\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "wide_resnet101_2",
        "kind": 2,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"Wide ResNet-101-2 model from\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n           'wide_resnet50_2', 'wide_resnet101_2']\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',\n    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "model_urls",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "model_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',\n    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "func_dict",
        "kind": 5,
        "importPath": "alignment3D.core3d.models.networks",
        "description": "alignment3D.core3d.models.networks",
        "peekOfCode": "func_dict = {\n    'resnet18': (resnet18, 512),\n    'resnet50': (resnet50, 2048)\n}",
        "detail": "alignment3D.core3d.models.networks",
        "documentation": {}
    },
    {
        "label": "TemplateModel",
        "kind": 6,
        "importPath": "alignment3D.core3d.models.template_model",
        "description": "alignment3D.core3d.models.template_model",
        "peekOfCode": "class TemplateModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        \"\"\"Add new model-specific options and rewrite default values for existing options.\n        Parameters:\n            parser -- the option parser\n            is_train -- if it is training phase or test phase. You can use this flag to add training-specific or test-specific options.\n        Returns:\n            the modified parser.\n        \"\"\"",
        "detail": "alignment3D.core3d.models.template_model",
        "documentation": {}
    },
    {
        "label": "Deep3DReconstructor",
        "kind": 6,
        "importPath": "alignment3D.core3d.deep_3d_recon",
        "description": "alignment3D.core3d.deep_3d_recon",
        "peekOfCode": "class Deep3DReconstructor:\n    \"\"\"\n    A wrapper for the Deep3DFaceRecon_pytorch model to handle 3D face reconstruction.\n    \"\"\"\n    def __init__(self, checkpoint_path: Path, bfm_path: Path):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.net_recon = self._load_model(checkpoint_path).to(self.device)\n        self.facemodel = self._load_bfm(bfm_path)\n    def _load_model(self, checkpoint_path: Path):\n        \"\"\"Loads the pre-trained PyTorch model.\"\"\"",
        "detail": "alignment3D.core3d.deep_3d_recon",
        "documentation": {}
    },
    {
        "label": "FaceDetector",
        "kind": 6,
        "importPath": "alignment3D.core3d.face_detector3d",
        "description": "alignment3D.core3d.face_detector3d",
        "peekOfCode": "class FaceDetector:\n    \"\"\"A wrapper class for the insightface face detection model.\"\"\"\n    def __init__(self):\n        self.model = None\n    def load_model(self):\n        \"\"\"Loads the face analysis model.\"\"\"\n        self.model = insightface.app.FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n        self.model.prepare(ctx_id=0, det_size=(640, 640))\n    def get_face(self, img: np.ndarray):\n        \"\"\"",
        "detail": "alignment3D.core3d.face_detector3d",
        "documentation": {}
    },
    {
        "label": "MeshGenerator",
        "kind": 6,
        "importPath": "alignment3D.core3d.mesh_generator3d",
        "description": "alignment3D.core3d.mesh_generator3d",
        "peekOfCode": "class MeshGenerator:\n    \"\"\"\n    Wraps the 3DDFA_V2 pipeline to:\n      1) detect a face\n      2) reconstruct a dense 3D mesh\n      3) render it as a semi-transparent overlay\n      4) save to disk, returning the saved JPEG path\n    \"\"\"\n    def __init__(self,\n                 config_file: str = \"configs/mb1_120x120.yml\",",
        "detail": "alignment3D.core3d.mesh_generator3d",
        "documentation": {}
    },
    {
        "label": "get_mesh_data",
        "kind": 2,
        "importPath": "alignment3D.core3d.mesh_generator3d",
        "description": "alignment3D.core3d.mesh_generator3d",
        "peekOfCode": "def get_mesh_data(self, image_path: str) -> dict:\n    \"\"\"\n    Extract mesh data (vertices, faces, UVs, normals) from an image.\n    Returns a dictionary with mesh data for animation export.\n    \"\"\"\n    try:\n        # Load and preprocess image\n        image = cv2.imread(image_path)\n        if image is None:\n            raise FileNotFoundError(f\"Could not load image: {image_path}\")",
        "detail": "alignment3D.core3d.mesh_generator3d",
        "documentation": {}
    },
    {
        "label": "integrate_with_existing_pipeline",
        "kind": 2,
        "importPath": "alignment3D.core3d.mesh_generator3d",
        "description": "alignment3D.core3d.mesh_generator3d",
        "peekOfCode": "def integrate_with_existing_pipeline(self, image_path: str):\n    \"\"\"\n    This method should integrate with your existing 3D face reconstruction pipeline.\n    Replace the contents with calls to your actual 3D reconstruction methods.\n    \"\"\"\n    try:\n        # Example integration points (replace with your actual methods):\n        # 1. If you're using 3DDFA_V2 or similar:\n        # from core3d.face_reconstruction import FaceReconstructor\n        # reconstructor = FaceReconstructor()",
        "detail": "alignment3D.core3d.mesh_generator3d",
        "documentation": {}
    },
    {
        "label": "manual_blend",
        "kind": 2,
        "importPath": "alignment3D.core3d.processors3d",
        "description": "alignment3D.core3d.processors3d",
        "peekOfCode": "def manual_blend(original: np.ndarray, swapped: np.ndarray, bbox: np.ndarray, alpha: float, softness: float, custom_mask: np.ndarray = None) -> np.ndarray:\n    \"\"\"\n    Blends the 'swapped' image into the 'original' image.\n    Uses a custom mask if provided, otherwise creates a default elliptical mask.\n    \"\"\"\n    x1, y1, x2, y2 = bbox.astype(int)\n    h, w = y2 - y1, x2 - x1\n    if custom_mask is not None and w > 0 and h > 0:\n        # The custom_mask is a \"face-relative\" template.\n        # We resize it to fit the bounding box of the face in the current frame.",
        "detail": "alignment3D.core3d.processors3d",
        "documentation": {}
    },
    {
        "label": "apply_color_correction",
        "kind": 2,
        "importPath": "alignment3D.core3d.processors3d",
        "description": "alignment3D.core3d.processors3d",
        "peekOfCode": "def apply_color_correction(original_img: np.ndarray, swapped_face_img: np.ndarray, target_face_bbox: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Applies color correction to a swapped face by matching the color distribution\n    of the original face in the target image.\n    \"\"\"\n    x1, y1, x2, y2 = target_face_bbox.astype(int)\n    original_face = original_img[y1:y2, x1:x2]\n    swapped_face = swapped_face_img[y1:y2, x1:x2]\n    original_face_lab = cv2.cvtColor(original_face, cv2.COLOR_BGR2LAB)\n    swapped_face_lab = cv2.cvtColor(swapped_face, cv2.COLOR_BGR2LAB)",
        "detail": "alignment3D.core3d.processors3d",
        "documentation": {}
    },
    {
        "label": "extract_frames",
        "kind": 2,
        "importPath": "alignment3D.core3d.video_utils3d",
        "description": "alignment3D.core3d.video_utils3d",
        "peekOfCode": "def extract_frames(video_path: Path, output_dir: Path, progress_callback=None):\n    \"\"\"\n    Extracts all frames from a video file and saves them to a directory.\n    Returns:\n        Tuple[float, int]: The original FPS and the total number of frames.\n    \"\"\"\n    if progress_callback: progress_callback(f\"🎞️ Extracting frames from video...\")\n    cap = cv2.VideoCapture(str(video_path))\n    if not cap.isOpened():\n        raise FileNotFoundError(f\"Cannot open video file: {video_path}\")",
        "detail": "alignment3D.core3d.video_utils3d",
        "documentation": {}
    },
    {
        "label": "reconstruct_video",
        "kind": 2,
        "importPath": "alignment3D.core3d.video_utils3d",
        "description": "alignment3D.core3d.video_utils3d",
        "peekOfCode": "def reconstruct_video(frames_dir: Path, output_path: Path, fps: float, progress_callback=None):\n    \"\"\"\n    Stitches all frames from a directory back into a video file.\n    \"\"\"\n    if progress_callback: progress_callback(\"🎬 Reconstructing video...\")\n    frame_paths = sorted(glob.glob(str(frames_dir / '*.jpg')))\n    if not frame_paths:\n        raise FileNotFoundError(\"No frames found to reconstruct.\")\n    first_frame = cv2.imread(frame_paths[0])\n    height, width, _ = first_frame.shape",
        "detail": "alignment3D.core3d.video_utils3d",
        "documentation": {}
    },
    {
        "label": "BFMModel",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.bfm.bfm",
        "description": "alignment3D.DDFA_V2.bfm.bfm",
        "peekOfCode": "class BFMModel(object):\n    def __init__(self, bfm_fp, shape_dim=40, exp_dim=10):\n        bfm = _load(bfm_fp)\n        self.u = bfm.get('u').astype(np.float32)  # fix bug\n        self.w_shp = bfm.get('w_shp').astype(np.float32)[..., :shape_dim]\n        self.w_exp = bfm.get('w_exp').astype(np.float32)[..., :exp_dim]\n        if osp.split(bfm_fp)[-1] == 'bfm_noneck_v3.pkl':\n            self.tri = _load(make_abs_path('../configs/tri.pkl'))  # this tri/face is re-built for bfm_noneck_v3\n        else:\n            self.tri = bfm.get('tri')",
        "detail": "alignment3D.DDFA_V2.bfm.bfm",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.bfm.bfm",
        "description": "alignment3D.DDFA_V2.bfm.bfm",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport os.path as osp\nimport numpy as np\nfrom utils.io import _load\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\ndef _to_ctype(arr):\n    if not arr.flags.c_contiguous:\n        return arr.copy(order='C')",
        "detail": "alignment3D.DDFA_V2.bfm.bfm",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.bfm.bfm",
        "description": "alignment3D.DDFA_V2.bfm.bfm",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\ndef _to_ctype(arr):\n    if not arr.flags.c_contiguous:\n        return arr.copy(order='C')\n    return arr\nclass BFMModel(object):\n    def __init__(self, bfm_fp, shape_dim=40, exp_dim=10):\n        bfm = _load(bfm_fp)\n        self.u = bfm.get('u').astype(np.float32)  # fix bug\n        self.w_shp = bfm.get('w_shp').astype(np.float32)[..., :shape_dim]",
        "detail": "alignment3D.DDFA_V2.bfm.bfm",
        "documentation": {}
    },
    {
        "label": "BFMModel_ONNX",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "description": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "peekOfCode": "class BFMModel_ONNX(nn.Module):\n    \"\"\"BFM serves as a decoder\"\"\"\n    def __init__(self, bfm_fp, shape_dim=40, exp_dim=10):\n        super(BFMModel_ONNX, self).__init__()\n        _to_tensor = _numpy_to_tensor\n        # load bfm\n        bfm = _load(bfm_fp)\n        u = _to_tensor(bfm.get('u').astype(np.float32))\n        self.u = u.view(-1, 3).transpose(1, 0)\n        w_shp = _to_tensor(bfm.get('w_shp').astype(np.float32)[..., :shape_dim])",
        "detail": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "documentation": {}
    },
    {
        "label": "convert_bfm_to_onnx",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "description": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "peekOfCode": "def convert_bfm_to_onnx(bfm_onnx_fp, shape_dim=40, exp_dim=10):\n    # print(shape_dim, exp_dim)\n    bfm_fp = bfm_onnx_fp.replace('.onnx', '.pkl')\n    bfm_decoder = BFMModel_ONNX(bfm_fp=bfm_fp, shape_dim=shape_dim, exp_dim=exp_dim)\n    bfm_decoder.eval()\n    # dummy_input = torch.randn(12 + shape_dim + exp_dim)\n    dummy_input = torch.randn(3, 3), torch.randn(3, 1), torch.randn(shape_dim, 1), torch.randn(exp_dim, 1)\n    R, offset, alpha_shp, alpha_exp = dummy_input\n    torch.onnx.export(\n        bfm_decoder,",
        "detail": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "description": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport os.path as osp\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom utils.io import _load, _numpy_to_cuda, _numpy_to_tensor\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\ndef _to_ctype(arr):",
        "detail": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "description": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\ndef _to_ctype(arr):\n    if not arr.flags.c_contiguous:\n        return arr.copy(order='C')\n    return arr\ndef _load_tri(bfm_fp):\n    if osp.split(bfm_fp)[-1] == 'bfm_noneck_v3.pkl':\n        tri = _load(make_abs_path('../configs/tri.pkl'))  # this tri/face is re-built for bfm_noneck_v3\n    else:\n        tri = _load(bfm_fp).get('tri')",
        "detail": "alignment3D.DDFA_V2.bfm.bfm_onnx",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "peekOfCode": "class BasicConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\nclass Inception(nn.Module):",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "documentation": {}
    },
    {
        "label": "Inception",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "peekOfCode": "class Inception(nn.Module):\n    def __init__(self):\n        super(Inception, self).__init__()\n        self.branch1x1 = BasicConv2d(128, 32, kernel_size=1, padding=0)\n        self.branch1x1_2 = BasicConv2d(128, 32, kernel_size=1, padding=0)\n        self.branch3x3_reduce = BasicConv2d(128, 24, kernel_size=1, padding=0)\n        self.branch3x3 = BasicConv2d(24, 32, kernel_size=3, padding=1)\n        self.branch3x3_reduce_2 = BasicConv2d(128, 24, kernel_size=1, padding=0)\n        self.branch3x3_2 = BasicConv2d(24, 32, kernel_size=3, padding=1)\n        self.branch3x3_3 = BasicConv2d(32, 32, kernel_size=3, padding=1)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "documentation": {}
    },
    {
        "label": "CRelu",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "peekOfCode": "class CRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(CRelu, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = torch.cat([x, -x], 1)\n        x = F.relu(x, inplace=True)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "documentation": {}
    },
    {
        "label": "FaceBoxesNet",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "peekOfCode": "class FaceBoxesNet(nn.Module):\n    def __init__(self, phase, size, num_classes):\n        super(FaceBoxesNet, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.size = size\n        self.conv1 = CRelu(3, 24, kernel_size=7, stride=4, padding=3)\n        self.conv2 = CRelu(48, 64, kernel_size=5, stride=2, padding=2)\n        self.inception1 = Inception()\n        self.inception2 = Inception()",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.models.faceboxes",
        "documentation": {}
    },
    {
        "label": "py_cpu_nms",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.nms.py_cpu_nms",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.nms.py_cpu_nms",
        "peekOfCode": "def py_cpu_nms(dets, thresh):\n    \"\"\"Pure Python NMS baseline.\"\"\"\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n    keep = []",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.nms.py_cpu_nms",
        "documentation": {}
    },
    {
        "label": "extensions",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.nms.setup",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.nms.setup",
        "peekOfCode": "extensions = [\n    Extension(\n        name=\"cpu_nms\",\n        sources=[\"cpu_nms.pyx\"],\n        include_dirs=[np.get_include()],\n        extra_compile_args=[\"-O3\"],\n    )\n]\nsetup(\n    name=\"cpu_nms\",",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.nms.setup",
        "documentation": {}
    },
    {
        "label": "point_form",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def point_form(boxes):\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat((boxes[:, :2] - boxes[:, 2:] / 2,  # xmin, ymin\n                      boxes[:, :2] + boxes[:, 2:] / 2), 1)  # xmax, ymax",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "center_size",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def center_size(boxes):\n    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat((boxes[:, 2:] + boxes[:, :2]) / 2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "intersect",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def intersect(box_a, box_b):\n    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "jaccard",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "matrix_iou",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def matrix_iou(a, b):\n    \"\"\"\n    return iou of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "matrix_iof",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def matrix_iof(a, b):\n    \"\"\"\n    return iof of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    return area_i / np.maximum(area_a[:, np.newaxis], 1)\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "match",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "encode",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def encode(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "decode",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def decode(loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "log_sum_exp",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def log_sum_exp(x):\n    \"\"\"Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    \"\"\"\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x - x_max), 1, keepdim=True)) + x_max\n# Original author: Francisco Massa:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "nms",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "peekOfCode": "def nms(boxes, scores, overlap=0.5, top_k=200):\n    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.box_utils",
        "documentation": {}
    },
    {
        "label": "custom_build_ext",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "peekOfCode": "class custom_build_ext(build_ext):\n    def build_extensions(self):\n        # customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\next_modules = [\n    Extension(\n        \"nms.cpu_nms\",\n        [\"nms/cpu_nms.pyx\"],\n        # extra_compile_args={'gcc': [\"-Wno-cpp\", \"-Wno-unused-function\"]},\n        extra_compile_args=[\"-Wno-cpp\", \"-Wno-unused-function\"],",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "documentation": {}
    },
    {
        "label": "find_in_path",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "peekOfCode": "def find_in_path(name, path):\n    \"Find a file in a search path\"\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "documentation": {}
    },
    {
        "label": "ext_modules",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "peekOfCode": "ext_modules = [\n    Extension(\n        \"nms.cpu_nms\",\n        [\"nms/cpu_nms.pyx\"],\n        # extra_compile_args={'gcc': [\"-Wno-cpp\", \"-Wno-unused-function\"]},\n        extra_compile_args=[\"-Wno-cpp\", \"-Wno-unused-function\"],\n        include_dirs=[numpy_include]\n    )\n]\nsetup(",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.build",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.config",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.config",
        "peekOfCode": "cfg = {\n    'name': 'FaceBoxes',\n    'min_sizes': [[32, 64, 128], [256], [512]],\n    'steps': [32, 64, 128],\n    'variance': [0.1, 0.2],\n    'clip': False\n}",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.config",
        "documentation": {}
    },
    {
        "label": "check_keys",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "peekOfCode": "def check_keys(model, pretrained_state_dict):\n    ckpt_keys = set(pretrained_state_dict.keys())\n    model_keys = set(model.state_dict().keys())\n    used_pretrained_keys = model_keys & ckpt_keys\n    unused_pretrained_keys = ckpt_keys - model_keys\n    missing_keys = model_keys - ckpt_keys\n    # print('Missing keys:{}'.format(len(missing_keys)))\n    # print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n    # print('Used keys:{}'.format(len(used_pretrained_keys)))\n    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "documentation": {}
    },
    {
        "label": "remove_prefix",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "peekOfCode": "def remove_prefix(state_dict, prefix):\n    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n    # print('remove prefix \\'{}\\''.format(prefix))\n    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n    return {f(key): value for key, value in state_dict.items()}\ndef load_model(model, pretrained_path, load_to_cpu):\n    if not osp.isfile(pretrained_path):\n        print(f'The pre-trained FaceBoxes model {pretrained_path} does not exist')\n        sys.exit('-1')\n    # print('Loading pretrained model from {}'.format(pretrained_path))",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "peekOfCode": "def load_model(model, pretrained_path, load_to_cpu):\n    if not osp.isfile(pretrained_path):\n        print(f'The pre-trained FaceBoxes model {pretrained_path} does not exist')\n        sys.exit('-1')\n    # print('Loading pretrained model from {}'.format(pretrained_path))\n    if load_to_cpu:\n        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n    else:\n        device = torch.cuda.current_device()\n        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.functions",
        "documentation": {}
    },
    {
        "label": "nms",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.nms_wrapper",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.nms_wrapper",
        "peekOfCode": "def nms(dets, thresh):\n    \"\"\"Dispatch to either CPU or GPU NMS implementations.\"\"\"\n    if dets.shape[0] == 0:\n        return []\n    return cpu_nms(dets, thresh)\n    # return gpu_nms(dets, thresh)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.nms_wrapper",
        "documentation": {}
    },
    {
        "label": "PriorBox",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.prior_box",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.prior_box",
        "peekOfCode": "class PriorBox(object):\n    def __init__(self, image_size=None):\n        super(PriorBox, self).__init__()\n        # self.aspect_ratios = cfg['aspect_ratios']\n        self.min_sizes = cfg['min_sizes']\n        self.steps = cfg['steps']\n        self.clip = cfg['clip']\n        self.image_size = image_size\n        self.feature_maps = [[ceil(self.image_size[0] / step), ceil(self.image_size[1] / step)] for step in self.steps]\n    def forward(self):",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.prior_box",
        "documentation": {}
    },
    {
        "label": "Timer",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.utils.timer",
        "description": "alignment3D.DDFA_V2.FaceBoxes.utils.timer",
        "peekOfCode": "class Timer(object):\n    \"\"\"A simple timer.\"\"\"\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.utils.timer",
        "documentation": {}
    },
    {
        "label": "FaceBoxes",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "class FaceBoxes:\n    def __init__(self, timer_flag=False):\n        torch.set_grad_enabled(False)\n        net = FaceBoxesNet(phase='test', size=None, num_classes=2)  # initialize detector\n        self.net = load_model(net, pretrained_path=pretrained_path, load_to_cpu=True)\n        self.net.eval()\n        # print('Finished loading model!')\n        self.timer_flag = timer_flag\n    def __call__(self, img_):\n        img_raw = img_.copy()",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "viz_bbox",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "def viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n        cx = b[0]\n        cy = b[1] + 12",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "def main():\n    face_boxes = FaceBoxes(timer_flag=True)\n    fn = 'trump_hillary.jpg'\n    img_fp = f'../examples/inputs/{fn}'\n    img = cv2.imread(img_fp)\n    print(f'input shape: {img.shape}')\n    dets = face_boxes(img)  # xmin, ymin, w, h\n    # print(dets)\n    # repeating inference for `n` times\n    n = 10",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "confidence_threshold",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "confidence_threshold = 0.05\ntop_k = 5000\nkeep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "top_k",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "top_k = 5000\nkeep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "keep_top_k",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "keep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "nms_threshold",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "nms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "vis_thres",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "vis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "resize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "scale_flag",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "scale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "pretrained_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "peekOfCode": "pretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n        cx = b[0]",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes",
        "documentation": {}
    },
    {
        "label": "FaceBoxes_ONNX",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "class FaceBoxes_ONNX(object):\n    def __init__(self, timer_flag=False):\n        if not osp.exists(onnx_path):\n            convert_to_onnx(onnx_path)\n        self.session = onnxruntime.InferenceSession(onnx_path, None)\n        self.timer_flag = timer_flag\n    def __call__(self, img_):\n        img_raw = img_.copy()\n        # scaling to speed up\n        scale = 1",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "viz_bbox",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "def viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n        cx = b[0]\n        cy = b[1] + 12",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "def main():\n    face_boxes = FaceBoxes_ONNX(timer_flag=True)\n    fn = 'trump_hillary.jpg'\n    img_fp = f'../examples/inputs/{fn}'\n    img = cv2.imread(img_fp)\n    print(f'input shape: {img.shape}')\n    dets = face_boxes(img)  # xmin, ymin, w, h\n    # print(dets)\n    # repeating inference for `n` times\n    n = 10",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "confidence_threshold",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "confidence_threshold = 0.05\ntop_k = 5000\nkeep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "top_k",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "top_k = 5000\nkeep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "keep_top_k",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "keep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "nms_threshold",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "nms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "vis_thres",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "vis_thres = 0.5\nresize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "resize = 1\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "scale_flag",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "scale_flag = True\nHEIGHT, WIDTH = 720, 1080\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "onnx_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "description": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "peekOfCode": "onnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n        cx = b[0]",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.FaceBoxes_ONNX",
        "documentation": {}
    },
    {
        "label": "convert_to_onnx",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.onnx",
        "description": "alignment3D.DDFA_V2.FaceBoxes.onnx",
        "peekOfCode": "def convert_to_onnx(onnx_path):\n    pretrained_path = onnx_path.replace('.onnx', '.pth')\n    # 1. load model\n    torch.set_grad_enabled(False)\n    net = FaceBoxesNet(phase='test', size=None, num_classes=2)  # initialize detector\n    net = load_model(net, pretrained_path=pretrained_path, load_to_cpu=True)\n    net.eval()\n    # 2. convert\n    batch_size = 1\n    dummy_input = torch.randn(batch_size, 3, 720, 1080)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.onnx",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.FaceBoxes.onnx",
        "description": "alignment3D.DDFA_V2.FaceBoxes.onnx",
        "peekOfCode": "__author__ = 'cleardusk'\nimport torch\nfrom .models.faceboxes import FaceBoxesNet\nfrom .utils.functions import load_model\ndef convert_to_onnx(onnx_path):\n    pretrained_path = onnx_path.replace('.onnx', '.pth')\n    # 1. load model\n    torch.set_grad_enabled(False)\n    net = FaceBoxesNet(phase='test', size=None, num_classes=2)  # initialize detector\n    net = load_model(net, pretrained_path=pretrained_path, load_to_cpu=True)",
        "detail": "alignment3D.DDFA_V2.FaceBoxes.onnx",
        "documentation": {}
    },
    {
        "label": "DepthWiseBlock",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "class DepthWiseBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, prelu=False):\n        super(DepthWiseBlock, self).__init__()\n        inplanes, planes = int(inplanes), int(planes)\n        self.conv_dw = nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1, stride=stride, groups=inplanes,\n                                 bias=False)\n        self.bn_dw = nn.BatchNorm2d(inplanes)\n        self.conv_sep = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_sep = nn.BatchNorm2d(planes)\n        if prelu:",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "MobileNet",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "class MobileNet(nn.Module):\n    def __init__(self, widen_factor=1.0, num_classes=1000, prelu=False, input_channel=3):\n        \"\"\" Constructor\n        Args:\n            widen_factor: config of widen_factor\n            num_classes: number of classes\n        \"\"\"\n        super(MobileNet, self).__init__()\n        block = DepthWiseBlock\n        self.conv1 = nn.Conv2d(input_channel, int(32 * widen_factor), kernel_size=3, stride=2, padding=1,",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "mobilenet",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "def mobilenet(**kwargs):\n    \"\"\"\n    Construct MobileNet.\n    widen_factor=1.0  for mobilenet_1\n    widen_factor=0.75 for mobilenet_075\n    widen_factor=0.5  for mobilenet_05\n    widen_factor=0.25 for mobilenet_025\n    \"\"\"\n    # widen_factor = 1.0, num_classes = 1000\n    # model = MobileNet(widen_factor=widen_factor, num_classes=num_classes)",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "mobilenet_2",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "def mobilenet_2(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=2.0, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_1(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=1.0, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_075(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.75, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_05(num_classes=62, input_channel=3):",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "mobilenet_1",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "def mobilenet_1(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=1.0, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_075(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.75, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_05(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.5, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_025(num_classes=62, input_channel=3):",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "mobilenet_075",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "def mobilenet_075(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.75, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_05(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.5, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_025(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.25, num_classes=num_classes, input_channel=input_channel)\n    return model",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "mobilenet_05",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "def mobilenet_05(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.5, num_classes=num_classes, input_channel=input_channel)\n    return model\ndef mobilenet_025(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.25, num_classes=num_classes, input_channel=input_channel)\n    return model",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "mobilenet_025",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "def mobilenet_025(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.25, num_classes=num_classes, input_channel=input_channel)\n    return model",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "peekOfCode": "__all__ = ['MobileNet', 'mobilenet']\n# __all__ = ['mobilenet_2', 'mobilenet_1', 'mobilenet_075', 'mobilenet_05', 'mobilenet_025']\nclass DepthWiseBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, prelu=False):\n        super(DepthWiseBlock, self).__init__()\n        inplanes, planes = int(inplanes), int(planes)\n        self.conv_dw = nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1, stride=stride, groups=inplanes,\n                                 bias=False)\n        self.bn_dw = nn.BatchNorm2d(inplanes)\n        self.conv_sep = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, padding=0, bias=False)",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v1",
        "documentation": {}
    },
    {
        "label": "Hswish",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "class Hswish(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hswish, self).__init__()\n        self.inplace = inplace\n    def forward(self, x):\n        return x * F.relu6(x + 3., inplace=self.inplace) / 6.\nclass Hsigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hsigmoid, self).__init__()\n        self.inplace = inplace",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "Hsigmoid",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "class Hsigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hsigmoid, self).__init__()\n        self.inplace = inplace\n    def forward(self, x):\n        return F.relu6(x + 3., inplace=self.inplace) / 6.\nclass SEModule(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "class SEModule(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            Hsigmoid()\n            # nn.Sigmoid()",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "Identity",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "class Identity(nn.Module):\n    def __init__(self, channel):\n        super(Identity, self).__init__()\n    def forward(self, x):\n        return x\ndef make_divisible(x, divisible_by=8):\n    import numpy as np\n    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\nclass MobileBottleneck(nn.Module):\n    def __init__(self, inp, oup, kernel, stride, exp, se=False, nl='RE'):",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "MobileBottleneck",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "class MobileBottleneck(nn.Module):\n    def __init__(self, inp, oup, kernel, stride, exp, se=False, nl='RE'):\n        super(MobileBottleneck, self).__init__()\n        assert stride in [1, 2]\n        assert kernel in [3, 5]\n        padding = (kernel - 1) // 2\n        self.use_res_connect = stride == 1 and inp == oup\n        conv_layer = nn.Conv2d\n        norm_layer = nn.BatchNorm2d\n        if nl == 'RE':",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "MobileNetV3",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "class MobileNetV3(nn.Module):\n    def __init__(self, widen_factor=1.0, num_classes=141, num_landmarks=136, input_size=120, mode='small'):\n        super(MobileNetV3, self).__init__()\n        input_channel = 16\n        last_channel = 1280\n        if mode == 'large':\n            # refer to Table 1 in paper\n            mobile_setting = [\n                # k, exp, c,  se,     nl,  s,\n                [3, 16, 16, False, 'RE', 1],",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "conv_bn",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "def conv_bn(inp, oup, stride, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n    return nn.Sequential(\n        conv_layer(inp, oup, 3, stride, 1, bias=False),\n        norm_layer(oup),\n        nlin_layer(inplace=True)\n    )\ndef conv_1x1_bn(inp, oup, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n    return nn.Sequential(\n        conv_layer(inp, oup, 1, 1, 0, bias=False),\n        norm_layer(oup),",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "conv_1x1_bn",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "def conv_1x1_bn(inp, oup, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n    return nn.Sequential(\n        conv_layer(inp, oup, 1, 1, 0, bias=False),\n        norm_layer(oup),\n        nlin_layer(inplace=True)\n    )\nclass Hswish(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hswish, self).__init__()\n        self.inplace = inplace",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "def make_divisible(x, divisible_by=8):\n    import numpy as np\n    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\nclass MobileBottleneck(nn.Module):\n    def __init__(self, inp, oup, kernel, stride, exp, se=False, nl='RE'):\n        super(MobileBottleneck, self).__init__()\n        assert stride in [1, 2]\n        assert kernel in [3, 5]\n        padding = (kernel - 1) // 2\n        self.use_res_connect = stride == 1 and inp == oup",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "mobilenet_v3",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "def mobilenet_v3(**kwargs):\n    model = MobileNetV3(\n        widen_factor=kwargs.get('widen_factor', 1.0),\n        num_classes=kwargs.get('num_classes', 62),\n        num_landmarks=kwargs.get('num_landmarks', 136),\n        input_size=kwargs.get('size', 128),\n        mode=kwargs.get('mode', 'small')\n    )\n    return model",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "description": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "peekOfCode": "__all__ = ['MobileNetV3', 'mobilenet_v3']\ndef conv_bn(inp, oup, stride, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n    return nn.Sequential(\n        conv_layer(inp, oup, 3, stride, 1, bias=False),\n        norm_layer(oup),\n        nlin_layer(inplace=True)\n    )\ndef conv_1x1_bn(inp, oup, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n    return nn.Sequential(\n        conv_layer(inp, oup, 1, 1, 0, bias=False),",
        "detail": "alignment3D.DDFA_V2.models.mobilenet_v3",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.resnet",
        "description": "alignment3D.DDFA_V2.models.resnet",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample",
        "detail": "alignment3D.DDFA_V2.models.resnet",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.models.resnet",
        "description": "alignment3D.DDFA_V2.models.resnet",
        "peekOfCode": "class ResNet(nn.Module):\n    \"\"\"Another Strucutre used in caffe-resnet25\"\"\"\n    def __init__(self, block, layers, num_classes=62, num_landmarks=136, input_channel=3, fc_flg=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(input_channel, 32, kernel_size=5, stride=2, padding=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)  # 32 is input channels number\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)",
        "detail": "alignment3D.DDFA_V2.models.resnet",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.resnet",
        "description": "alignment3D.DDFA_V2.models.resnet",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)",
        "detail": "alignment3D.DDFA_V2.models.resnet",
        "documentation": {}
    },
    {
        "label": "resnet22",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.resnet",
        "description": "alignment3D.DDFA_V2.models.resnet",
        "peekOfCode": "def resnet22(**kwargs):\n    model = ResNet(\n        BasicBlock,\n        [3, 4, 3],\n        num_landmarks=kwargs.get('num_landmarks', 136),\n        input_channel=kwargs.get('input_channel', 3),\n        fc_flg=False\n    )\n    return model\ndef main():",
        "detail": "alignment3D.DDFA_V2.models.resnet",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.models.resnet",
        "description": "alignment3D.DDFA_V2.models.resnet",
        "peekOfCode": "def main():\n    pass\nif __name__ == '__main__':\n    main()",
        "detail": "alignment3D.DDFA_V2.models.resnet",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.models.resnet",
        "description": "alignment3D.DDFA_V2.models.resnet",
        "peekOfCode": "__all__ = ['ResNet', 'resnet22']\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)",
        "detail": "alignment3D.DDFA_V2.models.resnet",
        "documentation": {}
    },
    {
        "label": "RenderPipeline",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "description": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "peekOfCode": "class RenderPipeline(object):\n    def __init__(self, **kwargs):\n        self.intensity_ambient = convert_type(kwargs.get('intensity_ambient', 0.3))\n        self.intensity_directional = convert_type(kwargs.get('intensity_directional', 0.6))\n        self.intensity_specular = convert_type(kwargs.get('intensity_specular', 0.1))\n        self.specular_exp = kwargs.get('specular_exp', 5)\n        self.color_ambient = convert_type(kwargs.get('color_ambient', (1, 1, 1)))\n        self.color_directional = convert_type(kwargs.get('color_directional', (1, 1, 1)))\n        self.light_pos = convert_type(kwargs.get('light_pos', (0, 0, 5)))\n        self.view_pos = convert_type(kwargs.get('view_pos', (0, 0, 5)))",
        "detail": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "documentation": {}
    },
    {
        "label": "norm_vertices",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "description": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "peekOfCode": "def norm_vertices(vertices):\n    vertices -= vertices.min(0)[None, :]\n    vertices /= vertices.max()\n    vertices *= 2\n    vertices -= vertices.max(0)[None, :] / 2\n    return vertices\ndef convert_type(obj):\n    if isinstance(obj, tuple) or isinstance(obj, list):\n        return np.array(obj, dtype=np.float32)[None, :]\n    return obj",
        "detail": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "documentation": {}
    },
    {
        "label": "convert_type",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "description": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "peekOfCode": "def convert_type(obj):\n    if isinstance(obj, tuple) or isinstance(obj, list):\n        return np.array(obj, dtype=np.float32)[None, :]\n    return obj\nclass RenderPipeline(object):\n    def __init__(self, **kwargs):\n        self.intensity_ambient = convert_type(kwargs.get('intensity_ambient', 0.3))\n        self.intensity_directional = convert_type(kwargs.get('intensity_directional', 0.6))\n        self.intensity_specular = convert_type(kwargs.get('intensity_specular', 0.1))\n        self.specular_exp = kwargs.get('specular_exp', 5)",
        "detail": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "description": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "peekOfCode": "def main():\n    pass\nif __name__ == '__main__':\n    main()",
        "detail": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "documentation": {}
    },
    {
        "label": "_norm",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "description": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "peekOfCode": "_norm = lambda arr: arr / np.sqrt(np.sum(arr ** 2, axis=1))[:, None]\ndef norm_vertices(vertices):\n    vertices -= vertices.min(0)[None, :]\n    vertices /= vertices.max()\n    vertices *= 2\n    vertices -= vertices.max(0)[None, :] / 2\n    return vertices\ndef convert_type(obj):\n    if isinstance(obj, tuple) or isinstance(obj, list):\n        return np.array(obj, dtype=np.float32)[None, :]",
        "detail": "alignment3D.DDFA_V2.Sim3DR.lighting",
        "documentation": {}
    },
    {
        "label": "get_normal",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.Sim3DR.Sim3DR",
        "description": "alignment3D.DDFA_V2.Sim3DR.Sim3DR",
        "peekOfCode": "def get_normal(vertices, triangles):\n    normal = np.zeros_like(vertices, dtype=np.float32)\n    Sim3DR_Cython.get_normal(normal, vertices, triangles, vertices.shape[0], triangles.shape[0])\n    return normal\ndef rasterize(vertices, triangles, colors, bg=None,\n              height=None, width=None, channel=None,\n              reverse=False):\n    if bg is not None:\n        height, width, channel = bg.shape\n    else:",
        "detail": "alignment3D.DDFA_V2.Sim3DR.Sim3DR",
        "documentation": {}
    },
    {
        "label": "rasterize",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.Sim3DR.Sim3DR",
        "description": "alignment3D.DDFA_V2.Sim3DR.Sim3DR",
        "peekOfCode": "def rasterize(vertices, triangles, colors, bg=None,\n              height=None, width=None, channel=None,\n              reverse=False):\n    if bg is not None:\n        height, width, channel = bg.shape\n    else:\n        assert height is not None and width is not None and channel is not None\n        bg = np.zeros((height, width, channel), dtype=np.uint8)\n    buffer = np.zeros((height, width), dtype=np.float32) - 1e8\n    if colors.dtype != np.float32:",
        "detail": "alignment3D.DDFA_V2.Sim3DR.Sim3DR",
        "documentation": {}
    },
    {
        "label": "add_path",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "description": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "peekOfCode": "def add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\nthis_dir = osp.dirname(__file__)\nlib_path = osp.join(this_dir, '.')\nadd_path(lib_path)",
        "detail": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "documentation": {}
    },
    {
        "label": "this_dir",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "description": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "peekOfCode": "this_dir = osp.dirname(__file__)\nlib_path = osp.join(this_dir, '.')\nadd_path(lib_path)",
        "detail": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "documentation": {}
    },
    {
        "label": "lib_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "description": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "peekOfCode": "lib_path = osp.join(this_dir, '.')\nadd_path(lib_path)",
        "detail": "alignment3D.DDFA_V2.Sim3DR._init_paths",
        "documentation": {}
    },
    {
        "label": "depth",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.depth",
        "description": "alignment3D.DDFA_V2.utils.depth",
        "peekOfCode": "def depth(img, ver_lst, tri, show_flag=False, wfp=None, with_bg_flag=True):\n    if with_bg_flag:\n        overlap = img.copy()\n    else:\n        overlap = np.zeros_like(img)\n    for ver_ in ver_lst:\n        ver = _to_ctype(ver_.T)  # transpose\n        z = ver[:, 2]\n        z_min, z_max = min(z), max(z)\n        z = (z - z_min) / (z_max - z_min)",
        "detail": "alignment3D.DDFA_V2.utils.depth",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.depth",
        "description": "alignment3D.DDFA_V2.utils.depth",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport cv2\nimport numpy as np\nfrom Sim3DR import rasterize\nfrom utils.functions import plot_image\nfrom .tddfa_util import _to_ctype\ndef depth(img, ver_lst, tri, show_flag=False, wfp=None, with_bg_flag=True):\n    if with_bg_flag:",
        "detail": "alignment3D.DDFA_V2.utils.depth",
        "documentation": {}
    },
    {
        "label": "get_suffix",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')\n    if pos == -1:\n        return ''\n    return filename[pos:]\ndef crop_img(img, roi_box):\n    h, w = img.shape[:2]\n    sx, sy, ex, ey = [int(round(_)) for _ in roi_box]\n    dh, dw = ey - sy, ex - sx",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "crop_img",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def crop_img(img, roi_box):\n    h, w = img.shape[:2]\n    sx, sy, ex, ey = [int(round(_)) for _ in roi_box]\n    dh, dw = ey - sy, ex - sx\n    if len(img.shape) == 3:\n        res = np.zeros((dh, dw, 3), dtype=np.uint8)\n    else:\n        res = np.zeros((dh, dw), dtype=np.uint8)\n    if sx < 0:\n        sx, dsx = 0, -sx",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "calc_hypotenuse",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def calc_hypotenuse(pts):\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    radius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\n    bbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n    llength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\n    return llength / 3\ndef parse_roi_box_from_landmark(pts):\n    \"\"\"calc roi box from landmark\"\"\"\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "parse_roi_box_from_landmark",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def parse_roi_box_from_landmark(pts):\n    \"\"\"calc roi box from landmark\"\"\"\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    radius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\n    bbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n    llength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\n    center_x = (bbox[2] + bbox[0]) / 2\n    center_y = (bbox[3] + bbox[1]) / 2\n    roi_box = [0] * 4",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "parse_roi_box_from_bbox",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def parse_roi_box_from_bbox(bbox):\n    left, top, right, bottom = bbox[:4]\n    old_size = (right - left + bottom - top) / 2\n    center_x = right - (right - left) / 2.0\n    center_y = bottom - (bottom - top) / 2.0 + old_size * 0.14\n    size = int(old_size * 1.58)\n    roi_box = [0] * 4\n    roi_box[0] = center_x - size / 2\n    roi_box[1] = center_y - size / 2\n    roi_box[2] = roi_box[0] + size",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "plot_image",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def plot_image(img):\n    height, width = img.shape[:2]\n    plt.figure(figsize=(12, height / width * 12))\n    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n    plt.axis('off')\n    plt.imshow(img[..., ::-1])\n    plt.show()\ndef draw_landmarks(img, pts, style='fancy', wfp=None, show_flag=False, **kwargs):\n    \"\"\"Draw landmarks using matplotlib\"\"\"\n    height, width = img.shape[:2]",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "draw_landmarks",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def draw_landmarks(img, pts, style='fancy', wfp=None, show_flag=False, **kwargs):\n    \"\"\"Draw landmarks using matplotlib\"\"\"\n    height, width = img.shape[:2]\n    plt.figure(figsize=(12, height / width * 12))\n    plt.imshow(img[..., ::-1])\n    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n    plt.axis('off')\n    dense_flag = kwargs.get('dense_flag')\n    if not type(pts) in [tuple, list]:\n        pts = [pts]",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "cv_draw_landmark",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "def cv_draw_landmark(img_ori, pts, box=None, color=GREEN, size=1):\n    img = img_ori.copy()\n    n = pts.shape[1]\n    if n <= 106:\n        for i in range(n):\n            cv2.circle(img, (int(round(pts[0, i])), int(round(pts[1, i]))), size, color, -1)\n    else:\n        sep = 1\n        for i in range(0, n, sep):\n            cv2.circle(img, (int(round(pts[0, i])), int(round(pts[1, i]))), size, color, 1)",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "__author__ = 'cleardusk'\nimport numpy as np\nimport cv2\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nRED = (0, 0, 255)\nGREEN = (0, 255, 0)\nBLUE = (255, 0, 0)\ndef get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "RED",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "RED = (0, 0, 255)\nGREEN = (0, 255, 0)\nBLUE = (255, 0, 0)\ndef get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')\n    if pos == -1:\n        return ''\n    return filename[pos:]\ndef crop_img(img, roi_box):",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "GREEN",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "GREEN = (0, 255, 0)\nBLUE = (255, 0, 0)\ndef get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')\n    if pos == -1:\n        return ''\n    return filename[pos:]\ndef crop_img(img, roi_box):\n    h, w = img.shape[:2]",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "BLUE",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.functions",
        "description": "alignment3D.DDFA_V2.utils.functions",
        "peekOfCode": "BLUE = (255, 0, 0)\ndef get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')\n    if pos == -1:\n        return ''\n    return filename[pos:]\ndef crop_img(img, roi_box):\n    h, w = img.shape[:2]\n    sx, sy, ex, ey = [int(round(_)) for _ in roi_box]",
        "detail": "alignment3D.DDFA_V2.utils.functions",
        "documentation": {}
    },
    {
        "label": "mkdir",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "def mkdir(d):\n    os.makedirs(d, exist_ok=True)\ndef _get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')\n    if pos == -1:\n        return ''\n    return filename[pos + 1:]\ndef _load(fp):\n    suffix = _get_suffix(fp)",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "__author__ = 'cleardusk'\nimport os\nimport numpy as np\nimport torch\nimport pickle\ndef mkdir(d):\n    os.makedirs(d, exist_ok=True)\ndef _get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "_load_cpu",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "_load_cpu = _load\n_numpy_to_tensor = lambda x: torch.from_numpy(x)\n_tensor_to_numpy = lambda x: x.numpy()\n_numpy_to_cuda = lambda x: _tensor_to_cuda(torch.from_numpy(x))\n_cuda_to_tensor = lambda x: x.cpu()\n_cuda_to_numpy = lambda x: x.cpu().numpy()",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "_numpy_to_tensor",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "_numpy_to_tensor = lambda x: torch.from_numpy(x)\n_tensor_to_numpy = lambda x: x.numpy()\n_numpy_to_cuda = lambda x: _tensor_to_cuda(torch.from_numpy(x))\n_cuda_to_tensor = lambda x: x.cpu()\n_cuda_to_numpy = lambda x: x.cpu().numpy()",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "_tensor_to_numpy",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "_tensor_to_numpy = lambda x: x.numpy()\n_numpy_to_cuda = lambda x: _tensor_to_cuda(torch.from_numpy(x))\n_cuda_to_tensor = lambda x: x.cpu()\n_cuda_to_numpy = lambda x: x.cpu().numpy()",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "_numpy_to_cuda",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "_numpy_to_cuda = lambda x: _tensor_to_cuda(torch.from_numpy(x))\n_cuda_to_tensor = lambda x: x.cpu()\n_cuda_to_numpy = lambda x: x.cpu().numpy()",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "_cuda_to_tensor",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "_cuda_to_tensor = lambda x: x.cpu()\n_cuda_to_numpy = lambda x: x.cpu().numpy()",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "_cuda_to_numpy",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.io",
        "description": "alignment3D.DDFA_V2.utils.io",
        "peekOfCode": "_cuda_to_numpy = lambda x: x.cpu().numpy()",
        "detail": "alignment3D.DDFA_V2.utils.io",
        "documentation": {}
    },
    {
        "label": "convert_to_onnx",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.onnx",
        "description": "alignment3D.DDFA_V2.utils.onnx",
        "peekOfCode": "def convert_to_onnx(**kvs):\n    # 1. load model\n    size = kvs.get('size', 120)\n    model = getattr(models, kvs.get('arch'))(\n        num_classes=kvs.get('num_params', 62),\n        widen_factor=kvs.get('widen_factor', 1),\n        size=size,\n        mode=kvs.get('mode', 'small')\n    )\n    checkpoint_fp = kvs.get('checkpoint_fp')",
        "detail": "alignment3D.DDFA_V2.utils.onnx",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.onnx",
        "description": "alignment3D.DDFA_V2.utils.onnx",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport torch\nimport models\nfrom utils.tddfa_util import load_model\ndef convert_to_onnx(**kvs):\n    # 1. load model\n    size = kvs.get('size', 120)\n    model = getattr(models, kvs.get('arch'))(",
        "detail": "alignment3D.DDFA_V2.utils.onnx",
        "documentation": {}
    },
    {
        "label": "calc_ncc_code",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pncc",
        "description": "alignment3D.DDFA_V2.utils.pncc",
        "peekOfCode": "def calc_ncc_code():\n    from bfm import bfm\n    # formula: ncc_d = ( u_d - min(u_d) ) / ( max(u_d) - min(u_d) ), d = {r, g, b}\n    u = bfm.u\n    u = u.reshape(3, -1, order='F')\n    for i in range(3):\n        u[i] = (u[i] - u[i].min()) / (u[i].max() - u[i].min())\n    _dump('../configs/ncc_code.npy', u)\ndef pncc(img, ver_lst, tri, show_flag=False, wfp=None, with_bg_flag=True):\n    ncc_code = _load(make_abs_path('../configs/ncc_code.npy'))",
        "detail": "alignment3D.DDFA_V2.utils.pncc",
        "documentation": {}
    },
    {
        "label": "pncc",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pncc",
        "description": "alignment3D.DDFA_V2.utils.pncc",
        "peekOfCode": "def pncc(img, ver_lst, tri, show_flag=False, wfp=None, with_bg_flag=True):\n    ncc_code = _load(make_abs_path('../configs/ncc_code.npy'))\n    if with_bg_flag:\n        overlap = img.copy()\n    else:\n        overlap = np.zeros_like(img)\n    # rendering pncc\n    for ver_ in ver_lst:\n        ver = _to_ctype(ver_.T)  # transpose\n        overlap = rasterize(ver, tri, ncc_code.T, bg=overlap)  # m x 3",
        "detail": "alignment3D.DDFA_V2.utils.pncc",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pncc",
        "description": "alignment3D.DDFA_V2.utils.pncc",
        "peekOfCode": "def main():\n    # `configs/ncc_code.npy` is generated by `calc_nnc_code` function\n    # calc_ncc_code()\n    pass\nif __name__ == '__main__':\n    main()",
        "detail": "alignment3D.DDFA_V2.utils.pncc",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.pncc",
        "description": "alignment3D.DDFA_V2.utils.pncc",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport cv2\nimport numpy as np\nimport os.path as osp\nfrom Sim3DR import rasterize\nfrom utils.functions import plot_image\nfrom utils.io import _load, _dump\nfrom utils.tddfa_util import _to_ctype",
        "detail": "alignment3D.DDFA_V2.utils.pncc",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.pncc",
        "description": "alignment3D.DDFA_V2.utils.pncc",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\ndef calc_ncc_code():\n    from bfm import bfm\n    # formula: ncc_d = ( u_d - min(u_d) ) / ( max(u_d) - min(u_d) ), d = {r, g, b}\n    u = bfm.u\n    u = u.reshape(3, -1, order='F')\n    for i in range(3):\n        u[i] = (u[i] - u[i].min()) / (u[i].max() - u[i].min())\n    _dump('../configs/ncc_code.npy', u)\ndef pncc(img, ver_lst, tri, show_flag=False, wfp=None, with_bg_flag=True):",
        "detail": "alignment3D.DDFA_V2.utils.pncc",
        "documentation": {}
    },
    {
        "label": "P2sRt",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pose",
        "description": "alignment3D.DDFA_V2.utils.pose",
        "peekOfCode": "def P2sRt(P):\n    \"\"\" decompositing camera matrix P.\n    Args:\n        P: (3, 4). Affine Camera Matrix.\n    Returns:\n        s: scale factor.\n        R: (3, 3). rotation matrix.\n        t2d: (2,). 2d translation.\n    \"\"\"\n    t3d = P[:, 3]",
        "detail": "alignment3D.DDFA_V2.utils.pose",
        "documentation": {}
    },
    {
        "label": "matrix2angle",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pose",
        "description": "alignment3D.DDFA_V2.utils.pose",
        "peekOfCode": "def matrix2angle(R):\n    \"\"\" compute three Euler angles from a Rotation Matrix. Ref: http://www.gregslabaugh.net/publications/euler.pdf\n    refined by: https://stackoverflow.com/questions/43364900/rotation-matrix-to-euler-angles-with-opencv\n    todo: check and debug\n     Args:\n         R: (3,3). rotation matrix\n     Returns:\n         x: yaw\n         y: pitch\n         z: roll",
        "detail": "alignment3D.DDFA_V2.utils.pose",
        "documentation": {}
    },
    {
        "label": "calc_pose",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pose",
        "description": "alignment3D.DDFA_V2.utils.pose",
        "peekOfCode": "def calc_pose(param):\n    P = param[:12].reshape(3, -1)  # camera matrix\n    s, R, t3d = P2sRt(P)\n    P = np.concatenate((R, t3d.reshape(3, -1)), axis=1)  # without scale\n    pose = matrix2angle(R)\n    pose = [p * 180 / np.pi for p in pose]\n    return P, pose\ndef build_camera_box(rear_size=90):\n    point_3d = []\n    rear_depth = 0",
        "detail": "alignment3D.DDFA_V2.utils.pose",
        "documentation": {}
    },
    {
        "label": "build_camera_box",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pose",
        "description": "alignment3D.DDFA_V2.utils.pose",
        "peekOfCode": "def build_camera_box(rear_size=90):\n    point_3d = []\n    rear_depth = 0\n    point_3d.append((-rear_size, -rear_size, rear_depth))\n    point_3d.append((-rear_size, rear_size, rear_depth))\n    point_3d.append((rear_size, rear_size, rear_depth))\n    point_3d.append((rear_size, -rear_size, rear_depth))\n    point_3d.append((-rear_size, -rear_size, rear_depth))\n    front_size = int(4 / 3 * rear_size)\n    front_depth = int(4 / 3 * rear_size)",
        "detail": "alignment3D.DDFA_V2.utils.pose",
        "documentation": {}
    },
    {
        "label": "plot_pose_box",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pose",
        "description": "alignment3D.DDFA_V2.utils.pose",
        "peekOfCode": "def plot_pose_box(img, P, ver, color=(40, 255, 0), line_width=2):\n    \"\"\" Draw a 3D box as annotation of pose.\n    Ref:https://github.com/yinguobing/head-pose-estimation/blob/master/pose_estimator.py\n    Args:\n        img: the input image\n        P: (3, 4). Affine Camera Matrix.\n        kpt: (2, 68) or (3, 68)\n    \"\"\"\n    llength = calc_hypotenuse(ver)\n    point_3d = build_camera_box(llength)",
        "detail": "alignment3D.DDFA_V2.utils.pose",
        "documentation": {}
    },
    {
        "label": "viz_pose",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.pose",
        "description": "alignment3D.DDFA_V2.utils.pose",
        "peekOfCode": "def viz_pose(img, param_lst, ver_lst, show_flag=False, wfp=None):\n    for param, ver in zip(param_lst, ver_lst):\n        P, pose = calc_pose(param)\n        img = plot_pose_box(img, P, ver)\n        # print(P[:, :3])\n        print(f'yaw: {pose[0]:.1f}, pitch: {pose[1]:.1f}, roll: {pose[2]:.1f}')\n    if wfp is not None:\n        cv2.imwrite(wfp, img)\n        print(f'Save visualization result to {wfp}')\n    if show_flag:",
        "detail": "alignment3D.DDFA_V2.utils.pose",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.pose",
        "description": "alignment3D.DDFA_V2.utils.pose",
        "peekOfCode": "__author__ = 'cleardusk'\nimport cv2\nimport numpy as np\nfrom math import cos, sin, atan2, asin, sqrt\nfrom .functions import calc_hypotenuse, plot_image\ndef P2sRt(P):\n    \"\"\" decompositing camera matrix P.\n    Args:\n        P: (3, 4). Affine Camera Matrix.\n    Returns:",
        "detail": "alignment3D.DDFA_V2.utils.pose",
        "documentation": {}
    },
    {
        "label": "render",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.render",
        "description": "alignment3D.DDFA_V2.utils.render",
        "peekOfCode": "def render(img, ver_lst, tri, alpha=0.6, show_flag=False, wfp=None, with_bg_flag=True):\n    if with_bg_flag:\n        overlap = img.copy()\n    else:\n        overlap = np.zeros_like(img)\n    for ver_ in ver_lst:\n        ver = _to_ctype(ver_.T)  # transpose\n        overlap = render_app(ver, tri, overlap)\n    if with_bg_flag:\n        res = cv2.addWeighted(img, 1 - alpha, overlap, alpha, 0)",
        "detail": "alignment3D.DDFA_V2.utils.render",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.render",
        "description": "alignment3D.DDFA_V2.utils.render",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport cv2\nimport numpy as np\nfrom Sim3DR import RenderPipeline\nfrom utils.functions import plot_image\nfrom .tddfa_util import _to_ctype\ncfg = {\n    'intensity_ambient': 0.3,",
        "detail": "alignment3D.DDFA_V2.utils.render",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.render",
        "description": "alignment3D.DDFA_V2.utils.render",
        "peekOfCode": "cfg = {\n    'intensity_ambient': 0.3,\n    'color_ambient': (1, 1, 1),\n    'intensity_directional': 0.6,\n    'color_directional': (1, 1, 1),\n    'intensity_specular': 0.1,\n    'specular_exp': 5,\n    'light_pos': (0, 0, 5),\n    'view_pos': (0, 0, 5)\n}",
        "detail": "alignment3D.DDFA_V2.utils.render",
        "documentation": {}
    },
    {
        "label": "render_app",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.render",
        "description": "alignment3D.DDFA_V2.utils.render",
        "peekOfCode": "render_app = RenderPipeline(**cfg)\ndef render(img, ver_lst, tri, alpha=0.6, show_flag=False, wfp=None, with_bg_flag=True):\n    if with_bg_flag:\n        overlap = img.copy()\n    else:\n        overlap = np.zeros_like(img)\n    for ver_ in ver_lst:\n        ver = _to_ctype(ver_.T)  # transpose\n        overlap = render_app(ver, tri, overlap)\n    if with_bg_flag:",
        "detail": "alignment3D.DDFA_V2.utils.render",
        "documentation": {}
    },
    {
        "label": "TrianglesMeshRender",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.utils.render_ctypes",
        "description": "alignment3D.DDFA_V2.utils.render_ctypes",
        "peekOfCode": "class TrianglesMeshRender(object):\n    def __init__(\n            self,\n            clibs,\n            light=(0, 0, 5),\n            direction=(0.6, 0.6, 0.6),\n            ambient=(0.3, 0.3, 0.3)\n    ):\n        if not osp.exists(clibs):\n            raise Exception(f'{clibs} not found, please build it first, by run '",
        "detail": "alignment3D.DDFA_V2.utils.render_ctypes",
        "documentation": {}
    },
    {
        "label": "render",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.render_ctypes",
        "description": "alignment3D.DDFA_V2.utils.render_ctypes",
        "peekOfCode": "def render(img, ver_lst, tri, alpha=0.6, show_flag=False, wfp=None, with_bg_flag=True):\n    if with_bg_flag:\n        overlap = img.copy()\n    else:\n        overlap = np.zeros_like(img)\n    for ver_ in ver_lst:\n        ver = np.ascontiguousarray(ver_.T)  # transpose\n        render_app(ver, tri, bg=overlap)\n    if with_bg_flag:\n        res = cv2.addWeighted(img, 1 - alpha, overlap, alpha, 0)",
        "detail": "alignment3D.DDFA_V2.utils.render_ctypes",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.render_ctypes",
        "description": "alignment3D.DDFA_V2.utils.render_ctypes",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nclass TrianglesMeshRender(object):\n    def __init__(\n            self,\n            clibs,\n            light=(0, 0, 5),\n            direction=(0.6, 0.6, 0.6),\n            ambient=(0.3, 0.3, 0.3)\n    ):\n        if not osp.exists(clibs):",
        "detail": "alignment3D.DDFA_V2.utils.render_ctypes",
        "documentation": {}
    },
    {
        "label": "render_app",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.render_ctypes",
        "description": "alignment3D.DDFA_V2.utils.render_ctypes",
        "peekOfCode": "render_app = TrianglesMeshRender(clibs=make_abs_path('asset/render.so'))\ndef render(img, ver_lst, tri, alpha=0.6, show_flag=False, wfp=None, with_bg_flag=True):\n    if with_bg_flag:\n        overlap = img.copy()\n    else:\n        overlap = np.zeros_like(img)\n    for ver_ in ver_lst:\n        ver = np.ascontiguousarray(ver_.T)  # transpose\n        render_app(ver, tri, bg=overlap)\n    if with_bg_flag:",
        "detail": "alignment3D.DDFA_V2.utils.render_ctypes",
        "documentation": {}
    },
    {
        "label": "ser_to_ply_single",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "def ser_to_ply_single(ver_lst, tri, height, wfp, reverse=True):\n    suffix = get_suffix(wfp)\n    for i, ver in enumerate(ver_lst):\n        wfp_new = wfp.replace(suffix, f'_{i + 1}{suffix}')\n        n_vertex = ver.shape[1]\n        n_face = tri.shape[0]\n        header = header_temp.format(n_vertex, n_face)\n        with open(wfp_new, 'w') as f:\n            f.write(header + '\\n')\n            for i in range(n_vertex):",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_ply_multiple",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "def ser_to_ply_multiple(ver_lst, tri, height, wfp, reverse=True):\n    n_ply = len(ver_lst)  # count ply\n    if n_ply <= 0:\n        return\n    n_vertex = ver_lst[0].shape[1]\n    n_face = tri.shape[0]\n    header = header_temp.format(n_vertex * n_ply, n_face * n_ply)\n    with open(wfp, 'w') as f:\n        f.write(header + '\\n')\n        for i in range(n_ply):",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "get_colors",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "def get_colors(img, ver):\n    h, w, _ = img.shape\n    ver[0, :] = np.minimum(np.maximum(ver[0, :], 0), w - 1)  # x\n    ver[1, :] = np.minimum(np.maximum(ver[1, :], 0), h - 1)  # y\n    ind = np.round(ver).astype(np.int32)\n    colors = img[ind[1, :], ind[0, :], :] / 255.  # n x 3\n    return colors.copy()\ndef ser_to_obj_single(img, ver_lst, tri, height, wfp):\n    suffix = get_suffix(wfp)\n    n_face = tri.shape[0]",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_obj_single",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "def ser_to_obj_single(img, ver_lst, tri, height, wfp):\n    suffix = get_suffix(wfp)\n    n_face = tri.shape[0]\n    for i, ver in enumerate(ver_lst):\n        colors = get_colors(img, ver)\n        n_vertex = ver.shape[1]\n        wfp_new = wfp.replace(suffix, f'_{i + 1}{suffix}')\n        with open(wfp_new, 'w') as f:\n            for i in range(n_vertex):\n                x, y, z = ver[:, i]",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_obj_multiple",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "def ser_to_obj_multiple(img, ver_lst, tri, height, wfp):\n    n_obj = len(ver_lst)  # count obj\n    if n_obj <= 0:\n        return\n    n_vertex = ver_lst[0].shape[1]\n    n_face = tri.shape[0]\n    with open(wfp, 'w') as f:\n        for i in range(n_obj):\n            ver = ver_lst[i]\n            colors = get_colors(img, ver)",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "__author__ = 'cleardusk'\nimport numpy as np\nfrom .tddfa_util import _to_ctype\nfrom .functions import get_suffix\nheader_temp = \"\"\"ply\nformat ascii 1.0\nelement vertex {}\nproperty float x\nproperty float y\nproperty float z",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "header_temp",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "header_temp = \"\"\"ply\nformat ascii 1.0\nelement vertex {}\nproperty float x\nproperty float y\nproperty float z\nelement face {}\nproperty list uchar int vertex_indices\nend_header\n\"\"\"",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_ply",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "ser_to_ply = ser_to_ply_multiple  # ser_to_ply_single\nser_to_obj = ser_to_obj_multiple  # ser_to_obj_multiple",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "ser_to_obj",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.serialization",
        "description": "alignment3D.DDFA_V2.utils.serialization",
        "peekOfCode": "ser_to_obj = ser_to_obj_multiple  # ser_to_obj_multiple",
        "detail": "alignment3D.DDFA_V2.utils.serialization",
        "documentation": {}
    },
    {
        "label": "ToTensorGjz",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.utils.tddfa_util",
        "description": "alignment3D.DDFA_V2.utils.tddfa_util",
        "peekOfCode": "class ToTensorGjz(object):\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img.float()\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\nclass NormalizeGjz(object):\n    def __init__(self, mean, std):\n        self.mean = mean",
        "detail": "alignment3D.DDFA_V2.utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "NormalizeGjz",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.utils.tddfa_util",
        "description": "alignment3D.DDFA_V2.utils.tddfa_util",
        "peekOfCode": "class NormalizeGjz(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        tensor.sub_(self.mean).div_(self.std)\n        return tensor\ndef similar_transform(pts3d, roi_box, size):\n    pts3d[0, :] -= 1  # for Python compatibility\n    pts3d[2, :] -= 1",
        "detail": "alignment3D.DDFA_V2.utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.tddfa_util",
        "description": "alignment3D.DDFA_V2.utils.tddfa_util",
        "peekOfCode": "def str2bool(v):\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected')\ndef load_model(model, checkpoint_fp):\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n    model_dict = model.state_dict()",
        "detail": "alignment3D.DDFA_V2.utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.tddfa_util",
        "description": "alignment3D.DDFA_V2.utils.tddfa_util",
        "peekOfCode": "def load_model(model, checkpoint_fp):\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        kc = k.replace('module.', '')\n        if kc in model_dict.keys():\n            model_dict[kc] = checkpoint[k]\n        if kc in ['fc_param.bias', 'fc_param.weight']:\n            model_dict[kc.replace('_param', '')] = checkpoint[k]",
        "detail": "alignment3D.DDFA_V2.utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "similar_transform",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.tddfa_util",
        "description": "alignment3D.DDFA_V2.utils.tddfa_util",
        "peekOfCode": "def similar_transform(pts3d, roi_box, size):\n    pts3d[0, :] -= 1  # for Python compatibility\n    pts3d[2, :] -= 1\n    pts3d[1, :] = size - pts3d[1, :]\n    sx, sy, ex, ey = roi_box\n    scale_x = (ex - sx) / size\n    scale_y = (ey - sy) / size\n    pts3d[0, :] = pts3d[0, :] * scale_x + sx\n    pts3d[1, :] = pts3d[1, :] * scale_y + sy\n    s = (scale_x + scale_y) / 2",
        "detail": "alignment3D.DDFA_V2.utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.tddfa_util",
        "description": "alignment3D.DDFA_V2.utils.tddfa_util",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport argparse\nimport numpy as np\nimport torch\ndef _to_ctype(arr):\n    if not arr.flags.c_contiguous:\n        return arr.copy(order='C')\n    return arr",
        "detail": "alignment3D.DDFA_V2.utils.tddfa_util",
        "documentation": {}
    },
    {
        "label": "load_uv_coords",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "def load_uv_coords(fp):\n    C = sio.loadmat(fp)\n    uv_coords = C['UV'].copy(order='C').astype(np.float32)\n    return uv_coords\ndef process_uv(uv_coords, uv_h=256, uv_w=256):\n    uv_coords[:, 0] = uv_coords[:, 0] * (uv_w - 1)\n    uv_coords[:, 1] = uv_coords[:, 1] * (uv_h - 1)\n    uv_coords[:, 1] = uv_h - uv_coords[:, 1] - 1\n    uv_coords = np.hstack((uv_coords, np.zeros((uv_coords.shape[0], 1), dtype=np.float32)))  # add z\n    return uv_coords",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "process_uv",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "def process_uv(uv_coords, uv_h=256, uv_w=256):\n    uv_coords[:, 0] = uv_coords[:, 0] * (uv_w - 1)\n    uv_coords[:, 1] = uv_coords[:, 1] * (uv_h - 1)\n    uv_coords[:, 1] = uv_h - uv_coords[:, 1] - 1\n    uv_coords = np.hstack((uv_coords, np.zeros((uv_coords.shape[0], 1), dtype=np.float32)))  # add z\n    return uv_coords\ng_uv_coords = load_uv_coords(make_abs_path('../configs/BFM_UV.mat'))\nindices = _load(make_abs_path('../configs/indices.npy'))  # todo: handle bfm_slim\ng_uv_coords = g_uv_coords[indices, :]\ndef get_colors(img, ver):",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "get_colors",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "def get_colors(img, ver):\n    # nearest-neighbor sampling\n    [h, w, _] = img.shape\n    ver[0, :] = np.minimum(np.maximum(ver[0, :], 0), w - 1)  # x\n    ver[1, :] = np.minimum(np.maximum(ver[1, :], 0), h - 1)  # y\n    ind = np.round(ver).astype(np.int32)\n    colors = img[ind[1, :], ind[0, :], :]  # n x 3\n    return colors\ndef bilinear_interpolate(img, x, y):\n    \"\"\"",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "bilinear_interpolate",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "def bilinear_interpolate(img, x, y):\n    \"\"\"\n    https://stackoverflow.com/questions/12729228/simple-efficient-bilinear-interpolation-of-images-in-numpy-and-python\n    \"\"\"\n    x0 = np.floor(x).astype(np.int32)\n    x1 = x0 + 1\n    y0 = np.floor(y).astype(np.int32)\n    y1 = y0 + 1\n    x0 = np.clip(x0, 0, img.shape[1] - 1)\n    x1 = np.clip(x1, 0, img.shape[1] - 1)",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "uv_tex",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "def uv_tex(img, ver_lst, tri, uv_h=256, uv_w=256, uv_c=3, show_flag=False, wfp=None):\n    uv_coords = process_uv(g_uv_coords.copy(), uv_h=uv_h, uv_w=uv_w)\n    res_lst = []\n    for ver_ in ver_lst:\n        ver = _to_ctype(ver_.T)  # transpose to m x 3\n        colors = bilinear_interpolate(img, ver[:, 0], ver[:, 1]) / 255.\n        # `rasterize` here serves as texture sampling, may need to optimization\n        res = rasterize(uv_coords, tri, colors, height=uv_h, width=uv_w, channel=uv_c)\n        res_lst.append(res)\n    # concat if there more than one image",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nsys.path.append('..')\nimport cv2\nimport numpy as np\nimport os.path as osp\nimport scipy.io as sio\nfrom Sim3DR import rasterize\nfrom utils.functions import plot_image\nfrom utils.io import _load",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\ndef load_uv_coords(fp):\n    C = sio.loadmat(fp)\n    uv_coords = C['UV'].copy(order='C').astype(np.float32)\n    return uv_coords\ndef process_uv(uv_coords, uv_h=256, uv_w=256):\n    uv_coords[:, 0] = uv_coords[:, 0] * (uv_w - 1)\n    uv_coords[:, 1] = uv_coords[:, 1] * (uv_h - 1)\n    uv_coords[:, 1] = uv_h - uv_coords[:, 1] - 1\n    uv_coords = np.hstack((uv_coords, np.zeros((uv_coords.shape[0], 1), dtype=np.float32)))  # add z",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "g_uv_coords",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "g_uv_coords = load_uv_coords(make_abs_path('../configs/BFM_UV.mat'))\nindices = _load(make_abs_path('../configs/indices.npy'))  # todo: handle bfm_slim\ng_uv_coords = g_uv_coords[indices, :]\ndef get_colors(img, ver):\n    # nearest-neighbor sampling\n    [h, w, _] = img.shape\n    ver[0, :] = np.minimum(np.maximum(ver[0, :], 0), w - 1)  # x\n    ver[1, :] = np.minimum(np.maximum(ver[1, :], 0), h - 1)  # y\n    ind = np.round(ver).astype(np.int32)\n    colors = img[ind[1, :], ind[0, :], :]  # n x 3",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "indices",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "indices = _load(make_abs_path('../configs/indices.npy'))  # todo: handle bfm_slim\ng_uv_coords = g_uv_coords[indices, :]\ndef get_colors(img, ver):\n    # nearest-neighbor sampling\n    [h, w, _] = img.shape\n    ver[0, :] = np.minimum(np.maximum(ver[0, :], 0), w - 1)  # x\n    ver[1, :] = np.minimum(np.maximum(ver[1, :], 0), h - 1)  # y\n    ind = np.round(ver).astype(np.int32)\n    colors = img[ind[1, :], ind[0, :], :]  # n x 3\n    return colors",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "g_uv_coords",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.utils.uv",
        "description": "alignment3D.DDFA_V2.utils.uv",
        "peekOfCode": "g_uv_coords = g_uv_coords[indices, :]\ndef get_colors(img, ver):\n    # nearest-neighbor sampling\n    [h, w, _] = img.shape\n    ver[0, :] = np.minimum(np.maximum(ver[0, :], 0), w - 1)  # x\n    ver[1, :] = np.minimum(np.maximum(ver[1, :], 0), h - 1)  # y\n    ind = np.round(ver).astype(np.int32)\n    colors = img[ind[1, :], ind[0, :], :]  # n x 3\n    return colors\ndef bilinear_interpolate(img, x, y):",
        "detail": "alignment3D.DDFA_V2.utils.uv",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.demo",
        "description": "alignment3D.DDFA_V2.demo",
        "peekOfCode": "def main(args):\n    cfg = yaml.load(open(args.config), Loader=yaml.SafeLoader)\n    # Init FaceBoxes and TDDFA, recommend using onnx flag\n    if args.onnx:\n        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n        os.environ['OMP_NUM_THREADS'] = '4'\n        from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n        from TDDFA_ONNX import TDDFA_ONNX\n        face_boxes = FaceBoxes_ONNX()\n        tddfa = TDDFA_ONNX(**cfg)",
        "detail": "alignment3D.DDFA_V2.demo",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.demo",
        "description": "alignment3D.DDFA_V2.demo",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nimport argparse\nimport cv2\nimport yaml\nimport os\nfrom FaceBoxes import FaceBoxes\nfrom TDDFA import TDDFA\nfrom utils.render import render\n# from utils.render_ctypes import render  # faster",
        "detail": "alignment3D.DDFA_V2.demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.demo_video",
        "description": "alignment3D.DDFA_V2.demo_video",
        "peekOfCode": "def main(args):\n    cfg = yaml.load(open(args.config), Loader=yaml.SafeLoader)\n    # Init FaceBoxes and TDDFA, recommend using onnx flag\n    if args.onnx:\n        import os\n        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n        os.environ['OMP_NUM_THREADS'] = '4'\n        from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n        from TDDFA_ONNX import TDDFA_ONNX\n        face_boxes = FaceBoxes_ONNX()",
        "detail": "alignment3D.DDFA_V2.demo_video",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.demo_video",
        "description": "alignment3D.DDFA_V2.demo_video",
        "peekOfCode": "__author__ = 'cleardusk'\nimport argparse\nimport imageio\nfrom tqdm import tqdm\nimport yaml\nfrom FaceBoxes import FaceBoxes\nfrom TDDFA import TDDFA\nfrom utils.render import render\n# from utils.render_ctypes import render\nfrom utils.functions import cv_draw_landmark, get_suffix",
        "detail": "alignment3D.DDFA_V2.demo_video",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.demo_video_smooth",
        "description": "alignment3D.DDFA_V2.demo_video_smooth",
        "peekOfCode": "def main(args):\n    cfg = yaml.load(open(args.config), Loader=yaml.SafeLoader)\n    # Init FaceBoxes and TDDFA, recommend using onnx flag\n    if args.onnx:\n        import os\n        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n        os.environ['OMP_NUM_THREADS'] = '4'\n        from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n        from TDDFA_ONNX import TDDFA_ONNX\n        face_boxes = FaceBoxes_ONNX()",
        "detail": "alignment3D.DDFA_V2.demo_video_smooth",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.demo_video_smooth",
        "description": "alignment3D.DDFA_V2.demo_video_smooth",
        "peekOfCode": "__author__ = 'cleardusk'\nimport argparse\nimport imageio\nimport numpy as np\nfrom tqdm import tqdm\nimport yaml\nfrom collections import deque\nfrom FaceBoxes import FaceBoxes\nfrom TDDFA import TDDFA\nfrom utils.render import render",
        "detail": "alignment3D.DDFA_V2.demo_video_smooth",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.demo_webcam_smooth",
        "description": "alignment3D.DDFA_V2.demo_webcam_smooth",
        "peekOfCode": "def main(args):\n    cfg = yaml.load(open(args.config), Loader=yaml.SafeLoader)\n    # Init FaceBoxes and TDDFA, recommend using onnx flag\n    if args.onnx:\n        import os\n        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n        os.environ['OMP_NUM_THREADS'] = '4'\n        from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n        from TDDFA_ONNX import TDDFA_ONNX\n        face_boxes = FaceBoxes_ONNX()",
        "detail": "alignment3D.DDFA_V2.demo_webcam_smooth",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.demo_webcam_smooth",
        "description": "alignment3D.DDFA_V2.demo_webcam_smooth",
        "peekOfCode": "__author__ = 'cleardusk'\nimport argparse\nimport imageio\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport yaml\nfrom collections import deque\nfrom FaceBoxes import FaceBoxes\nfrom TDDFA import TDDFA",
        "detail": "alignment3D.DDFA_V2.demo_webcam_smooth",
        "documentation": {}
    },
    {
        "label": "run_cmd",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "def run_cmd(command):\n    try:\n        print(command)\n        call(command, shell=True)\n    except Exception as e:\n        print(f\"Errorrrrr: {e}!\")\nprint(os.getcwd())\nos.chdir(\"/app/FaceBoxes/utils\")\nprint(os.getcwd())\nrun_cmd(\"python3 build.py build_ext --inplace\")",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "inferenc",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "def inference (img):\n    # face detection\n    boxes = face_boxes(img)\n    # regress 3DMM params\n    param_lst, roi_box_lst = tddfa(img, boxes)\n    # reconstruct vertices and render\n    ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=True)\n    return render(img, ver_lst, tddfa.tri, alpha=0.6, show_flag=False);    \ntitle = \"3DDFA V2\"\ndescription = \"demo for 3DDFA V2. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "cfg = yaml.load(open('configs/mb1_120x120.yml'), Loader=yaml.SafeLoader)\n# Init FaceBoxes and TDDFA, recommend using onnx flag\nonnx_flag = True  # or True to use ONNX to speed up\nif onnx_flag:    \n    import os\n    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n    os.environ['OMP_NUM_THREADS'] = '4'\n    from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n    from TDDFA_ONNX import TDDFA_ONNX\n    face_boxes = FaceBoxes_ONNX()",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "onnx_flag",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "onnx_flag = True  # or True to use ONNX to speed up\nif onnx_flag:    \n    import os\n    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n    os.environ['OMP_NUM_THREADS'] = '4'\n    from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n    from TDDFA_ONNX import TDDFA_ONNX\n    face_boxes = FaceBoxes_ONNX()\n    tddfa = TDDFA_ONNX(**cfg)\nelse:",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "title",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "title = \"3DDFA V2\"\ndescription = \"demo for 3DDFA V2. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\narticle = \"<p style='text-align: center'><a href='https://arxiv.org/abs/2009.09960'>Towards Fast, Accurate and Stable 3D Dense Face Alignment</a> | <a href='https://github.com/cleardusk/3DDFA_V2'>Github Repo</a></p>\"\nexamples = [\n    ['solvay.jpg']\n]\ngr.Interface(\n    inference, \n    [gr.inputs.Image(type=\"numpy\", label=\"Input\")], \n    gr.outputs.Image(type=\"numpy\", label=\"Output\"),",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "description",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "description = \"demo for 3DDFA V2. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\narticle = \"<p style='text-align: center'><a href='https://arxiv.org/abs/2009.09960'>Towards Fast, Accurate and Stable 3D Dense Face Alignment</a> | <a href='https://github.com/cleardusk/3DDFA_V2'>Github Repo</a></p>\"\nexamples = [\n    ['solvay.jpg']\n]\ngr.Interface(\n    inference, \n    [gr.inputs.Image(type=\"numpy\", label=\"Input\")], \n    gr.outputs.Image(type=\"numpy\", label=\"Output\"),\n    title=title,",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "article",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "article = \"<p style='text-align: center'><a href='https://arxiv.org/abs/2009.09960'>Towards Fast, Accurate and Stable 3D Dense Face Alignment</a> | <a href='https://github.com/cleardusk/3DDFA_V2'>Github Repo</a></p>\"\nexamples = [\n    ['solvay.jpg']\n]\ngr.Interface(\n    inference, \n    [gr.inputs.Image(type=\"numpy\", label=\"Input\")], \n    gr.outputs.Image(type=\"numpy\", label=\"Output\"),\n    title=title,\n    description=description,",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "examples",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.gradiodemo",
        "description": "alignment3D.DDFA_V2.gradiodemo",
        "peekOfCode": "examples = [\n    ['solvay.jpg']\n]\ngr.Interface(\n    inference, \n    [gr.inputs.Image(type=\"numpy\", label=\"Input\")], \n    gr.outputs.Image(type=\"numpy\", label=\"Output\"),\n    title=title,\n    description=description,\n    article=article,",
        "detail": "alignment3D.DDFA_V2.gradiodemo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.latency",
        "description": "alignment3D.DDFA_V2.latency",
        "peekOfCode": "def main(args):\n    _t = {\n        'det': Timer(),\n        'reg': Timer(),\n        'recon': Timer()\n    }\n    cfg = yaml.load(open(args.config), Loader=yaml.SafeLoader)\n    # Init FaceBoxes and TDDFA\n    if args.onnx:\n        import os",
        "detail": "alignment3D.DDFA_V2.latency",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.latency",
        "description": "alignment3D.DDFA_V2.latency",
        "peekOfCode": "__author__ = 'cleardusk'\nimport sys\nimport argparse\nimport cv2\nimport yaml\nfrom FaceBoxes import FaceBoxes\nfrom TDDFA import TDDFA\nfrom utils.tddfa_util import str2bool\nfrom FaceBoxes.utils.timer import Timer\ndef main(args):",
        "detail": "alignment3D.DDFA_V2.latency",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "def main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,\n                        stmt=TEST_CODE,\n                        repeat=repeat,\n                        number=number)\n    res = np.array(res, dtype=np.float32)\n    res /= number\n    mean, var = np.mean(res), np.std(res)\n    print('Inference speed: {:.2f}±{:.2f} ms'.format(mean * 1000, var * 1000))",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "__author__ = 'cleardusk'\nimport timeit\nimport numpy as np\nSETUP_CODE = '''\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nimport numpy as np\nimport onnxruntime\nonnx_fp = \"weights/mb1_120x120.onnx\" # if not existed, convert it, see \"convert_to_onnx function in utils/onnx.py\"\nsession = onnxruntime.InferenceSession(onnx_fp, None)",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "SETUP_CODE",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "SETUP_CODE = '''\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nimport numpy as np\nimport onnxruntime\nonnx_fp = \"weights/mb1_120x120.onnx\" # if not existed, convert it, see \"convert_to_onnx function in utils/onnx.py\"\nsession = onnxruntime.InferenceSession(onnx_fp, None)\nimg = np.random.randn(1, 3, 120, 120).astype(np.float32)\n'''\nTEST_CODE = '''",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OMP_NUM_THREADS\"]",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\nimport numpy as np\nimport onnxruntime\nonnx_fp = \"weights/mb1_120x120.onnx\" # if not existed, convert it, see \"convert_to_onnx function in utils/onnx.py\"\nsession = onnxruntime.InferenceSession(onnx_fp, None)\nimg = np.random.randn(1, 3, 120, 120).astype(np.float32)\n'''\nTEST_CODE = '''\nsession.run(None, {\"input\": img})\n'''",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "onnx_fp",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "onnx_fp = \"weights/mb1_120x120.onnx\" # if not existed, convert it, see \"convert_to_onnx function in utils/onnx.py\"\nsession = onnxruntime.InferenceSession(onnx_fp, None)\nimg = np.random.randn(1, 3, 120, 120).astype(np.float32)\n'''\nTEST_CODE = '''\nsession.run(None, {\"input\": img})\n'''\ndef main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "session",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "session = onnxruntime.InferenceSession(onnx_fp, None)\nimg = np.random.randn(1, 3, 120, 120).astype(np.float32)\n'''\nTEST_CODE = '''\nsession.run(None, {\"input\": img})\n'''\ndef main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,\n                        stmt=TEST_CODE,",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "img = np.random.randn(1, 3, 120, 120).astype(np.float32)\n'''\nTEST_CODE = '''\nsession.run(None, {\"input\": img})\n'''\ndef main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,\n                        stmt=TEST_CODE,\n                        repeat=repeat,",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "TEST_CODE",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.speed_cpu",
        "description": "alignment3D.DDFA_V2.speed_cpu",
        "peekOfCode": "TEST_CODE = '''\nsession.run(None, {\"input\": img})\n'''\ndef main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,\n                        stmt=TEST_CODE,\n                        repeat=repeat,\n                        number=number)\n    res = np.array(res, dtype=np.float32)",
        "detail": "alignment3D.DDFA_V2.speed_cpu",
        "documentation": {}
    },
    {
        "label": "TDDFA",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.TDDFA",
        "description": "alignment3D.DDFA_V2.TDDFA",
        "peekOfCode": "class TDDFA(object):\n    \"\"\"TDDFA: named Three-D Dense Face Alignment (TDDFA)\"\"\"\n    def __init__(self, **kvs):\n        torch.set_grad_enabled(False)\n        # load BFM\n        self.bfm = BFMModel(\n            bfm_fp=kvs.get('bfm_fp', make_abs_path('configs/bfm_noneck_v3.pkl')),\n            shape_dim=kvs.get('shape_dim', 40),\n            exp_dim=kvs.get('exp_dim', 10)\n        )",
        "detail": "alignment3D.DDFA_V2.TDDFA",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.TDDFA",
        "description": "alignment3D.DDFA_V2.TDDFA",
        "peekOfCode": "__author__ = 'cleardusk'\nimport os.path as osp\nimport time\nimport numpy as np\nimport cv2\nimport torch\nfrom torchvision.transforms import Compose\nimport torch.backends.cudnn as cudnn\nimport models\nfrom bfm import BFMModel",
        "detail": "alignment3D.DDFA_V2.TDDFA",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.TDDFA",
        "description": "alignment3D.DDFA_V2.TDDFA",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nclass TDDFA(object):\n    \"\"\"TDDFA: named Three-D Dense Face Alignment (TDDFA)\"\"\"\n    def __init__(self, **kvs):\n        torch.set_grad_enabled(False)\n        # load BFM\n        self.bfm = BFMModel(\n            bfm_fp=kvs.get('bfm_fp', make_abs_path('configs/bfm_noneck_v3.pkl')),\n            shape_dim=kvs.get('shape_dim', 40),\n            exp_dim=kvs.get('exp_dim', 10)",
        "detail": "alignment3D.DDFA_V2.TDDFA",
        "documentation": {}
    },
    {
        "label": "TDDFA_ONNX",
        "kind": 6,
        "importPath": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "description": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "peekOfCode": "class TDDFA_ONNX(object):\n    \"\"\"TDDFA_ONNX: the ONNX version of Three-D Dense Face Alignment (TDDFA)\"\"\"\n    def __init__(self, **kvs):\n        # torch.set_grad_enabled(False)\n        # load onnx version of BFM\n        bfm_fp = kvs.get('bfm_fp', make_abs_path('configs/bfm_noneck_v3.pkl'))\n        bfm_onnx_fp = bfm_fp.replace('.pkl', '.onnx')\n        if not osp.exists(bfm_onnx_fp):\n            convert_bfm_to_onnx(\n                bfm_onnx_fp,",
        "detail": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "description": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "peekOfCode": "__author__ = 'cleardusk'\nimport os.path as osp\nimport numpy as np\nimport cv2\nimport onnxruntime\nfrom utils.onnx import convert_to_onnx\nfrom utils.io import _load\nfrom utils.functions import (\n    crop_img, parse_roi_box_from_bbox, parse_roi_box_from_landmark,\n)",
        "detail": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "documentation": {}
    },
    {
        "label": "make_abs_path",
        "kind": 5,
        "importPath": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "description": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "peekOfCode": "make_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nclass TDDFA_ONNX(object):\n    \"\"\"TDDFA_ONNX: the ONNX version of Three-D Dense Face Alignment (TDDFA)\"\"\"\n    def __init__(self, **kvs):\n        # torch.set_grad_enabled(False)\n        # load onnx version of BFM\n        bfm_fp = kvs.get('bfm_fp', make_abs_path('configs/bfm_noneck_v3.pkl'))\n        bfm_onnx_fp = bfm_fp.replace('.pkl', '.onnx')\n        if not osp.exists(bfm_onnx_fp):\n            convert_bfm_to_onnx(",
        "detail": "alignment3D.DDFA_V2.TDDFA_ONNX",
        "documentation": {}
    },
    {
        "label": "MeshController",
        "kind": 6,
        "importPath": "alignment3D.controller3D",
        "description": "alignment3D.controller3D",
        "peekOfCode": "class MeshController:\n    def __init__(self, progress_callback=None):\n        self.progress_callback = progress_callback\n        self.mesh_generator = MeshGenerator(\n            config_file=\"configs/mb1_120x120.yml\",\n            onnx=True,  \n            alpha=0.6,\n            out_dir=str(Path(__file__).parent / \"3d_results\")\n        )\n    def initialize_models(self):",
        "detail": "alignment3D.controller3D",
        "documentation": {}
    },
    {
        "label": "ScalableLabel",
        "kind": 6,
        "importPath": "alignment3D.dialog3d",
        "description": "alignment3D.dialog3d",
        "peekOfCode": "class ScalableLabel(QLabel):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setAlignment(Qt.AlignCenter)\n        self.setMinimumSize(1, 1)\n        self._pixmap = None\n    def set_pixmap(self, pixmap):\n        self._pixmap = pixmap\n        self._update_pixmap()\n    def _update_pixmap(self):",
        "detail": "alignment3D.dialog3d",
        "documentation": {}
    },
    {
        "label": "Worker",
        "kind": 6,
        "importPath": "alignment3D.dialog3d",
        "description": "alignment3D.dialog3d",
        "peekOfCode": "class Worker(QRunnable):\n    def __init__(self, fn, *args, **kwargs):\n        super().__init__()\n        self.fn = fn\n        self.args = args\n        self.kwargs = kwargs\n        self.signals = WorkerSignals()\n    @Slot()\n    def run(self):\n        try:",
        "detail": "alignment3D.dialog3d",
        "documentation": {}
    },
    {
        "label": "WorkerSignals",
        "kind": 6,
        "importPath": "alignment3D.dialog3d",
        "description": "alignment3D.dialog3d",
        "peekOfCode": "class WorkerSignals(QObject):\n    finished = Signal()\n    error = Signal(str)\n    result = Signal(object)\nclass FaceSwapUI(QMainWindow):\n    update_video_frame_signal = Signal(np.ndarray)\n    def __init__(self, target_video_path=None, cached_swapped_video=None, theme_name=\"dark\"):\n        super().__init__()\n        apply_theme(self, theme_name)",
        "detail": "alignment3D.dialog3d",
        "documentation": {}
    },
    {
        "label": "FaceSwapUI",
        "kind": 6,
        "importPath": "alignment3D.dialog3d",
        "description": "alignment3D.dialog3d",
        "peekOfCode": "class FaceSwapUI(QMainWindow):\n    update_video_frame_signal = Signal(np.ndarray)\n    def __init__(self, target_video_path=None, cached_swapped_video=None, theme_name=\"dark\"):\n        super().__init__()\n        apply_theme(self, theme_name)\n        self.setWindowTitle(\"PixelPersona\")\n        self.setGeometry(100, 100, 1200, 800)\n        self.setMinimumSize(1000, 700)\n        self.thread_pool = QThreadPool()\n        self.source_face_data = None",
        "detail": "alignment3D.dialog3d",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "core.apply_age",
        "description": "core.apply_age",
        "peekOfCode": "def preprocess(img, size):\n    img = cv2.resize(img, size)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) / 255.0\n    img = (img - 0.5) / 0.5  \n    img = img.transpose(2, 0, 1)  \n    img = np.expand_dims(img, axis=0)\n    return img\ndef postprocess(output, size):\n    img = np.clip(output, -1, 1)",
        "detail": "core.apply_age",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "kind": 2,
        "importPath": "core.apply_age",
        "description": "core.apply_age",
        "peekOfCode": "def postprocess(output, size):\n    img = np.clip(output, -1, 1)\n    img = (img + 1) / 2  \n    img = img.transpose(1, 2, 0)  \n    img = (img * 255.0).clip(0, 255).astype(np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    img = cv2.resize(img, size)\n    return img\ndef apply_ageing(img: np.ndarray, target_age: int):\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']",
        "detail": "core.apply_age",
        "documentation": {}
    },
    {
        "label": "apply_ageing",
        "kind": 2,
        "importPath": "core.apply_age",
        "description": "core.apply_age",
        "peekOfCode": "def apply_ageing(img: np.ndarray, target_age: int):\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    session = ort.InferenceSession(r\"Models\\styleganex_age.onnx\", providers=providers)\n    print(\"Model input names and shapes:\")\n    for inp in session.get_inputs():\n        print(f\"  {inp.name}: {inp.shape}\")\n    img_bg = preprocess(img, (384, 384))\n    img_tgt = preprocess(img, (256, 256))\n    direction = np.interp(target_age, [-100, 100], [2.5, -2.5]).astype(np.float32)\n    direction = np.array(direction, dtype=np.float32)",
        "detail": "core.apply_age",
        "documentation": {}
    },
    {
        "label": "manual_blend",
        "kind": 2,
        "importPath": "core.color_blender",
        "description": "core.color_blender",
        "peekOfCode": "def manual_blend(original: np.ndarray, swapped: np.ndarray, bbox: np.ndarray, alpha: float, softness: float, custom_mask: np.ndarray = None) -> np.ndarray:\n    x1, y1, x2, y2 = bbox.astype(int)\n    img_h, img_w = original.shape[:2]\n    x1 = max(0, min(x1, img_w))\n    y1 = max(0, min(y1, img_h))\n    x2 = max(0, min(x2, img_w))\n    y2 = max(0, min(y2, img_h))\n    if x1 >= x2:\n        x1, x2 = max(0, x2-1), min(img_w, x1+1)\n    if y1 >= y2:",
        "detail": "core.color_blender",
        "documentation": {}
    },
    {
        "label": "apply_face_specific_blending",
        "kind": 2,
        "importPath": "core.color_blender",
        "description": "core.color_blender",
        "peekOfCode": "def apply_face_specific_blending(original: np.ndarray, swapped: np.ndarray, bbox: np.ndarray, \n                                blend_strength: float, edge_softness: float, quality: float = 1.0) -> np.ndarray:\n    alpha = blend_strength / 100.0\n    strength = quality / 100.0\n    result = manual_blend(original, swapped, bbox, alpha, edge_softness, strength)\n    if quality < 80 and edge_softness > 0:\n        kernel_size = max(1, int((100 - quality) / 20) * 2 + 1)\n        result = cv2.GaussianBlur(result, (kernel_size, kernel_size), 0.5)\n    return result\ndef _match_channel_stats(source_channel: np.ndarray, target_channel: np.ndarray) -> np.ndarray:",
        "detail": "core.color_blender",
        "documentation": {}
    },
    {
        "label": "apply_color_correction",
        "kind": 2,
        "importPath": "core.color_blender",
        "description": "core.color_blender",
        "peekOfCode": "def apply_color_correction(original_img: np.ndarray, swapped_face_img: np.ndarray, target_face_bbox: np.ndarray) -> np.ndarray:\n    try:\n        x1, y1, x2, y2 = target_face_bbox.astype(int)\n        img_h, img_w = original_img.shape[:2]\n        x1 = max(0, min(x1, img_w))\n        y1 = max(0, min(y1, img_h))\n        x2 = max(0, min(x2, img_w))\n        y2 = max(0, min(y2, img_h))\n        if x1 >= x2 or y1 >= y2:\n            print(\"Warning: Invalid bbox for color correction\")",
        "detail": "core.color_blender",
        "documentation": {}
    },
    {
        "label": "FaceSwapController",
        "kind": 6,
        "importPath": "core.controller",
        "description": "core.controller",
        "peekOfCode": "class FaceSwapController:\n    def __init__(self, processor_instance=None):\n        self.processor = processor_instance\n        self.detector = FaceDetector()\n        self.swapper = FaceSwapperModel()\n        self.custom_mask = None\n        self.current_mask = None\n        self.tracked_data = None\n    def initialize_models(self) -> bool:\n        self.processor.progress_updated.emit(\"🚀 Initializing models...\")",
        "detail": "core.controller",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.controller",
        "description": "core.controller",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass FaceSwapController:\n    def __init__(self, processor_instance=None):\n        self.processor = processor_instance\n        self.detector = FaceDetector()\n        self.swapper = FaceSwapperModel()\n        self.custom_mask = None\n        self.current_mask = None\n        self.tracked_data = None\n    def initialize_models(self) -> bool:",
        "detail": "core.controller",
        "documentation": {}
    },
    {
        "label": "FaceDetector",
        "kind": 6,
        "importPath": "core.face_detector",
        "description": "core.face_detector",
        "peekOfCode": "class FaceDetector:\n    \"\"\"A wrapper class for the insightface face detection model.\"\"\"\n    def __init__(self):\n        self.model = None\n        self._setup_model_path()\n        self.load_model()\n    def _setup_model_path(self):\n        \"\"\"Set up the model path for insightface models.\"\"\"\n        possible_model_dirs = [\n            Path(\"models\"),",
        "detail": "core.face_detector",
        "documentation": {}
    },
    {
        "label": "FaceSwapperModel",
        "kind": 6,
        "importPath": "core.face_swapper",
        "description": "core.face_swapper",
        "peekOfCode": "class FaceSwapperModel:\n    def __init__(self):\n        self.model = None\n        self._model_loaded = False\n    def _find_model_path(self) -> str:\n        possible_paths = [\n            Path(\"models/inswapper_128.onnx\"),\n            Path(\"Models/inswapper_128.onnx\"),\n            Path(\"model/inswapper_128.onnx\"),\n            Path(__file__).parent / \"models\" / \"inswapper_128.onnx\",",
        "detail": "core.face_swapper",
        "documentation": {}
    },
    {
        "label": "FaceProcessor",
        "kind": 6,
        "importPath": "core.face_swapper",
        "description": "core.face_swapper",
        "peekOfCode": "class FaceProcessor:\n    def __init__(self, gpu_ctx_id: int = 0, det_size: Tuple[int, int] = (640, 640)):\n        self.gpu_ctx_id = gpu_ctx_id\n        self.det_size = det_size\n        self.app = None\n        self.face_swapper = None\n        self._initialized = False\n        self._initialize()\n    def _initialize(self) -> bool:\n        try:",
        "detail": "core.face_swapper",
        "documentation": {}
    },
    {
        "label": "HaarFaceDetector",
        "kind": 6,
        "importPath": "core.face_swapper",
        "description": "core.face_swapper",
        "peekOfCode": "class HaarFaceDetector:\n    def __init__(self, cascade_path: Optional[str] = None):\n        if cascade_path is None:\n            self.cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n        else:\n            self.cascade_path = cascade_path\n        self.face_cascade = cv2.CascadeClassifier(self.cascade_path)\n        if self.face_cascade.empty():\n            raise IOError(f\"Error: Could not load Haar cascade from: {self.cascade_path}\")\n    def detect_faces(self, image_frame: np.ndarray, ",
        "detail": "core.face_swapper",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.face_swapper",
        "description": "core.face_swapper",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass FaceSwapperModel:\n    def __init__(self):\n        self.model = None\n        self._model_loaded = False\n    def _find_model_path(self) -> str:\n        possible_paths = [\n            Path(\"models/inswapper_128.onnx\"),\n            Path(\"Models/inswapper_128.onnx\"),\n            Path(\"model/inswapper_128.onnx\"),",
        "detail": "core.face_swapper",
        "documentation": {}
    },
    {
        "label": "check_models",
        "kind": 2,
        "importPath": "core.model_checker",
        "description": "core.model_checker",
        "peekOfCode": "def check_models():\n    print(\"🔍 Checking Face Swap Models Setup...\")\n    print(\"=\" * 50)\n    required_models = {\n        'inswapper_128.onnx': {\n            'size_mb': 528,\n            'url': 'https://github.com/deepinsight/insightface/releases/download/v0.7/inswapper_128.onnx'\n        }\n    }\n    possible_dirs = [",
        "detail": "core.model_checker",
        "documentation": {}
    },
    {
        "label": "FaceSwapProcessor",
        "kind": 6,
        "importPath": "core.process",
        "description": "core.process",
        "peekOfCode": "class FaceSwapProcessor(QObject):\n    progress_updated = Signal(str)\n    def __init__(self):\n        super().__init__()\n        self.controller = FaceSwapController(processor_instance=self)\n        self.is_initialized = False\n        self.is_processing = False\n        self._processing_lock = threading.Lock()\n        self.default_options = {\n            'mode': 'default',",
        "detail": "core.process",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.process",
        "description": "core.process",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass FaceSwapProcessor(QObject):\n    progress_updated = Signal(str)\n    def __init__(self):\n        super().__init__()\n        self.controller = FaceSwapController(processor_instance=self)\n        self.is_initialized = False\n        self.is_processing = False\n        self._processing_lock = threading.Lock()\n        self.default_options = {",
        "detail": "core.process",
        "documentation": {}
    },
    {
        "label": "apply_face_enhancement",
        "kind": 2,
        "importPath": "core.processors",
        "description": "core.processors",
        "peekOfCode": "def apply_face_enhancement(image, enhancement_type='quality', strength=50):\n    import cv2\n    import numpy as np\n    if enhancement_type == 'sharpness':\n        gaussian = cv2.GaussianBlur(image, (0, 0), strength / 10)\n        enhanced = cv2.addWeighted(image, 1.5, gaussian, -0.5, 0)\n    elif enhancement_type == 'smoothing':\n        enhanced = cv2.bilateralFilter(image, int(strength / 5), strength, strength)\n    elif enhancement_type == 'color':\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)",
        "detail": "core.processors",
        "documentation": {}
    },
    {
        "label": "create_face_mask",
        "kind": 2,
        "importPath": "core.processors",
        "description": "core.processors",
        "peekOfCode": "def create_face_mask(face_bbox, image_shape, expand_ratio=1.2):\n    import cv2\n    import numpy as np\n    x1, y1, x2, y2 = face_bbox.astype(int)\n    h, w = image_shape[:2]\n    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n    w_half = int((x2 - x1) * expand_ratio / 2)\n    h_half = int((y2 - y1) * expand_ratio / 2)\n    x1 = max(0, cx - w_half)\n    x2 = min(w, cx + w_half)",
        "detail": "core.processors",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "core.processors",
        "description": "core.processors",
        "peekOfCode": "__all__ = ['apply_color_correction', 'manual_blend']\ndef apply_face_enhancement(image, enhancement_type='quality', strength=50):\n    import cv2\n    import numpy as np\n    if enhancement_type == 'sharpness':\n        gaussian = cv2.GaussianBlur(image, (0, 0), strength / 10)\n        enhanced = cv2.addWeighted(image, 1.5, gaussian, -0.5, 0)\n    elif enhancement_type == 'smoothing':\n        enhanced = cv2.bilateralFilter(image, int(strength / 5), strength, strength)\n    elif enhancement_type == 'color':",
        "detail": "core.processors",
        "documentation": {}
    },
    {
        "label": "Colors",
        "kind": 6,
        "importPath": "core.requirements_checker",
        "description": "core.requirements_checker",
        "peekOfCode": "class Colors:\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    RED = '\\033[91m'\n    BLUE = '\\033[94m'\n    RESET = '\\033[0m'\n    BOLD = '\\033[1m'\nclass RequirementsChecker:\n    def __init__(self):\n        self.missing_packages = []",
        "detail": "core.requirements_checker",
        "documentation": {}
    },
    {
        "label": "RequirementsChecker",
        "kind": 6,
        "importPath": "core.requirements_checker",
        "description": "core.requirements_checker",
        "peekOfCode": "class RequirementsChecker:\n    def __init__(self):\n        self.missing_packages = []\n        self.missing_models = []\n        self.errors = []\n        self.warnings = []\n    def print_header(self):\n        print(f\"\\n{Colors.BOLD}{Colors.BLUE}{'='*60}{Colors.RESET}\")\n        print(f\"{Colors.BOLD}Face Swapping Application - Requirements Checker{Colors.RESET}\")\n        print(f\"{Colors.BLUE}{'='*60}{Colors.RESET}\\n\")",
        "detail": "core.requirements_checker",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "core.requirements_checker",
        "description": "core.requirements_checker",
        "peekOfCode": "def main():\n    import argparse\n    parser = argparse.ArgumentParser(description='Check requirements for Face Swapping Application')\n    parser.add_argument('--auto-install', '-a', action='store_true',\n                        help='Automatically install missing packages and download models')\n    parser.add_argument('--run-app', '-r', action='store_true',\n                        help='Run the application after checking requirements')\n    args = parser.parse_args()\n    checker = RequirementsChecker()\n    success = checker.run_checks(auto_install=args.auto_install)",
        "detail": "core.requirements_checker",
        "documentation": {}
    },
    {
        "label": "REQUIRED_PACKAGES",
        "kind": 5,
        "importPath": "core.requirements_checker",
        "description": "core.requirements_checker",
        "peekOfCode": "REQUIRED_PACKAGES = {\n    'PySide6': '6.0.0',\n    'numpy': '1.19.0',\n    'opencv-python': '4.5.0',\n    'insightface': '0.7.0',\n    'onnxruntime': '1.12.0',\n}\nREQUIRED_MODELS = {\n    'inswapper_128.onnx': {\n        'url': 'https://github.com/deepinsight/insightface/releases/download/v0.7/inswapper_128.onnx',",
        "detail": "core.requirements_checker",
        "documentation": {}
    },
    {
        "label": "REQUIRED_MODELS",
        "kind": 5,
        "importPath": "core.requirements_checker",
        "description": "core.requirements_checker",
        "peekOfCode": "REQUIRED_MODELS = {\n    'inswapper_128.onnx': {\n        'url': 'https://github.com/deepinsight/insightface/releases/download/v0.7/inswapper_128.onnx',\n        'size': 554253681,  \n        'md5': 'a3a155b90354160350efd66fed6b3d80',\n        'path': 'models/inswapper_128.onnx'\n    },\n    'buffalo_l': {\n        'url': 'https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip',\n        'size': 281906913,",
        "detail": "core.requirements_checker",
        "documentation": {}
    },
    {
        "label": "extract_frames",
        "kind": 2,
        "importPath": "core.video_utils",
        "description": "core.video_utils",
        "peekOfCode": "def extract_frames(video_path: Union[str, Path], output_dir: Union[str, Path], progress_callback=None):\n    if progress_callback: \n        progress_callback(f\"🎞️ Extracting frames from video...\")\n    if isinstance(video_path, str):\n        video_path = Path(video_path)\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n    cap = cv2.VideoCapture(str(video_path))\n    if not cap.isOpened():\n        raise FileNotFoundError(f\"Cannot open video file: {video_path}\")",
        "detail": "core.video_utils",
        "documentation": {}
    },
    {
        "label": "extract_audio",
        "kind": 2,
        "importPath": "core.video_utils",
        "description": "core.video_utils",
        "peekOfCode": "def extract_audio(video_path: Union[str, Path], output_audio_path: Union[str, Path], progress_callback=None):\n    if not MOVIEPY_AVAILABLE:\n        if progress_callback:\n            progress_callback(\"⚠️ Audio features disabled: moviepy not installed.\")\n        return False\n    if isinstance(video_path, str):\n        video_path = Path(video_path)\n    if isinstance(output_audio_path, str):\n        output_audio_path = Path(output_audio_path)\n    if progress_callback:",
        "detail": "core.video_utils",
        "documentation": {}
    },
    {
        "label": "reconstruct_video",
        "kind": 2,
        "importPath": "core.video_utils",
        "description": "core.video_utils",
        "peekOfCode": "def reconstruct_video(frames_dir: Union[str, Path], output_path: Union[str, Path], fps: float, audio_path: Optional[Union[str, Path]] = None, progress_callback=None):\n    if progress_callback:\n        progress_callback(\"🎬 Reconstructing video from frames...\")\n    if isinstance(frames_dir, str):\n        frames_dir = Path(frames_dir)\n    if isinstance(output_path, str):\n        output_path = Path(output_path)\n    if isinstance(audio_path, str) and audio_path:\n        audio_path = Path(audio_path)\n    frame_paths = sorted(glob.glob(str(frames_dir / '*.jpg')))",
        "detail": "core.video_utils",
        "documentation": {}
    },
    {
        "label": "has_audio_track",
        "kind": 2,
        "importPath": "core.video_utils",
        "description": "core.video_utils",
        "peekOfCode": "def has_audio_track(video_path: Union[str, Path]):\n    if not MOVIEPY_AVAILABLE:\n        return False\n    try:\n        if isinstance(video_path, str):\n            video_path = Path(video_path)\n        video_clip = VideoFileClip(str(video_path))\n        has_audio = video_clip.audio is not None\n        video_clip.close()\n        return has_audio",
        "detail": "core.video_utils",
        "documentation": {}
    },
    {
        "label": "get_video_info",
        "kind": 2,
        "importPath": "core.video_utils",
        "description": "core.video_utils",
        "peekOfCode": "def get_video_info(video_path: Union[str, Path]):\n    if isinstance(video_path, str):\n        video_path = Path(video_path)\n    cap = cv2.VideoCapture(str(video_path))\n    if not cap.isOpened():\n        return None\n    info = {\n        'fps': cap.get(cv2.CAP_PROP_FPS),\n        'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n        'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),",
        "detail": "core.video_utils",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "Models.Sound.configs.config",
        "description": "Models.Sound.configs.config",
        "peekOfCode": "class Config:\n    def __init__(self):\n        self.device = \"cuda:0\"\n        self.is_half = True\n        self.use_jit = False\n        self.n_cpu = 0\n        self.gpu_name = None\n        self.json_config = self.load_config_json()\n        self.gpu_mem = None\n        (",
        "detail": "Models.Sound.configs.config",
        "documentation": {}
    },
    {
        "label": "singleton_variable",
        "kind": 2,
        "importPath": "Models.Sound.configs.config",
        "description": "Models.Sound.configs.config",
        "peekOfCode": "def singleton_variable(func):\n    def wrapper(*args, **kwargs):\n        if not wrapper.instance:\n            wrapper.instance = func(*args, **kwargs)\n        return wrapper.instance\n    wrapper.instance = None\n    return wrapper\n@singleton_variable\nclass Config:\n    def __init__(self):",
        "detail": "Models.Sound.configs.config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.configs.config",
        "description": "Models.Sound.configs.config",
        "peekOfCode": "logger = logging.getLogger(__name__)\nversion_config_list = [\n    \"v1/32k.json\",\n    \"v1/40k.json\",\n    \"v1/48k.json\",\n    \"v2/48k.json\",\n    \"v2/32k.json\",\n]\ndef singleton_variable(func):\n    def wrapper(*args, **kwargs):",
        "detail": "Models.Sound.configs.config",
        "documentation": {}
    },
    {
        "label": "version_config_list",
        "kind": 5,
        "importPath": "Models.Sound.configs.config",
        "description": "Models.Sound.configs.config",
        "peekOfCode": "version_config_list = [\n    \"v1/32k.json\",\n    \"v1/40k.json\",\n    \"v1/48k.json\",\n    \"v2/48k.json\",\n    \"v2/32k.json\",\n]\ndef singleton_variable(func):\n    def wrapper(*args, **kwargs):\n        if not wrapper.instance:",
        "detail": "Models.Sound.configs.config",
        "documentation": {}
    },
    {
        "label": "I18nAuto",
        "kind": 6,
        "importPath": "Models.Sound.i18n.i18n",
        "description": "Models.Sound.i18n.i18n",
        "peekOfCode": "class I18nAuto:\n    def __init__(self, language=None):\n        if language in [\"Auto\", None]:\n            language = locale.getdefaultlocale()[\n                0\n            ]  # getlocale can't identify the system's language ((None, None))\n        if not os.path.exists(f\"./i18n/locale/{language}.json\"):\n            language = \"en_US\"\n        self.language = language\n        self.language_map = load_language_list(language)",
        "detail": "Models.Sound.i18n.i18n",
        "documentation": {}
    },
    {
        "label": "load_language_list",
        "kind": 2,
        "importPath": "Models.Sound.i18n.i18n",
        "description": "Models.Sound.i18n.i18n",
        "peekOfCode": "def load_language_list(language):\n    with open(f\"./i18n/locale/{language}.json\", \"r\", encoding=\"utf-8\") as f:\n        language_list = json.load(f)\n    return language_list\nclass I18nAuto:\n    def __init__(self, language=None):\n        if language in [\"Auto\", None]:\n            language = locale.getdefaultlocale()[\n                0\n            ]  # getlocale can't identify the system's language ((None, None))",
        "detail": "Models.Sound.i18n.i18n",
        "documentation": {}
    },
    {
        "label": "standard_file",
        "kind": 5,
        "importPath": "Models.Sound.i18n.locale_diff",
        "description": "Models.Sound.i18n.locale_diff",
        "peekOfCode": "standard_file = \"locale/zh_CN.json\"\n# Find all JSON files in the directory\ndir_path = \"locale/\"\nlanguages = [\n    os.path.join(dir_path, f)\n    for f in os.listdir(dir_path)\n    if f.endswith(\".json\") and f != standard_file\n]\n# Load the standard file\nwith open(standard_file, \"r\", encoding=\"utf-8\") as f:",
        "detail": "Models.Sound.i18n.locale_diff",
        "documentation": {}
    },
    {
        "label": "dir_path",
        "kind": 5,
        "importPath": "Models.Sound.i18n.locale_diff",
        "description": "Models.Sound.i18n.locale_diff",
        "peekOfCode": "dir_path = \"locale/\"\nlanguages = [\n    os.path.join(dir_path, f)\n    for f in os.listdir(dir_path)\n    if f.endswith(\".json\") and f != standard_file\n]\n# Load the standard file\nwith open(standard_file, \"r\", encoding=\"utf-8\") as f:\n    standard_data = json.load(f, object_pairs_hook=OrderedDict)\n# Loop through each language file",
        "detail": "Models.Sound.i18n.locale_diff",
        "documentation": {}
    },
    {
        "label": "languages",
        "kind": 5,
        "importPath": "Models.Sound.i18n.locale_diff",
        "description": "Models.Sound.i18n.locale_diff",
        "peekOfCode": "languages = [\n    os.path.join(dir_path, f)\n    for f in os.listdir(dir_path)\n    if f.endswith(\".json\") and f != standard_file\n]\n# Load the standard file\nwith open(standard_file, \"r\", encoding=\"utf-8\") as f:\n    standard_data = json.load(f, object_pairs_hook=OrderedDict)\n# Loop through each language file\nfor lang_file in languages:",
        "detail": "Models.Sound.i18n.locale_diff",
        "documentation": {}
    },
    {
        "label": "extract_i18n_strings",
        "kind": 2,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "def extract_i18n_strings(node):\n    i18n_strings = []\n    if (\n        isinstance(node, ast.Call)\n        and isinstance(node.func, ast.Name)\n        and node.func.id == \"i18n\"\n    ):\n        for arg in node.args:\n            if isinstance(arg, ast.Str):\n                i18n_strings.append(arg.s)",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "strings",
        "kind": 5,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "strings = []\nfor filename in glob.iglob(\"**/*.py\", recursive=True):\n    with open(filename, \"r\") as f:\n        code = f.read()\n        if \"I18nAuto\" in code:\n            tree = ast.parse(code)\n            i18n_strings = extract_i18n_strings(tree)\n            print(filename, len(i18n_strings))\n            strings.extend(i18n_strings)\ncode_keys = set(strings)",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "code_keys",
        "kind": 5,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "code_keys = set(strings)\n\"\"\"\nn_i18n.py\ngui_v1.py 26\napp.py 16\ninfer-web.py 147\nscan_i18n.py 0\ni18n.py 0\nlib/train/process_ckpt.py 1\n\"\"\"",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "standard_file",
        "kind": 5,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "standard_file = \"i18n/locale/zh_CN.json\"\nwith open(standard_file, \"r\", encoding=\"utf-8\") as f:\n    standard_data = json.load(f, object_pairs_hook=OrderedDict)\nstandard_keys = set(standard_data.keys())\n# Define the standard file name\nunused_keys = standard_keys - code_keys\nprint(\"Unused keys:\", len(unused_keys))\nfor unused_key in unused_keys:\n    print(\"\\t\", unused_key)\nmissing_keys = code_keys - standard_keys",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "standard_keys",
        "kind": 5,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "standard_keys = set(standard_data.keys())\n# Define the standard file name\nunused_keys = standard_keys - code_keys\nprint(\"Unused keys:\", len(unused_keys))\nfor unused_key in unused_keys:\n    print(\"\\t\", unused_key)\nmissing_keys = code_keys - standard_keys\nprint(\"Missing keys:\", len(missing_keys))\nfor missing_key in missing_keys:\n    print(\"\\t\", missing_key)",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "unused_keys",
        "kind": 5,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "unused_keys = standard_keys - code_keys\nprint(\"Unused keys:\", len(unused_keys))\nfor unused_key in unused_keys:\n    print(\"\\t\", unused_key)\nmissing_keys = code_keys - standard_keys\nprint(\"Missing keys:\", len(missing_keys))\nfor missing_key in missing_keys:\n    print(\"\\t\", missing_key)\ncode_keys_dict = OrderedDict()\nfor s in strings:",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "missing_keys",
        "kind": 5,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "missing_keys = code_keys - standard_keys\nprint(\"Missing keys:\", len(missing_keys))\nfor missing_key in missing_keys:\n    print(\"\\t\", missing_key)\ncode_keys_dict = OrderedDict()\nfor s in strings:\n    code_keys_dict[s] = s\n# write back\nwith open(standard_file, \"w\", encoding=\"utf-8\") as f:\n    json.dump(code_keys_dict, f, ensure_ascii=False, indent=4, sort_keys=True)",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "code_keys_dict",
        "kind": 5,
        "importPath": "Models.Sound.i18n.scan_i18n",
        "description": "Models.Sound.i18n.scan_i18n",
        "peekOfCode": "code_keys_dict = OrderedDict()\nfor s in strings:\n    code_keys_dict[s] = s\n# write back\nwith open(standard_file, \"w\", encoding=\"utf-8\") as f:\n    json.dump(code_keys_dict, f, ensure_ascii=False, indent=4, sort_keys=True)\n    f.write(\"\\n\")",
        "detail": "Models.Sound.i18n.scan_i18n",
        "documentation": {}
    },
    {
        "label": "DioF0Predictor",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.DioF0Predictor",
        "description": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.DioF0Predictor",
        "peekOfCode": "class DioF0Predictor(F0Predictor):\n    def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):\n        self.hop_length = hop_length\n        self.f0_min = f0_min\n        self.f0_max = f0_max\n        self.sampling_rate = sampling_rate\n    def interpolate_f0(self, f0):\n        \"\"\"\n        对F0进行插值处理\n        \"\"\"",
        "detail": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.DioF0Predictor",
        "documentation": {}
    },
    {
        "label": "F0Predictor",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "description": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "peekOfCode": "class F0Predictor(object):\n    def compute_f0(self, wav, p_len):\n        \"\"\"\n        input: wav:[signal_length]\n               p_len:int\n        output: f0:[signal_length//hop_length]\n        \"\"\"\n        pass\n    def compute_f0_uv(self, wav, p_len):\n        \"\"\"",
        "detail": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.F0Predictor",
        "documentation": {}
    },
    {
        "label": "HarvestF0Predictor",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.HarvestF0Predictor",
        "description": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.HarvestF0Predictor",
        "peekOfCode": "class HarvestF0Predictor(F0Predictor):\n    def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):\n        self.hop_length = hop_length\n        self.f0_min = f0_min\n        self.f0_max = f0_max\n        self.sampling_rate = sampling_rate\n    def interpolate_f0(self, f0):\n        \"\"\"\n        对F0进行插值处理\n        \"\"\"",
        "detail": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.HarvestF0Predictor",
        "documentation": {}
    },
    {
        "label": "PMF0Predictor",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.PMF0Predictor",
        "description": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.PMF0Predictor",
        "peekOfCode": "class PMF0Predictor(F0Predictor):\n    def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):\n        self.hop_length = hop_length\n        self.f0_min = f0_min\n        self.f0_max = f0_max\n        self.sampling_rate = sampling_rate\n    def interpolate_f0(self, f0):\n        \"\"\"\n        对F0进行插值处理\n        \"\"\"",
        "detail": "Models.Sound.infer.lib.infer_pack.modules.F0Predictor.PMF0Predictor",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions",
        "description": "Models.Sound.infer.lib.infer_pack.attentions",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        window_size=10,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions",
        "description": "Models.Sound.infer.lib.infer_pack.attentions",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        proximal_bias=False,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions",
        "description": "Models.Sound.infer.lib.infer_pack.attentions",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    def __init__(\n        self,\n        channels,\n        out_channels,\n        n_heads,\n        p_dropout=0.0,\n        window_size=None,\n        heads_share=True,\n        block_length=None,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions",
        "documentation": {}
    },
    {
        "label": "FFN",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions",
        "description": "Models.Sound.infer.lib.infer_pack.attentions",
        "peekOfCode": "class FFN(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        filter_channels,\n        kernel_size,\n        p_dropout=0.0,\n        activation: str = None,\n        causal=False,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        window_size=10,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        proximal_bias=False,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    def __init__(\n        self,\n        channels,\n        out_channels,\n        n_heads,\n        p_dropout=0.0,\n        window_size=None,\n        heads_share=True,\n        block_length=None,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "documentation": {}
    },
    {
        "label": "FFN",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "peekOfCode": "class FFN(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        filter_channels,\n        kernel_size,\n        p_dropout=0.0,\n        activation: str = None,\n        causal=False,",
        "detail": "Models.Sound.infer.lib.infer_pack.attentions_onnx",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\n# def convert_pad_shape(pad_shape):\n#     l = pad_shape[::-1]\n#     pad_shape = [item for sublist in l for item in sublist]\n#     return pad_shape",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\n# def convert_pad_shape(pad_shape):\n#     l = pad_shape[::-1]\n#     pad_shape = [item for sublist in l for item in sublist]\n#     return pad_shape\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\n    \"\"\"KL(P||Q)\"\"\"\n    kl = (logs_q - logs_p) - 0.5\n    kl += (",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "kl_divergence",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def kl_divergence(m_p, logs_p, m_q, logs_q):\n    \"\"\"KL(P||Q)\"\"\"\n    kl = (logs_q - logs_p) - 0.5\n    kl += (\n        0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n    )\n    return kl\ndef rand_gumbel(shape):\n    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "rand_gumbel",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def rand_gumbel(shape):\n    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n    return -torch.log(-torch.log(uniform_samples))\ndef rand_gumbel_like(x):\n    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n    return g\ndef slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "rand_gumbel_like",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def rand_gumbel_like(x):\n    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n    return g\ndef slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, :, idx_str:idx_end]\n    return ret",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "slice_segments",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, :, idx_str:idx_end]\n    return ret\ndef slice_segments2(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :segment_size])\n    for i in range(x.size(0)):",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "slice_segments2",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def slice_segments2(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, idx_str:idx_end]\n    return ret\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "rand_slice_segments",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def rand_slice_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:\n        x_lengths = t\n    ids_str_max = x_lengths - segment_size + 1\n    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n    ret = slice_segments(x, ids_str, segment_size)\n    return ret, ids_str\ndef get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    position = torch.arange(length, dtype=torch.float)",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "get_timing_signal_1d",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    position = torch.arange(length, dtype=torch.float)\n    num_timescales = channels // 2\n    log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / (\n        num_timescales - 1\n    )\n    inv_timescales = min_timescale * torch.exp(\n        torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment\n    )\n    scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "add_timing_signal_1d",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return x + signal.to(dtype=x.dtype, device=x.device)\ndef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\ndef subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "cat_timing_signal_1d",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\ndef subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n    return mask\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "subsequent_mask",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n    return mask\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "fused_add_tanh_sigmoid_multiply",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts\n# def convert_pad_shape(pad_shape):\n#     l = pad_shape[::-1]\n#     pad_shape = [item for sublist in l for item in sublist]",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "convert_pad_shape",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def convert_pad_shape(pad_shape: List[List[int]]) -> List[int]:\n    return torch.tensor(pad_shape).flip(0).reshape(-1).int().tolist()\ndef shift_1d(x):\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n    return x\ndef sequence_mask(length: torch.Tensor, max_length: Optional[int] = None):\n    if max_length is None:\n        max_length = length.max()\n    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n    return x.unsqueeze(0) < length.unsqueeze(1)",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "shift_1d",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def shift_1d(x):\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n    return x\ndef sequence_mask(length: torch.Tensor, max_length: Optional[int] = None):\n    if max_length is None:\n        max_length = length.max()\n    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n    return x.unsqueeze(0) < length.unsqueeze(1)\ndef generate_path(duration, mask):\n    \"\"\"",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def sequence_mask(length: torch.Tensor, max_length: Optional[int] = None):\n    if max_length is None:\n        max_length = length.max()\n    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n    return x.unsqueeze(0) < length.unsqueeze(1)\ndef generate_path(duration, mask):\n    \"\"\"\n    duration: [b, 1, t_x]\n    mask: [b, 1, t_y, t_x]\n    \"\"\"",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def generate_path(duration, mask):\n    \"\"\"\n    duration: [b, 1, t_x]\n    mask: [b, 1, t_y, t_x]\n    \"\"\"\n    device = duration.device\n    b, _, t_y, t_x = mask.shape\n    cum_duration = torch.cumsum(duration, -1)\n    cum_duration_flat = cum_duration.view(b * t_x)\n    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "clip_grad_value_",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.commons",
        "description": "Models.Sound.infer.lib.infer_pack.commons",
        "peekOfCode": "def clip_grad_value_(parameters, clip_value, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    if clip_value is not None:\n        clip_value = float(clip_value)\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)",
        "detail": "Models.Sound.infer.lib.infer_pack.commons",
        "documentation": {}
    },
    {
        "label": "TextEncoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class TextEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingBlock",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class ResidualCouplingBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        n_flows=4,\n        gin_channels=0,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "PosteriorEncoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class PosteriorEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "Generator",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class Generator(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SineGen",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class SineGen(torch.nn.Module):\n    \"\"\"Definition of sine generator\n    SineGen(samp_rate, harmonic_num = 0,\n            sine_amp = 0.1, noise_std = 0.003,\n            voiced_threshold = 0,\n            flag_for_pulse=False)\n    samp_rate: sampling rate in Hz\n    harmonic_num: number of harmonic overtones (default 0)\n    sine_amp: amplitude of sine-wavefrom (default 0.1)\n    noise_std: std of Gaussian noise (default 0.003)",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SourceModuleHnNSF",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class SourceModuleHnNSF(torch.nn.Module):\n    \"\"\"SourceModule for hn-nsf\n    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,\n                 add_noise_std=0.003, voiced_threshod=0)\n    sampling_rate: sampling_rate in Hz\n    harmonic_num: number of harmonic above F0 (default: 0)\n    sine_amp: amplitude of sine source signal (default: 0.1)\n    add_noise_std: std of additive Gaussian noise (default: 0.003)\n        note that amplitude of noise in unvoiced is decided\n        by sine_amp",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "GeneratorNSF",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class GeneratorNSF(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs256NSFsid",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class SynthesizerTrnMs256NSFsid(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs768NSFsid",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class SynthesizerTrnMs768NSFsid(SynthesizerTrnMs256NSFsid):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs256NSFsid_nono",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class SynthesizerTrnMs256NSFsid_nono(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMs768NSFsid_nono",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class SynthesizerTrnMs768NSFsid_nono(SynthesizerTrnMs256NSFsid_nono):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminator",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminator, self).__init__()\n        periods = [2, 3, 5, 7, 11, 17]\n        # periods = [3, 5, 7, 11, 17, 23, 37]\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [\n            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n        ]\n        self.discriminators = nn.ModuleList(discs)",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminatorV2",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class MultiPeriodDiscriminatorV2(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminatorV2, self).__init__()\n        # periods = [2, 3, 5, 7, 11, 17]\n        periods = [2, 3, 5, 7, 11, 17, 23, 37]\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [\n            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n        ]\n        self.discriminators = nn.ModuleList(discs)",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "DiscriminatorS",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "DiscriminatorP",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "class DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(\n                    Conv2d(",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\nfrom infer.lib.infer_pack import attentions, commons, modules\nfrom infer.lib.infer_pack.commons import get_padding, init_weights\nhas_xpu = bool(hasattr(torch, \"xpu\") and torch.xpu.is_available())",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "has_xpu",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "has_xpu = bool(hasattr(torch, \"xpu\") and torch.xpu.is_available())\nclass TextEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "sr2sr",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.models",
        "description": "Models.Sound.infer.lib.infer_pack.models",
        "peekOfCode": "sr2sr = {\n    \"32k\": 32000,\n    \"40k\": 40000,\n    \"48k\": 48000,\n}\nclass SynthesizerTrnMs256NSFsid(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,",
        "detail": "Models.Sound.infer.lib.infer_pack.models",
        "documentation": {}
    },
    {
        "label": "TextEncoder256",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class TextEncoder256(nn.Module):\n    def __init__(\n        self,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "TextEncoder768",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class TextEncoder768(nn.Module):\n    def __init__(\n        self,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingBlock",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class ResidualCouplingBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        n_flows=4,\n        gin_channels=0,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "PosteriorEncoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class PosteriorEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "Generator",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class Generator(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "SineGen",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class SineGen(torch.nn.Module):\n    \"\"\"Definition of sine generator\n    SineGen(samp_rate, harmonic_num = 0,\n            sine_amp = 0.1, noise_std = 0.003,\n            voiced_threshold = 0,\n            flag_for_pulse=False)\n    samp_rate: sampling rate in Hz\n    harmonic_num: number of harmonic overtones (default 0)\n    sine_amp: amplitude of sine-wavefrom (default 0.1)\n    noise_std: std of Gaussian noise (default 0.003)",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "SourceModuleHnNSF",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class SourceModuleHnNSF(torch.nn.Module):\n    \"\"\"SourceModule for hn-nsf\n    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,\n                 add_noise_std=0.003, voiced_threshod=0)\n    sampling_rate: sampling_rate in Hz\n    harmonic_num: number of harmonic above F0 (default: 0)\n    sine_amp: amplitude of sine source signal (default: 0.1)\n    add_noise_std: std of additive Gaussian noise (default: 0.003)\n        note that amplitude of noise in unvoiced is decided\n        by sine_amp",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "GeneratorNSF",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class GeneratorNSF(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "SynthesizerTrnMsNSFsidM",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class SynthesizerTrnMsNSFsidM(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminator",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminator, self).__init__()\n        periods = [2, 3, 5, 7, 11, 17]\n        # periods = [3, 5, 7, 11, 17, 23, 37]\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [\n            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n        ]\n        self.discriminators = nn.ModuleList(discs)",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminatorV2",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class MultiPeriodDiscriminatorV2(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminatorV2, self).__init__()\n        # periods = [2, 3, 5, 7, 11, 17]\n        periods = [2, 3, 5, 7, 11, 17, 23, 37]\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [\n            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n        ]\n        self.discriminators = nn.ModuleList(discs)",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "DiscriminatorS",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "DiscriminatorP",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "class DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(\n                    Conv2d(",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\nfrom infer.lib.infer_pack import commons, modules\nimport infer.lib.infer_pack.attentions_onnx as attentions\nfrom infer.lib.infer_pack.commons import get_padding, init_weights",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "sr2sr",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "description": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "peekOfCode": "sr2sr = {\n    \"32k\": 32000,\n    \"40k\": 40000,\n    \"48k\": 48000,\n}\nclass SynthesizerTrnMsNSFsidM(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,",
        "detail": "Models.Sound.infer.lib.infer_pack.models_onnx",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super(LayerNorm, self).__init__()\n        self.channels = channels\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n    def forward(self, x):\n        x = x.transpose(1, -1)\n        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "ConvReluNorm",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class ConvReluNorm(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        out_channels,\n        kernel_size,\n        n_layers,\n        p_dropout,\n    ):",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "DDSConv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class DDSConv(nn.Module):\n    \"\"\"\n    Dialted and Depth-Separable Convolution\n    \"\"\"\n    def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):\n        super(DDSConv, self).__init__()\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = float(p_dropout)",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "WN",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class WN(torch.nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0,\n        p_dropout=0,\n    ):",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "ResBlock1",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "ResBlock2",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "Log",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class Log(nn.Module):\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_mask: torch.Tensor,\n        g: Optional[torch.Tensor] = None,\n        reverse: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        if not reverse:\n            y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "Flip",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class Flip(nn.Module):\n    # torch.jit.script() Compiled functions \\\n    # can't take variable number of arguments or \\\n    # use keyword-only arguments with defaults\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_mask: torch.Tensor,\n        g: Optional[torch.Tensor] = None,\n        reverse: bool = False,",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "ElementwiseAffine",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class ElementwiseAffine(nn.Module):\n    def __init__(self, channels):\n        super(ElementwiseAffine, self).__init__()\n        self.channels = channels\n        self.m = nn.Parameter(torch.zeros(channels, 1))\n        self.logs = nn.Parameter(torch.zeros(channels, 1))\n    def forward(self, x, x_mask, reverse=False, **kwargs):\n        if not reverse:\n            y = self.m + torch.exp(self.logs) * x\n            y = y * x_mask",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingLayer",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class ResidualCouplingLayer(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        p_dropout=0,\n        gin_channels=0,",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "ConvFlow",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "class ConvFlow(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        filter_channels,\n        kernel_size,\n        n_layers,\n        num_bins=10,\n        tail_bound=5.0,\n    ):",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.modules",
        "description": "Models.Sound.infer.lib.infer_pack.modules",
        "peekOfCode": "LRELU_SLOPE = 0.1\nclass LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super(LayerNorm, self).__init__()\n        self.channels = channels\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n    def forward(self, x):\n        x = x.transpose(1, -1)",
        "detail": "Models.Sound.infer.lib.infer_pack.modules",
        "documentation": {}
    },
    {
        "label": "ContentVec",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "description": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "peekOfCode": "class ContentVec:\n    def __init__(self, vec_path=\"pretrained/vec-768-layer-12.onnx\", device=None):\n        logger.info(\"Load model(s) from {}\".format(vec_path))\n        if device == \"cpu\" or device is None:\n            providers = [\"CPUExecutionProvider\"]\n        elif device == \"cuda\":\n            providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n        elif device == \"dml\":\n            providers = [\"DmlExecutionProvider\"]\n        else:",
        "detail": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "documentation": {}
    },
    {
        "label": "OnnxRVC",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "description": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "peekOfCode": "class OnnxRVC:\n    def __init__(\n        self,\n        model_path,\n        sr=40000,\n        hop_size=512,\n        vec_path=\"vec-768-layer-12\",\n        device=\"cpu\",\n    ):\n        vec_path = f\"pretrained/{vec_path}.onnx\"",
        "detail": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "documentation": {}
    },
    {
        "label": "get_f0_predictor",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "description": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "peekOfCode": "def get_f0_predictor(f0_predictor, hop_length, sampling_rate, **kargs):\n    if f0_predictor == \"pm\":\n        from lib.infer_pack.modules.F0Predictor.PMF0Predictor import PMF0Predictor\n        f0_predictor_object = PMF0Predictor(\n            hop_length=hop_length, sampling_rate=sampling_rate\n        )\n    elif f0_predictor == \"harvest\":\n        from lib.infer_pack.modules.F0Predictor.HarvestF0Predictor import (\n            HarvestF0Predictor,\n        )",
        "detail": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "description": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass ContentVec:\n    def __init__(self, vec_path=\"pretrained/vec-768-layer-12.onnx\", device=None):\n        logger.info(\"Load model(s) from {}\".format(vec_path))\n        if device == \"cpu\" or device is None:\n            providers = [\"CPUExecutionProvider\"]\n        elif device == \"cuda\":\n            providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n        elif device == \"dml\":\n            providers = [\"DmlExecutionProvider\"]",
        "detail": "Models.Sound.infer.lib.infer_pack.onnx_inference",
        "documentation": {}
    },
    {
        "label": "piecewise_rational_quadratic_transform",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.transforms",
        "description": "Models.Sound.infer.lib.infer_pack.transforms",
        "peekOfCode": "def piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,",
        "detail": "Models.Sound.infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "searchsorted",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.transforms",
        "description": "Models.Sound.infer.lib.infer_pack.transforms",
        "peekOfCode": "def searchsorted(bin_locations, inputs, eps=1e-6):\n    bin_locations[..., -1] += eps\n    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\ndef unconstrained_rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=\"linear\",",
        "detail": "Models.Sound.infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "unconstrained_rational_quadratic_spline",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.transforms",
        "description": "Models.Sound.infer.lib.infer_pack.transforms",
        "peekOfCode": "def unconstrained_rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=\"linear\",\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,",
        "detail": "Models.Sound.infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "rational_quadratic_spline",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.infer_pack.transforms",
        "description": "Models.Sound.infer.lib.infer_pack.transforms",
        "peekOfCode": "def rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    left=0.0,\n    right=1.0,\n    bottom=0.0,\n    top=1.0,",
        "detail": "Models.Sound.infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MIN_BIN_WIDTH",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.transforms",
        "description": "Models.Sound.infer.lib.infer_pack.transforms",
        "peekOfCode": "DEFAULT_MIN_BIN_WIDTH = 1e-3\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,",
        "detail": "Models.Sound.infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MIN_BIN_HEIGHT",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.transforms",
        "description": "Models.Sound.infer.lib.infer_pack.transforms",
        "peekOfCode": "DEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,",
        "detail": "Models.Sound.infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MIN_DERIVATIVE",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.infer_pack.transforms",
        "description": "Models.Sound.infer.lib.infer_pack.transforms",
        "peekOfCode": "DEFAULT_MIN_DERIVATIVE = 1e-3\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,",
        "detail": "Models.Sound.infer.lib.infer_pack.transforms",
        "documentation": {}
    },
    {
        "label": "pad_to_multiple",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.jit.get_hubert",
        "description": "Models.Sound.infer.lib.jit.get_hubert",
        "peekOfCode": "def pad_to_multiple(x, multiple, dim=-1, value=0):\n    # Inspired from https://github.com/lucidrains/local-attention/blob/master/local_attention/local_attention.py#L41\n    if x is None:\n        return None, 0\n    tsz = x.size(dim)\n    m = tsz / multiple\n    remainder = math.ceil(m) * multiple - tsz\n    if int(tsz % multiple) == 0:\n        return x, 0\n    pad_offset = (0,) * (-1 - dim) * 2",
        "detail": "Models.Sound.infer.lib.jit.get_hubert",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.jit.get_hubert",
        "description": "Models.Sound.infer.lib.jit.get_hubert",
        "peekOfCode": "def extract_features(\n    self,\n    x,\n    padding_mask=None,\n    tgt_layer=None,\n    min_layer=0,\n):\n    if padding_mask is not None:\n        x = index_put(x, padding_mask, 0)\n    x_conv = self.pos_conv(x.transpose(1, 2))",
        "detail": "Models.Sound.infer.lib.jit.get_hubert",
        "documentation": {}
    },
    {
        "label": "compute_mask_indices",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.jit.get_hubert",
        "description": "Models.Sound.infer.lib.jit.get_hubert",
        "peekOfCode": "def compute_mask_indices(\n    shape: Tuple[int, int],\n    padding_mask: Optional[torch.Tensor],\n    mask_prob: float,\n    mask_length: int,\n    mask_type: str = \"static\",\n    mask_other: float = 0.0,\n    min_masks: int = 0,\n    no_overlap: bool = False,\n    min_space: int = 0,",
        "detail": "Models.Sound.infer.lib.jit.get_hubert",
        "documentation": {}
    },
    {
        "label": "apply_mask",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.jit.get_hubert",
        "description": "Models.Sound.infer.lib.jit.get_hubert",
        "peekOfCode": "def apply_mask(self, x, padding_mask, target_list):\n    B, T, C = x.shape\n    torch.zeros_like(x)\n    if self.mask_prob > 0:\n        mask_indices = compute_mask_indices(\n            (B, T),\n            padding_mask,\n            self.mask_prob,\n            self.mask_length,\n            self.mask_selection,",
        "detail": "Models.Sound.infer.lib.jit.get_hubert",
        "documentation": {}
    },
    {
        "label": "get_hubert_model",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.jit.get_hubert",
        "description": "Models.Sound.infer.lib.jit.get_hubert",
        "peekOfCode": "def get_hubert_model(\n    model_path=\"assets/hubert/hubert_base.pt\", device=torch.device(\"cpu\")\n):\n    models, _, _ = load_model_ensemble_and_task(\n        [model_path],\n        suffix=\"\",\n    )\n    hubert_model = models[0]\n    hubert_model = hubert_model.to(device)\n    def _apply_mask(x, padding_mask, target_list):",
        "detail": "Models.Sound.infer.lib.jit.get_hubert",
        "documentation": {}
    },
    {
        "label": "get_rmvpe",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.jit.get_rmvpe",
        "description": "Models.Sound.infer.lib.jit.get_rmvpe",
        "peekOfCode": "def get_rmvpe(model_path=\"assets/rmvpe/rmvpe.pt\", device=torch.device(\"cpu\")):\n    from infer.lib.rmvpe import E2E\n    model = E2E(4, 1, (2, 2))\n    ckpt = torch.load(model_path, map_location=device)\n    model.load_state_dict(ckpt)\n    model.eval()\n    model = model.to(device)\n    return model",
        "detail": "Models.Sound.infer.lib.jit.get_rmvpe",
        "documentation": {}
    },
    {
        "label": "get_synthesizer",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.jit.get_synthesizer",
        "description": "Models.Sound.infer.lib.jit.get_synthesizer",
        "peekOfCode": "def get_synthesizer(pth_path, device=torch.device(\"cpu\")):\n    from infer.lib.infer_pack.models import (\n        SynthesizerTrnMs256NSFsid,\n        SynthesizerTrnMs256NSFsid_nono,\n        SynthesizerTrnMs768NSFsid,\n        SynthesizerTrnMs768NSFsid_nono,\n    )\n    cpt = torch.load(pth_path, map_location=torch.device(\"cpu\"))\n    # tgt_sr = cpt[\"config\"][-1]\n    cpt[\"config\"][-3] = cpt[\"weight\"][\"emb_g.weight\"].shape[0]",
        "detail": "Models.Sound.infer.lib.jit.get_synthesizer",
        "documentation": {}
    },
    {
        "label": "TextAudioLoaderMultiNSFsid",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.train.data_utils",
        "description": "Models.Sound.infer.lib.train.data_utils",
        "peekOfCode": "class TextAudioLoaderMultiNSFsid(torch.utils.data.Dataset):\n    \"\"\"\n    1) loads audio, text pairs\n    2) normalizes text and converts them to sequences of integers\n    3) computes spectrograms from audio files.\n    \"\"\"\n    def __init__(self, audiopaths_and_text, hparams):\n        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n        self.max_wav_value = hparams.max_wav_value\n        self.sampling_rate = hparams.sampling_rate",
        "detail": "Models.Sound.infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "TextAudioCollateMultiNSFsid",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.train.data_utils",
        "description": "Models.Sound.infer.lib.train.data_utils",
        "peekOfCode": "class TextAudioCollateMultiNSFsid:\n    \"\"\"Zero-pads model inputs and targets\"\"\"\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text and aduio\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized]\n        \"\"\"",
        "detail": "Models.Sound.infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "TextAudioLoader",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.train.data_utils",
        "description": "Models.Sound.infer.lib.train.data_utils",
        "peekOfCode": "class TextAudioLoader(torch.utils.data.Dataset):\n    \"\"\"\n    1) loads audio, text pairs\n    2) normalizes text and converts them to sequences of integers\n    3) computes spectrograms from audio files.\n    \"\"\"\n    def __init__(self, audiopaths_and_text, hparams):\n        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n        self.max_wav_value = hparams.max_wav_value\n        self.sampling_rate = hparams.sampling_rate",
        "detail": "Models.Sound.infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "TextAudioCollate",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.train.data_utils",
        "description": "Models.Sound.infer.lib.train.data_utils",
        "peekOfCode": "class TextAudioCollate:\n    \"\"\"Zero-pads model inputs and targets\"\"\"\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text and aduio\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized]\n        \"\"\"",
        "detail": "Models.Sound.infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "DistributedBucketSampler",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.train.data_utils",
        "description": "Models.Sound.infer.lib.train.data_utils",
        "peekOfCode": "class DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\n    \"\"\"\n    Maintain similar input lengths in a batch.\n    Length groups are specified by boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n    It removes samples which are not included in the boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "Models.Sound.infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.data_utils",
        "description": "Models.Sound.infer.lib.train.data_utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom infer.lib.train.mel_processing import spectrogram_torch\nfrom infer.lib.train.utils import load_filepaths_and_text, load_wav_to_torch\nclass TextAudioLoaderMultiNSFsid(torch.utils.data.Dataset):\n    \"\"\"\n    1) loads audio, text pairs\n    2) normalizes text and converts them to sequences of integers",
        "detail": "Models.Sound.infer.lib.train.data_utils",
        "documentation": {}
    },
    {
        "label": "feature_loss",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.losses",
        "description": "Models.Sound.infer.lib.train.losses",
        "peekOfCode": "def feature_loss(fmap_r, fmap_g):\n    loss = 0\n    for dr, dg in zip(fmap_r, fmap_g):\n        for rl, gl in zip(dr, dg):\n            rl = rl.float().detach()\n            gl = gl.float()\n            loss += torch.mean(torch.abs(rl - gl))\n    return loss * 2\ndef discriminator_loss(disc_real_outputs, disc_generated_outputs):\n    loss = 0",
        "detail": "Models.Sound.infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "discriminator_loss",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.losses",
        "description": "Models.Sound.infer.lib.train.losses",
        "peekOfCode": "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n    loss = 0\n    r_losses = []\n    g_losses = []\n    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n        dr = dr.float()\n        dg = dg.float()\n        r_loss = torch.mean((1 - dr) ** 2)\n        g_loss = torch.mean(dg**2)\n        loss += r_loss + g_loss",
        "detail": "Models.Sound.infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "generator_loss",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.losses",
        "description": "Models.Sound.infer.lib.train.losses",
        "peekOfCode": "def generator_loss(disc_outputs):\n    loss = 0\n    gen_losses = []\n    for dg in disc_outputs:\n        dg = dg.float()\n        l = torch.mean((1 - dg) ** 2)\n        gen_losses.append(l)\n        loss += l\n    return loss, gen_losses\ndef kl_loss(z_p, logs_q, m_p, logs_p, z_mask):",
        "detail": "Models.Sound.infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "kl_loss",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.losses",
        "description": "Models.Sound.infer.lib.train.losses",
        "peekOfCode": "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n    \"\"\"\n    z_p, logs_q: [b, h, t_t]\n    m_p, logs_p: [b, h, t_t]\n    \"\"\"\n    z_p = z_p.float()\n    logs_q = logs_q.float()\n    m_p = m_p.float()\n    logs_p = logs_p.float()\n    z_mask = z_mask.float()",
        "detail": "Models.Sound.infer.lib.train.losses",
        "documentation": {}
    },
    {
        "label": "dynamic_range_compression_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "dynamic_range_decompression_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "def dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\ndef spectral_normalize_torch(magnitudes):\n    return dynamic_range_compression_torch(magnitudes)\ndef spectral_de_normalize_torch(magnitudes):",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "spectral_normalize_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "def spectral_normalize_torch(magnitudes):\n    return dynamic_range_compression_torch(magnitudes)\ndef spectral_de_normalize_torch(magnitudes):\n    return dynamic_range_decompression_torch(magnitudes)\n# Reusable banks\nmel_basis = {}\nhann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    \"\"\"Convert waveform into Linear-frequency Linear-amplitude spectrogram.\n    Args:",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "spectral_de_normalize_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "def spectral_de_normalize_torch(magnitudes):\n    return dynamic_range_decompression_torch(magnitudes)\n# Reusable banks\nmel_basis = {}\nhann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    \"\"\"Convert waveform into Linear-frequency Linear-amplitude spectrogram.\n    Args:\n        y             :: (B, T) - Audio waveforms\n        n_fft",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "spectrogram_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    \"\"\"Convert waveform into Linear-frequency Linear-amplitude spectrogram.\n    Args:\n        y             :: (B, T) - Audio waveforms\n        n_fft\n        sampling_rate\n        hop_size\n        win_size\n        center\n    Returns:",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "spec_to_mel_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    # MelBasis - Cache if needed\n    global mel_basis\n    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(\n            sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax\n        )\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "mel_spectrogram_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "def mel_spectrogram_torch(\n    y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False\n):\n    \"\"\"Convert waveform into Mel-frequency Log-amplitude spectrogram.\n    Args:\n        y       :: (B, T)           - Waveforms\n    Returns:\n        melspec :: (B, Freq, Frame) - Mel-frequency Log-amplitude spectrogram\n    \"\"\"\n    # Linear-frequency Linear-amplitude spectrogram :: (B, T) -> (B, Freq, Frame)",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "logger = logging.getLogger(__name__)\nMAX_WAV_VALUE = 32768.0\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression_torch(x, C=1):",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "MAX_WAV_VALUE",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "MAX_WAV_VALUE = 32768.0\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "mel_basis",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "mel_basis = {}\nhann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    \"\"\"Convert waveform into Linear-frequency Linear-amplitude spectrogram.\n    Args:\n        y             :: (B, T) - Audio waveforms\n        n_fft\n        sampling_rate\n        hop_size\n        win_size",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "hann_window",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.mel_processing",
        "description": "Models.Sound.infer.lib.train.mel_processing",
        "peekOfCode": "hann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    \"\"\"Convert waveform into Linear-frequency Linear-amplitude spectrogram.\n    Args:\n        y             :: (B, T) - Audio waveforms\n        n_fft\n        sampling_rate\n        hop_size\n        win_size\n        center",
        "detail": "Models.Sound.infer.lib.train.mel_processing",
        "documentation": {}
    },
    {
        "label": "savee",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.process_ckpt",
        "description": "Models.Sound.infer.lib.train.process_ckpt",
        "peekOfCode": "def savee(ckpt, sr, if_f0, name, epoch, version, hps):\n    try:\n        opt = OrderedDict()\n        opt[\"weight\"] = {}\n        for key in ckpt.keys():\n            if \"enc_q\" in key:\n                continue\n            opt[\"weight\"][key] = ckpt[key].half()\n        opt[\"config\"] = [\n            hps.data.filter_length // 2 + 1,",
        "detail": "Models.Sound.infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "show_info",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.process_ckpt",
        "description": "Models.Sound.infer.lib.train.process_ckpt",
        "peekOfCode": "def show_info(path):\n    try:\n        a = torch.load(path, map_location=\"cpu\")\n        return \"模型信息:%s\\n采样率:%s\\n模型是否输入音高引导:%s\\n版本:%s\" % (\n            a.get(\"info\", \"None\"),\n            a.get(\"sr\", \"None\"),\n            a.get(\"f0\", \"None\"),\n            a.get(\"version\", \"None\"),\n        )\n    except:",
        "detail": "Models.Sound.infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "extract_small_model",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.process_ckpt",
        "description": "Models.Sound.infer.lib.train.process_ckpt",
        "peekOfCode": "def extract_small_model(path, name, sr, if_f0, info, version):\n    try:\n        ckpt = torch.load(path, map_location=\"cpu\")\n        if \"model\" in ckpt:\n            ckpt = ckpt[\"model\"]\n        opt = OrderedDict()\n        opt[\"weight\"] = {}\n        for key in ckpt.keys():\n            if \"enc_q\" in key:\n                continue",
        "detail": "Models.Sound.infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "change_info",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.process_ckpt",
        "description": "Models.Sound.infer.lib.train.process_ckpt",
        "peekOfCode": "def change_info(path, info, name):\n    try:\n        ckpt = torch.load(path, map_location=\"cpu\")\n        ckpt[\"info\"] = info\n        if name == \"\":\n            name = os.path.basename(path)\n        torch.save(ckpt, \"assets/weights/%s\" % name)\n        return \"Success.\"\n    except:\n        return traceback.format_exc()",
        "detail": "Models.Sound.infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "merge",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.process_ckpt",
        "description": "Models.Sound.infer.lib.train.process_ckpt",
        "peekOfCode": "def merge(path1, path2, alpha1, sr, f0, info, name, version):\n    try:\n        def extract(ckpt):\n            a = ckpt[\"model\"]\n            opt = OrderedDict()\n            opt[\"weight\"] = {}\n            for key in a.keys():\n                if \"enc_q\" in key:\n                    continue\n                opt[\"weight\"][key] = a[key]",
        "detail": "Models.Sound.infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "i18n",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.process_ckpt",
        "description": "Models.Sound.infer.lib.train.process_ckpt",
        "peekOfCode": "i18n = I18nAuto()\ndef savee(ckpt, sr, if_f0, name, epoch, version, hps):\n    try:\n        opt = OrderedDict()\n        opt[\"weight\"] = {}\n        for key in ckpt.keys():\n            if \"enc_q\" in key:\n                continue\n            opt[\"weight\"][key] = ckpt[key].half()\n        opt[\"config\"] = [",
        "detail": "Models.Sound.infer.lib.train.process_ckpt",
        "documentation": {}
    },
    {
        "label": "HParams",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "class HParams:\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            if type(v) == dict:\n                v = HParams(**v)\n            self[k] = v\n    def keys(self):\n        return self.__dict__.keys()\n    def items(self):\n        return self.__dict__.items()",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "load_checkpoint_d",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def load_checkpoint_d(checkpoint_path, combd, sbd, optimizer=None, load_opt=1):\n    assert os.path.isfile(checkpoint_path)\n    checkpoint_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n    ##################\n    def go(model, bkey):\n        saved_state_dict = checkpoint_dict[bkey]\n        if hasattr(model, \"module\"):\n            state_dict = model.module.state_dict()\n        else:\n            state_dict = model.state_dict()",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def load_checkpoint(checkpoint_path, model, optimizer=None, load_opt=1):\n    assert os.path.isfile(checkpoint_path)\n    checkpoint_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n    saved_state_dict = checkpoint_dict[\"model\"]\n    if hasattr(model, \"module\"):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n    new_state_dict = {}\n    for k, v in state_dict.items():  # 模型需要的shape",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):\n    logger.info(\n        \"Saving model and optimizer state at epoch {} to {}\".format(\n            iteration, checkpoint_path\n        )\n    )\n    if hasattr(model, \"module\"):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "save_checkpoint_d",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def save_checkpoint_d(combd, sbd, optimizer, learning_rate, iteration, checkpoint_path):\n    logger.info(\n        \"Saving model and optimizer state at epoch {} to {}\".format(\n            iteration, checkpoint_path\n        )\n    )\n    if hasattr(combd, \"module\"):\n        state_dict_combd = combd.module.state_dict()\n    else:\n        state_dict_combd = combd.state_dict()",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "summarize",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def summarize(\n    writer,\n    global_step,\n    scalars={},\n    histograms={},\n    images={},\n    audios={},\n    audio_sampling_rate=22050,\n):\n    for k, v in scalars.items():",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "latest_checkpoint_path",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):\n    f_list = glob.glob(os.path.join(dir_path, regex))\n    f_list.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n    x = f_list[-1]\n    logger.debug(x)\n    return x\ndef plot_spectrogram_to_numpy(spectrogram):\n    global MATPLOTLIB_FLAG\n    if not MATPLOTLIB_FLAG:\n        import matplotlib",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram_to_numpy",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def plot_spectrogram_to_numpy(spectrogram):\n    global MATPLOTLIB_FLAG\n    if not MATPLOTLIB_FLAG:\n        import matplotlib\n        matplotlib.use(\"Agg\")\n        MATPLOTLIB_FLAG = True\n        mpl_logger = logging.getLogger(\"matplotlib\")\n        mpl_logger.setLevel(logging.WARNING)\n    import matplotlib.pylab as plt\n    import numpy as np",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "plot_alignment_to_numpy",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def plot_alignment_to_numpy(alignment, info=None):\n    global MATPLOTLIB_FLAG\n    if not MATPLOTLIB_FLAG:\n        import matplotlib\n        matplotlib.use(\"Agg\")\n        MATPLOTLIB_FLAG = True\n        mpl_logger = logging.getLogger(\"matplotlib\")\n        mpl_logger.setLevel(logging.WARNING)\n    import matplotlib.pylab as plt\n    import numpy as np",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "load_wav_to_torch",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def load_wav_to_torch(full_path):\n    sampling_rate, data = read(full_path)\n    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\ndef load_filepaths_and_text(filename, split=\"|\"):\n    try:\n        with open(filename, encoding=\"utf-8\") as f:\n            filepaths_and_text = [line.strip().split(split) for line in f]\n    except UnicodeDecodeError:\n        with open(filename) as f:\n            filepaths_and_text = [line.strip().split(split) for line in f]",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "load_filepaths_and_text",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def load_filepaths_and_text(filename, split=\"|\"):\n    try:\n        with open(filename, encoding=\"utf-8\") as f:\n            filepaths_and_text = [line.strip().split(split) for line in f]\n    except UnicodeDecodeError:\n        with open(filename) as f:\n            filepaths_and_text = [line.strip().split(split) for line in f]\n    return filepaths_and_text\ndef get_hparams(init=True):\n    \"\"\"",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "get_hparams",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def get_hparams(init=True):\n    \"\"\"\n    todo:\n      结尾七人组：\n        保存频率、总epoch                     done\n        bs                                    done\n        pretrainG、pretrainD                  done\n        卡号：os.en[\"CUDA_VISIBLE_DEVICES\"]   done\n        if_latest                             done\n      模型：if_f0                             done",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "get_hparams_from_dir",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def get_hparams_from_dir(model_dir):\n    config_save_path = os.path.join(model_dir, \"config.json\")\n    with open(config_save_path, \"r\") as f:\n        data = f.read()\n    config = json.loads(data)\n    hparams = HParams(**config)\n    hparams.model_dir = model_dir\n    return hparams\ndef get_hparams_from_file(config_path):\n    with open(config_path, \"r\") as f:",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "get_hparams_from_file",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def get_hparams_from_file(config_path):\n    with open(config_path, \"r\") as f:\n        data = f.read()\n    config = json.loads(data)\n    hparams = HParams(**config)\n    return hparams\ndef check_git_hash(model_dir):\n    source_dir = os.path.dirname(os.path.realpath(__file__))\n    if not os.path.exists(os.path.join(source_dir, \".git\")):\n        logger.warning(",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "check_git_hash",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def check_git_hash(model_dir):\n    source_dir = os.path.dirname(os.path.realpath(__file__))\n    if not os.path.exists(os.path.join(source_dir, \".git\")):\n        logger.warning(\n            \"{} is not a git repository, therefore hash value comparison will be ignored.\".format(\n                source_dir\n            )\n        )\n        return\n    cur_hash = subprocess.getoutput(\"git rev-parse HEAD\")",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "def get_logger(model_dir, filename=\"train.log\"):\n    global logger\n    logger = logging.getLogger(os.path.basename(model_dir))\n    logger.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\"%(asctime)s\\t%(name)s\\t%(levelname)s\\t%(message)s\")\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    h = logging.FileHandler(os.path.join(model_dir, filename))\n    h.setLevel(logging.DEBUG)\n    h.setFormatter(formatter)",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "MATPLOTLIB_FLAG",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "MATPLOTLIB_FLAG = False\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogger = logging\ndef load_checkpoint_d(checkpoint_path, combd, sbd, optimizer=None, load_opt=1):\n    assert os.path.isfile(checkpoint_path)\n    checkpoint_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n    ##################\n    def go(model, bkey):\n        saved_state_dict = checkpoint_dict[bkey]\n        if hasattr(model, \"module\"):",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.train.utils",
        "description": "Models.Sound.infer.lib.train.utils",
        "peekOfCode": "logger = logging\ndef load_checkpoint_d(checkpoint_path, combd, sbd, optimizer=None, load_opt=1):\n    assert os.path.isfile(checkpoint_path)\n    checkpoint_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n    ##################\n    def go(model, bkey):\n        saved_state_dict = checkpoint_dict[bkey]\n        if hasattr(model, \"module\"):\n            state_dict = model.module.state_dict()\n        else:",
        "detail": "Models.Sound.infer.lib.train.utils",
        "documentation": {}
    },
    {
        "label": "VocalRemoverValidationSet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "peekOfCode": "class VocalRemoverValidationSet(torch.utils.data.Dataset):\n    def __init__(self, patch_list):\n        self.patch_list = patch_list\n    def __len__(self):\n        return len(self.patch_list)\n    def __getitem__(self, idx):\n        path = self.patch_list[idx]\n        data = np.load(path)\n        X, y = data[\"X\"], data[\"y\"]\n        X_mag = np.abs(X)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "documentation": {}
    },
    {
        "label": "make_pair",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "peekOfCode": "def make_pair(mix_dir, inst_dir):\n    input_exts = [\".wav\", \".m4a\", \".mp3\", \".mp4\", \".flac\"]\n    X_list = sorted(\n        [\n            os.path.join(mix_dir, fname)\n            for fname in os.listdir(mix_dir)\n            if os.path.splitext(fname)[1] in input_exts\n        ]\n    )\n    y_list = sorted(",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "documentation": {}
    },
    {
        "label": "train_val_split",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "peekOfCode": "def train_val_split(dataset_dir, split_mode, val_rate, val_filelist):\n    if split_mode == \"random\":\n        filelist = make_pair(\n            os.path.join(dataset_dir, \"mixtures\"),\n            os.path.join(dataset_dir, \"instruments\"),\n        )\n        random.shuffle(filelist)\n        if len(val_filelist) == 0:\n            val_size = int(len(filelist) * val_rate)\n            train_filelist = filelist[:-val_size]",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "documentation": {}
    },
    {
        "label": "augment",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "peekOfCode": "def augment(X, y, reduction_rate, reduction_mask, mixup_rate, mixup_alpha):\n    perm = np.random.permutation(len(X))\n    for i, idx in enumerate(tqdm(perm)):\n        if np.random.uniform() < reduction_rate:\n            y[idx] = spec_utils.reduce_vocal_aggressively(\n                X[idx], y[idx], reduction_mask\n            )\n        if np.random.uniform() < 0.5:\n            # swap channel\n            X[idx] = X[idx, ::-1]",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "documentation": {}
    },
    {
        "label": "make_padding",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "peekOfCode": "def make_padding(width, cropsize, offset):\n    left = offset\n    roi_size = cropsize - left * 2\n    if roi_size == 0:\n        roi_size = cropsize\n    right = roi_size - (width % roi_size) + left\n    return left, right, roi_size\ndef make_training_set(filelist, cropsize, patches, sr, hop_length, n_fft, offset):\n    len_dataset = patches * len(filelist)\n    X_dataset = np.zeros((len_dataset, 2, n_fft // 2 + 1, cropsize), dtype=np.complex64)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "documentation": {}
    },
    {
        "label": "make_training_set",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "peekOfCode": "def make_training_set(filelist, cropsize, patches, sr, hop_length, n_fft, offset):\n    len_dataset = patches * len(filelist)\n    X_dataset = np.zeros((len_dataset, 2, n_fft // 2 + 1, cropsize), dtype=np.complex64)\n    y_dataset = np.zeros((len_dataset, 2, n_fft // 2 + 1, cropsize), dtype=np.complex64)\n    for i, (X_path, y_path) in enumerate(tqdm(filelist)):\n        X, y = spec_utils.cache_or_load(X_path, y_path, sr, hop_length, n_fft)\n        coef = np.max([np.abs(X).max(), np.abs(y).max()])\n        X, y = X / coef, y / coef\n        l, r, roi_size = make_padding(X.shape[2], cropsize, offset)\n        X_pad = np.pad(X, ((0, 0), (0, 0), (l, r)), mode=\"constant\")",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "documentation": {}
    },
    {
        "label": "make_validation_set",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "peekOfCode": "def make_validation_set(filelist, cropsize, sr, hop_length, n_fft, offset):\n    patch_list = []\n    patch_dir = \"cs{}_sr{}_hl{}_nf{}_of{}\".format(\n        cropsize, sr, hop_length, n_fft, offset\n    )\n    os.makedirs(patch_dir, exist_ok=True)\n    for i, (X_path, y_path) in enumerate(tqdm(filelist)):\n        basename = os.path.splitext(os.path.basename(X_path))[0]\n        X, y = spec_utils.cache_or_load(X_path, y_path, sr, hop_length, n_fft)\n        coef = np.max([np.abs(X).max(), np.abs(y).max()])",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.dataset",
        "documentation": {}
    },
    {
        "label": "Conv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "peekOfCode": "class Conv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(Conv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nout,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "documentation": {}
    },
    {
        "label": "SeperableConv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "peekOfCode": "class SeperableConv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(SeperableConv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nin,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.conv2 = Conv2DBNActiv(nout, nout, ksize, stride, pad, activ=activ)\n    def __call__(self, x):\n        skip = self.conv1(x)\n        h = self.conv2(skip)\n        return h, skip\nclass Decoder(nn.Module):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.ReLU, dropout=False\n    ):\n        super(Decoder, self).__init__()\n        self.conv = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.dropout = nn.Dropout2d(0.1) if dropout else None\n    def __call__(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n        if skip is not None:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "documentation": {}
    },
    {
        "label": "ASPPModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "peekOfCode": "class ASPPModule(nn.Module):\n    def __init__(self, nin, nout, dilations=(4, 8, 16), activ=nn.ReLU):\n        super(ASPPModule, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, None)),\n            Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ),\n        )\n        self.conv2 = Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ)\n        self.conv3 = SeperableConv2DBNActiv(\n            nin, nin, 3, 1, dilations[0], dilations[0], activ=activ",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers",
        "documentation": {}
    },
    {
        "label": "Conv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "peekOfCode": "class Conv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(Conv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nout,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "documentation": {}
    },
    {
        "label": "SeperableConv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "peekOfCode": "class SeperableConv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(SeperableConv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nin,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.conv2 = Conv2DBNActiv(nout, nout, ksize, stride, pad, activ=activ)\n    def __call__(self, x):\n        skip = self.conv1(x)\n        h = self.conv2(skip)\n        return h, skip\nclass Decoder(nn.Module):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.ReLU, dropout=False\n    ):\n        super(Decoder, self).__init__()\n        self.conv = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.dropout = nn.Dropout2d(0.1) if dropout else None\n    def __call__(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n        if skip is not None:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "documentation": {}
    },
    {
        "label": "ASPPModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "peekOfCode": "class ASPPModule(nn.Module):\n    def __init__(self, nin, nout, dilations=(4, 8, 16), activ=nn.ReLU):\n        super(ASPPModule, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, None)),\n            Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ),\n        )\n        self.conv2 = Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ)\n        self.conv3 = SeperableConv2DBNActiv(\n            nin, nin, 3, 1, dilations[0], dilations[0], activ=activ",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123812KB ",
        "documentation": {}
    },
    {
        "label": "Conv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "peekOfCode": "class Conv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(Conv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nout,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "documentation": {}
    },
    {
        "label": "SeperableConv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "peekOfCode": "class SeperableConv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(SeperableConv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nin,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.conv2 = Conv2DBNActiv(nout, nout, ksize, stride, pad, activ=activ)\n    def __call__(self, x):\n        skip = self.conv1(x)\n        h = self.conv2(skip)\n        return h, skip\nclass Decoder(nn.Module):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.ReLU, dropout=False\n    ):\n        super(Decoder, self).__init__()\n        self.conv = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.dropout = nn.Dropout2d(0.1) if dropout else None\n    def __call__(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n        if skip is not None:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "documentation": {}
    },
    {
        "label": "ASPPModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "peekOfCode": "class ASPPModule(nn.Module):\n    def __init__(self, nin, nout, dilations=(4, 8, 16), activ=nn.ReLU):\n        super(ASPPModule, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, None)),\n            Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ),\n        )\n        self.conv2 = Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ)\n        self.conv3 = SeperableConv2DBNActiv(\n            nin, nin, 3, 1, dilations[0], dilations[0], activ=activ",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_123821KB",
        "documentation": {}
    },
    {
        "label": "Conv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "peekOfCode": "class Conv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(Conv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nout,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "documentation": {}
    },
    {
        "label": "SeperableConv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "peekOfCode": "class SeperableConv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(SeperableConv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nin,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.conv2 = Conv2DBNActiv(nout, nout, ksize, stride, pad, activ=activ)\n    def __call__(self, x):\n        skip = self.conv1(x)\n        h = self.conv2(skip)\n        return h, skip\nclass Decoder(nn.Module):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.ReLU, dropout=False\n    ):\n        super(Decoder, self).__init__()\n        self.conv = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.dropout = nn.Dropout2d(0.1) if dropout else None\n    def __call__(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n        if skip is not None:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "documentation": {}
    },
    {
        "label": "ASPPModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "peekOfCode": "class ASPPModule(nn.Module):\n    def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):\n        super(ASPPModule, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, None)),\n            Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ),\n        )\n        self.conv2 = Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ)\n        self.conv3 = SeperableConv2DBNActiv(\n            nin, nin, 3, 1, dilations[0], dilations[0], activ=activ",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_33966KB",
        "documentation": {}
    },
    {
        "label": "Conv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "peekOfCode": "class Conv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(Conv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nout,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "documentation": {}
    },
    {
        "label": "SeperableConv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "peekOfCode": "class SeperableConv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(SeperableConv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nin,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.conv2 = Conv2DBNActiv(nout, nout, ksize, stride, pad, activ=activ)\n    def __call__(self, x):\n        skip = self.conv1(x)\n        h = self.conv2(skip)\n        return h, skip\nclass Decoder(nn.Module):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.ReLU, dropout=False\n    ):\n        super(Decoder, self).__init__()\n        self.conv = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.dropout = nn.Dropout2d(0.1) if dropout else None\n    def __call__(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n        if skip is not None:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "documentation": {}
    },
    {
        "label": "ASPPModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "peekOfCode": "class ASPPModule(nn.Module):\n    def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):\n        super(ASPPModule, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, None)),\n            Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ),\n        )\n        self.conv2 = Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ)\n        self.conv3 = SeperableConv2DBNActiv(\n            nin, nin, 3, 1, dilations[0], dilations[0], activ=activ",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537227KB",
        "documentation": {}
    },
    {
        "label": "Conv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "peekOfCode": "class Conv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(Conv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nout,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "documentation": {}
    },
    {
        "label": "SeperableConv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "peekOfCode": "class SeperableConv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(SeperableConv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nin,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.conv2 = Conv2DBNActiv(nout, nout, ksize, stride, pad, activ=activ)\n    def __call__(self, x):\n        skip = self.conv1(x)\n        h = self.conv2(skip)\n        return h, skip\nclass Decoder(nn.Module):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.ReLU, dropout=False\n    ):\n        super(Decoder, self).__init__()\n        self.conv = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        self.dropout = nn.Dropout2d(0.1) if dropout else None\n    def __call__(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n        if skip is not None:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "documentation": {}
    },
    {
        "label": "ASPPModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "peekOfCode": "class ASPPModule(nn.Module):\n    def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):\n        super(ASPPModule, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, None)),\n            Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ),\n        )\n        self.conv2 = Conv2DBNActiv(nin, nin, 1, 1, 0, activ=activ)\n        self.conv3 = SeperableConv2DBNActiv(\n            nin, nin, 3, 1, dilations[0], dilations[0], activ=activ",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_537238KB",
        "documentation": {}
    },
    {
        "label": "Conv2DBNActiv",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "peekOfCode": "class Conv2DBNActiv(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):\n        super(Conv2DBNActiv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                nin,\n                nout,\n                kernel_size=ksize,\n                stride=stride,\n                padding=pad,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, stride, pad, activ=activ)\n        self.conv2 = Conv2DBNActiv(nout, nout, ksize, 1, pad, activ=activ)\n    def __call__(self, x):\n        h = self.conv1(x)\n        h = self.conv2(h)\n        return h\nclass Decoder(nn.Module):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(\n        self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.ReLU, dropout=False\n    ):\n        super(Decoder, self).__init__()\n        self.conv1 = Conv2DBNActiv(nin, nout, ksize, 1, pad, activ=activ)\n        # self.conv2 = Conv2DBNActiv(nout, nout, ksize, 1, pad, activ=activ)\n        self.dropout = nn.Dropout2d(0.1) if dropout else None\n    def __call__(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "documentation": {}
    },
    {
        "label": "ASPPModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "peekOfCode": "class ASPPModule(nn.Module):\n    def __init__(self, nin, nout, dilations=(4, 8, 12), activ=nn.ReLU, dropout=False):\n        super(ASPPModule, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, None)),\n            Conv2DBNActiv(nin, nout, 1, 1, 0, activ=activ),\n        )\n        self.conv2 = Conv2DBNActiv(nin, nout, 1, 1, 0, activ=activ)\n        self.conv3 = Conv2DBNActiv(\n            nin, nout, 3, 1, dilations[0], dilations[0], activ=activ",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "documentation": {}
    },
    {
        "label": "LSTMModule",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "peekOfCode": "class LSTMModule(nn.Module):\n    def __init__(self, nin_conv, nin_lstm, nout_lstm):\n        super(LSTMModule, self).__init__()\n        self.conv = Conv2DBNActiv(nin_conv, 1, 1, 1, 0)\n        self.lstm = nn.LSTM(\n            input_size=nin_lstm, hidden_size=nout_lstm // 2, bidirectional=True\n        )\n        self.dense = nn.Sequential(\n            nn.Linear(nout_lstm, nin_lstm), nn.BatchNorm1d(nin_lstm), nn.ReLU()\n        )",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.layers_new",
        "documentation": {}
    },
    {
        "label": "ModelParameters",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "class ModelParameters(object):\n    def __init__(self, config_path=\"\"):\n        if \".pth\" == pathlib.Path(config_path).suffix:\n            import zipfile\n            with zipfile.ZipFile(config_path, \"r\") as zip:\n                self.param = json.loads(\n                    zip.read(\"param.json\"), object_pairs_hook=int_keys\n                )\n        elif \".json\" == pathlib.Path(config_path).suffix:\n            with open(config_path, \"r\") as f:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "int_keys",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "def int_keys(d):\n    r = {}\n    for k, v in d:\n        if k.isdigit():\n            k = int(k)\n        r[k] = v\n    return r\nclass ModelParameters(object):\n    def __init__(self, config_path=\"\"):\n        if \".pth\" == pathlib.Path(config_path).suffix:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param = {}\ndefault_param[\"bins\"] = 768\ndefault_param[\"unstable_bins\"] = 9  # training only\ndefault_param[\"reduction_bins\"] = 762  # training only\ndefault_param[\"sr\"] = 44100\ndefault_param[\"pre_filter_start\"] = 757\ndefault_param[\"pre_filter_stop\"] = 768\ndefault_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"bins\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"bins\"] = 768\ndefault_param[\"unstable_bins\"] = 9  # training only\ndefault_param[\"reduction_bins\"] = 762  # training only\ndefault_param[\"sr\"] = 44100\ndefault_param[\"pre_filter_start\"] = 757\ndefault_param[\"pre_filter_stop\"] = 768\ndefault_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"unstable_bins\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"unstable_bins\"] = 9  # training only\ndefault_param[\"reduction_bins\"] = 762  # training only\ndefault_param[\"sr\"] = 44100\ndefault_param[\"pre_filter_start\"] = 757\ndefault_param[\"pre_filter_stop\"] = 768\ndefault_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,\n    \"n_fft\": 960,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"reduction_bins\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"reduction_bins\"] = 762  # training only\ndefault_param[\"sr\"] = 44100\ndefault_param[\"pre_filter_start\"] = 757\ndefault_param[\"pre_filter_stop\"] = 768\ndefault_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,\n    \"n_fft\": 960,\n    \"crop_start\": 0,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"sr\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"sr\"] = 44100\ndefault_param[\"pre_filter_start\"] = 757\ndefault_param[\"pre_filter_stop\"] = 768\ndefault_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,\n    \"n_fft\": 960,\n    \"crop_start\": 0,\n    \"crop_stop\": 245,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"pre_filter_start\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"pre_filter_start\"] = 757\ndefault_param[\"pre_filter_stop\"] = 768\ndefault_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,\n    \"n_fft\": 960,\n    \"crop_start\": 0,\n    \"crop_stop\": 245,\n    \"lpf_start\": 61,  # inference only",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"pre_filter_stop\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"pre_filter_stop\"] = 768\ndefault_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,\n    \"n_fft\": 960,\n    \"crop_start\": 0,\n    \"crop_stop\": 245,\n    \"lpf_start\": 61,  # inference only\n    \"res_type\": \"polyphase\",",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"band\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"band\"] = {}\ndefault_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,\n    \"n_fft\": 960,\n    \"crop_start\": 0,\n    \"crop_stop\": 245,\n    \"lpf_start\": 61,  # inference only\n    \"res_type\": \"polyphase\",\n}",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"band\"][1]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"band\"][1] = {\n    \"sr\": 11025,\n    \"hl\": 128,\n    \"n_fft\": 960,\n    \"crop_start\": 0,\n    \"crop_stop\": 245,\n    \"lpf_start\": 61,  # inference only\n    \"res_type\": \"polyphase\",\n}\ndefault_param[\"band\"][2] = {",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "default_param[\"band\"][2]",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "peekOfCode": "default_param[\"band\"][2] = {\n    \"sr\": 44100,\n    \"hl\": 512,\n    \"n_fft\": 1536,\n    \"crop_start\": 24,\n    \"crop_stop\": 547,\n    \"hpf_start\": 81,  # inference only\n    \"res_type\": \"sinc_best\",\n}\ndef int_keys(d):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.model_param_init",
        "documentation": {}
    },
    {
        "label": "BaseASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets",
        "peekOfCode": "class BaseASPPNet(nn.Module):\n    def __init__(self, nin, ch, dilations=(4, 8, 16)):\n        super(BaseASPPNet, self).__init__()\n        self.enc1 = layers.Encoder(nin, ch, 3, 2, 1)\n        self.enc2 = layers.Encoder(ch, ch * 2, 3, 2, 1)\n        self.enc3 = layers.Encoder(ch * 2, ch * 4, 3, 2, 1)\n        self.enc4 = layers.Encoder(ch * 4, ch * 8, 3, 2, 1)\n        self.aspp = layers.ASPPModule(ch * 8, ch * 16, dilations)\n        self.dec4 = layers.Decoder(ch * (8 + 16), ch * 8, 3, 1, 1)\n        self.dec3 = layers.Decoder(ch * (4 + 8), ch * 4, 3, 1, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets",
        "documentation": {}
    },
    {
        "label": "CascadedASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets",
        "peekOfCode": "class CascadedASPPNet(nn.Module):\n    def __init__(self, n_fft):\n        super(CascadedASPPNet, self).__init__()\n        self.stg1_low_band_net = BaseASPPNet(2, 16)\n        self.stg1_high_band_net = BaseASPPNet(2, 16)\n        self.stg2_bridge = layers.Conv2DBNActiv(18, 8, 1, 1, 0)\n        self.stg2_full_band_net = BaseASPPNet(8, 16)\n        self.stg3_bridge = layers.Conv2DBNActiv(34, 16, 1, 1, 0)\n        self.stg3_full_band_net = BaseASPPNet(16, 32)\n        self.out = nn.Conv2d(32, 2, 1, bias=False)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets",
        "documentation": {}
    },
    {
        "label": "BaseASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123812KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123812KB",
        "peekOfCode": "class BaseASPPNet(nn.Module):\n    def __init__(self, nin, ch, dilations=(4, 8, 16)):\n        super(BaseASPPNet, self).__init__()\n        self.enc1 = layers.Encoder(nin, ch, 3, 2, 1)\n        self.enc2 = layers.Encoder(ch, ch * 2, 3, 2, 1)\n        self.enc3 = layers.Encoder(ch * 2, ch * 4, 3, 2, 1)\n        self.enc4 = layers.Encoder(ch * 4, ch * 8, 3, 2, 1)\n        self.aspp = layers.ASPPModule(ch * 8, ch * 16, dilations)\n        self.dec4 = layers.Decoder(ch * (8 + 16), ch * 8, 3, 1, 1)\n        self.dec3 = layers.Decoder(ch * (4 + 8), ch * 4, 3, 1, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123812KB",
        "documentation": {}
    },
    {
        "label": "CascadedASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123812KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123812KB",
        "peekOfCode": "class CascadedASPPNet(nn.Module):\n    def __init__(self, n_fft):\n        super(CascadedASPPNet, self).__init__()\n        self.stg1_low_band_net = BaseASPPNet(2, 32)\n        self.stg1_high_band_net = BaseASPPNet(2, 32)\n        self.stg2_bridge = layers.Conv2DBNActiv(34, 16, 1, 1, 0)\n        self.stg2_full_band_net = BaseASPPNet(16, 32)\n        self.stg3_bridge = layers.Conv2DBNActiv(66, 32, 1, 1, 0)\n        self.stg3_full_band_net = BaseASPPNet(32, 64)\n        self.out = nn.Conv2d(64, 2, 1, bias=False)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123812KB",
        "documentation": {}
    },
    {
        "label": "BaseASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123821KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123821KB",
        "peekOfCode": "class BaseASPPNet(nn.Module):\n    def __init__(self, nin, ch, dilations=(4, 8, 16)):\n        super(BaseASPPNet, self).__init__()\n        self.enc1 = layers.Encoder(nin, ch, 3, 2, 1)\n        self.enc2 = layers.Encoder(ch, ch * 2, 3, 2, 1)\n        self.enc3 = layers.Encoder(ch * 2, ch * 4, 3, 2, 1)\n        self.enc4 = layers.Encoder(ch * 4, ch * 8, 3, 2, 1)\n        self.aspp = layers.ASPPModule(ch * 8, ch * 16, dilations)\n        self.dec4 = layers.Decoder(ch * (8 + 16), ch * 8, 3, 1, 1)\n        self.dec3 = layers.Decoder(ch * (4 + 8), ch * 4, 3, 1, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123821KB",
        "documentation": {}
    },
    {
        "label": "CascadedASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123821KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123821KB",
        "peekOfCode": "class CascadedASPPNet(nn.Module):\n    def __init__(self, n_fft):\n        super(CascadedASPPNet, self).__init__()\n        self.stg1_low_band_net = BaseASPPNet(2, 32)\n        self.stg1_high_band_net = BaseASPPNet(2, 32)\n        self.stg2_bridge = layers.Conv2DBNActiv(34, 16, 1, 1, 0)\n        self.stg2_full_band_net = BaseASPPNet(16, 32)\n        self.stg3_bridge = layers.Conv2DBNActiv(66, 32, 1, 1, 0)\n        self.stg3_full_band_net = BaseASPPNet(32, 64)\n        self.out = nn.Conv2d(64, 2, 1, bias=False)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_123821KB",
        "documentation": {}
    },
    {
        "label": "BaseASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_33966KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_33966KB",
        "peekOfCode": "class BaseASPPNet(nn.Module):\n    def __init__(self, nin, ch, dilations=(4, 8, 16, 32)):\n        super(BaseASPPNet, self).__init__()\n        self.enc1 = layers.Encoder(nin, ch, 3, 2, 1)\n        self.enc2 = layers.Encoder(ch, ch * 2, 3, 2, 1)\n        self.enc3 = layers.Encoder(ch * 2, ch * 4, 3, 2, 1)\n        self.enc4 = layers.Encoder(ch * 4, ch * 8, 3, 2, 1)\n        self.aspp = layers.ASPPModule(ch * 8, ch * 16, dilations)\n        self.dec4 = layers.Decoder(ch * (8 + 16), ch * 8, 3, 1, 1)\n        self.dec3 = layers.Decoder(ch * (4 + 8), ch * 4, 3, 1, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_33966KB",
        "documentation": {}
    },
    {
        "label": "CascadedASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_33966KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_33966KB",
        "peekOfCode": "class CascadedASPPNet(nn.Module):\n    def __init__(self, n_fft):\n        super(CascadedASPPNet, self).__init__()\n        self.stg1_low_band_net = BaseASPPNet(2, 16)\n        self.stg1_high_band_net = BaseASPPNet(2, 16)\n        self.stg2_bridge = layers.Conv2DBNActiv(18, 8, 1, 1, 0)\n        self.stg2_full_band_net = BaseASPPNet(8, 16)\n        self.stg3_bridge = layers.Conv2DBNActiv(34, 16, 1, 1, 0)\n        self.stg3_full_band_net = BaseASPPNet(16, 32)\n        self.out = nn.Conv2d(32, 2, 1, bias=False)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_33966KB",
        "documentation": {}
    },
    {
        "label": "BaseASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537227KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537227KB",
        "peekOfCode": "class BaseASPPNet(nn.Module):\n    def __init__(self, nin, ch, dilations=(4, 8, 16)):\n        super(BaseASPPNet, self).__init__()\n        self.enc1 = layers.Encoder(nin, ch, 3, 2, 1)\n        self.enc2 = layers.Encoder(ch, ch * 2, 3, 2, 1)\n        self.enc3 = layers.Encoder(ch * 2, ch * 4, 3, 2, 1)\n        self.enc4 = layers.Encoder(ch * 4, ch * 8, 3, 2, 1)\n        self.aspp = layers.ASPPModule(ch * 8, ch * 16, dilations)\n        self.dec4 = layers.Decoder(ch * (8 + 16), ch * 8, 3, 1, 1)\n        self.dec3 = layers.Decoder(ch * (4 + 8), ch * 4, 3, 1, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537227KB",
        "documentation": {}
    },
    {
        "label": "CascadedASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537227KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537227KB",
        "peekOfCode": "class CascadedASPPNet(nn.Module):\n    def __init__(self, n_fft):\n        super(CascadedASPPNet, self).__init__()\n        self.stg1_low_band_net = BaseASPPNet(2, 64)\n        self.stg1_high_band_net = BaseASPPNet(2, 64)\n        self.stg2_bridge = layers.Conv2DBNActiv(66, 32, 1, 1, 0)\n        self.stg2_full_band_net = BaseASPPNet(32, 64)\n        self.stg3_bridge = layers.Conv2DBNActiv(130, 64, 1, 1, 0)\n        self.stg3_full_band_net = BaseASPPNet(64, 128)\n        self.out = nn.Conv2d(128, 2, 1, bias=False)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537227KB",
        "documentation": {}
    },
    {
        "label": "BaseASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537238KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537238KB",
        "peekOfCode": "class BaseASPPNet(nn.Module):\n    def __init__(self, nin, ch, dilations=(4, 8, 16)):\n        super(BaseASPPNet, self).__init__()\n        self.enc1 = layers.Encoder(nin, ch, 3, 2, 1)\n        self.enc2 = layers.Encoder(ch, ch * 2, 3, 2, 1)\n        self.enc3 = layers.Encoder(ch * 2, ch * 4, 3, 2, 1)\n        self.enc4 = layers.Encoder(ch * 4, ch * 8, 3, 2, 1)\n        self.aspp = layers.ASPPModule(ch * 8, ch * 16, dilations)\n        self.dec4 = layers.Decoder(ch * (8 + 16), ch * 8, 3, 1, 1)\n        self.dec3 = layers.Decoder(ch * (4 + 8), ch * 4, 3, 1, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537238KB",
        "documentation": {}
    },
    {
        "label": "CascadedASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537238KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537238KB",
        "peekOfCode": "class CascadedASPPNet(nn.Module):\n    def __init__(self, n_fft):\n        super(CascadedASPPNet, self).__init__()\n        self.stg1_low_band_net = BaseASPPNet(2, 64)\n        self.stg1_high_band_net = BaseASPPNet(2, 64)\n        self.stg2_bridge = layers.Conv2DBNActiv(66, 32, 1, 1, 0)\n        self.stg2_full_band_net = BaseASPPNet(32, 64)\n        self.stg3_bridge = layers.Conv2DBNActiv(130, 64, 1, 1, 0)\n        self.stg3_full_band_net = BaseASPPNet(64, 128)\n        self.out = nn.Conv2d(128, 2, 1, bias=False)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_537238KB",
        "documentation": {}
    },
    {
        "label": "BaseASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_61968KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_61968KB",
        "peekOfCode": "class BaseASPPNet(nn.Module):\n    def __init__(self, nin, ch, dilations=(4, 8, 16)):\n        super(BaseASPPNet, self).__init__()\n        self.enc1 = layers.Encoder(nin, ch, 3, 2, 1)\n        self.enc2 = layers.Encoder(ch, ch * 2, 3, 2, 1)\n        self.enc3 = layers.Encoder(ch * 2, ch * 4, 3, 2, 1)\n        self.enc4 = layers.Encoder(ch * 4, ch * 8, 3, 2, 1)\n        self.aspp = layers.ASPPModule(ch * 8, ch * 16, dilations)\n        self.dec4 = layers.Decoder(ch * (8 + 16), ch * 8, 3, 1, 1)\n        self.dec3 = layers.Decoder(ch * (4 + 8), ch * 4, 3, 1, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_61968KB",
        "documentation": {}
    },
    {
        "label": "CascadedASPPNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_61968KB",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_61968KB",
        "peekOfCode": "class CascadedASPPNet(nn.Module):\n    def __init__(self, n_fft):\n        super(CascadedASPPNet, self).__init__()\n        self.stg1_low_band_net = BaseASPPNet(2, 32)\n        self.stg1_high_band_net = BaseASPPNet(2, 32)\n        self.stg2_bridge = layers.Conv2DBNActiv(34, 16, 1, 1, 0)\n        self.stg2_full_band_net = BaseASPPNet(16, 32)\n        self.stg3_bridge = layers.Conv2DBNActiv(66, 32, 1, 1, 0)\n        self.stg3_full_band_net = BaseASPPNet(32, 64)\n        self.out = nn.Conv2d(64, 2, 1, bias=False)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_61968KB",
        "documentation": {}
    },
    {
        "label": "BaseNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_new",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_new",
        "peekOfCode": "class BaseNet(nn.Module):\n    def __init__(\n        self, nin, nout, nin_lstm, nout_lstm, dilations=((4, 2), (8, 4), (12, 6))\n    ):\n        super(BaseNet, self).__init__()\n        self.enc1 = layers_new.Conv2DBNActiv(nin, nout, 3, 1, 1)\n        self.enc2 = layers_new.Encoder(nout, nout * 2, 3, 2, 1)\n        self.enc3 = layers_new.Encoder(nout * 2, nout * 4, 3, 2, 1)\n        self.enc4 = layers_new.Encoder(nout * 4, nout * 6, 3, 2, 1)\n        self.enc5 = layers_new.Encoder(nout * 6, nout * 8, 3, 2, 1)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_new",
        "documentation": {}
    },
    {
        "label": "CascadedNet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_new",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_new",
        "peekOfCode": "class CascadedNet(nn.Module):\n    def __init__(self, n_fft, nout=32, nout_lstm=128):\n        super(CascadedNet, self).__init__()\n        self.max_bin = n_fft // 2\n        self.output_bin = n_fft // 2 + 1\n        self.nin_lstm = self.max_bin // 2\n        self.offset = 64\n        self.stg1_low_band_net = nn.Sequential(\n            BaseNet(2, nout // 2, self.nin_lstm // 2, nout_lstm),\n            layers_new.Conv2DBNActiv(nout // 2, nout // 4, 1, 1, 0),",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.nets_new",
        "documentation": {}
    },
    {
        "label": "crop_center",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def crop_center(h1, h2):\n    h1_shape = h1.size()\n    h2_shape = h2.size()\n    if h1_shape[3] == h2_shape[3]:\n        return h1\n    elif h1_shape[3] < h2_shape[3]:\n        raise ValueError(\"h1_shape[3] must be greater than h2_shape[3]\")\n    # s_freq = (h2_shape[2] - h1_shape[2]) // 2\n    # e_freq = s_freq + h1_shape[2]\n    s_time = (h1_shape[3] - h2_shape[3]) // 2",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "wave_to_spectrogram",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def wave_to_spectrogram(\n    wave, hop_length, n_fft, mid_side=False, mid_side_b2=False, reverse=False\n):\n    if reverse:\n        wave_left = np.flip(np.asfortranarray(wave[0]))\n        wave_right = np.flip(np.asfortranarray(wave[1]))\n    elif mid_side:\n        wave_left = np.asfortranarray(np.add(wave[0], wave[1]) / 2)\n        wave_right = np.asfortranarray(np.subtract(wave[0], wave[1]))\n    elif mid_side_b2:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "wave_to_spectrogram_mt",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def wave_to_spectrogram_mt(\n    wave, hop_length, n_fft, mid_side=False, mid_side_b2=False, reverse=False\n):\n    import threading\n    if reverse:\n        wave_left = np.flip(np.asfortranarray(wave[0]))\n        wave_right = np.flip(np.asfortranarray(wave[1]))\n    elif mid_side:\n        wave_left = np.asfortranarray(np.add(wave[0], wave[1]) / 2)\n        wave_right = np.asfortranarray(np.subtract(wave[0], wave[1]))",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "combine_spectrograms",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def combine_spectrograms(specs, mp):\n    l = min([specs[i].shape[2] for i in specs])\n    spec_c = np.zeros(shape=(2, mp.param[\"bins\"] + 1, l), dtype=np.complex64)\n    offset = 0\n    bands_n = len(mp.param[\"band\"])\n    for d in range(1, bands_n + 1):\n        h = mp.param[\"band\"][d][\"crop_stop\"] - mp.param[\"band\"][d][\"crop_start\"]\n        spec_c[:, offset : offset + h, :l] = specs[d][\n            :, mp.param[\"band\"][d][\"crop_start\"] : mp.param[\"band\"][d][\"crop_stop\"], :l\n        ]",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "spectrogram_to_image",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def spectrogram_to_image(spec, mode=\"magnitude\"):\n    if mode == \"magnitude\":\n        if np.iscomplexobj(spec):\n            y = np.abs(spec)\n        else:\n            y = spec\n        y = np.log10(y**2 + 1e-8)\n    elif mode == \"phase\":\n        if np.iscomplexobj(spec):\n            y = np.angle(spec)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "reduce_vocal_aggressively",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def reduce_vocal_aggressively(X, y, softmask):\n    v = X - y\n    y_mag_tmp = np.abs(y)\n    v_mag_tmp = np.abs(v)\n    v_mask = v_mag_tmp > y_mag_tmp\n    y_mag = np.clip(y_mag_tmp - v_mag_tmp * v_mask * softmask, 0, np.inf)\n    return y_mag * np.exp(1.0j * np.angle(y))\ndef mask_silence(mag, ref, thres=0.2, min_range=64, fade_size=32):\n    if min_range < fade_size * 2:\n        raise ValueError(\"min_range must be >= fade_area * 2\")",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "mask_silence",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def mask_silence(mag, ref, thres=0.2, min_range=64, fade_size=32):\n    if min_range < fade_size * 2:\n        raise ValueError(\"min_range must be >= fade_area * 2\")\n    mag = mag.copy()\n    idx = np.where(ref.mean(axis=(0, 1)) < thres)[0]\n    starts = np.insert(idx[np.where(np.diff(idx) != 1)[0] + 1], 0, idx[0])\n    ends = np.append(idx[np.where(np.diff(idx) != 1)[0]], idx[-1])\n    uninformative = np.where(ends - starts > min_range)[0]\n    if len(uninformative) > 0:\n        starts = starts[uninformative]",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "align_wave_head_and_tail",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def align_wave_head_and_tail(a, b):\n    l = min([a[0].size, b[0].size])\n    return a[:l, :l], b[:l, :l]\ndef cache_or_load(mix_path, inst_path, mp):\n    mix_basename = os.path.splitext(os.path.basename(mix_path))[0]\n    inst_basename = os.path.splitext(os.path.basename(inst_path))[0]\n    cache_dir = \"mph{}\".format(\n        hashlib.sha1(json.dumps(mp.param, sort_keys=True).encode(\"utf-8\")).hexdigest()\n    )\n    mix_cache_dir = os.path.join(\"cache\", cache_dir)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "cache_or_load",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def cache_or_load(mix_path, inst_path, mp):\n    mix_basename = os.path.splitext(os.path.basename(mix_path))[0]\n    inst_basename = os.path.splitext(os.path.basename(inst_path))[0]\n    cache_dir = \"mph{}\".format(\n        hashlib.sha1(json.dumps(mp.param, sort_keys=True).encode(\"utf-8\")).hexdigest()\n    )\n    mix_cache_dir = os.path.join(\"cache\", cache_dir)\n    inst_cache_dir = os.path.join(\"cache\", cache_dir)\n    os.makedirs(mix_cache_dir, exist_ok=True)\n    os.makedirs(inst_cache_dir, exist_ok=True)",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "spectrogram_to_wave",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def spectrogram_to_wave(spec, hop_length, mid_side, mid_side_b2, reverse):\n    spec_left = np.asfortranarray(spec[0])\n    spec_right = np.asfortranarray(spec[1])\n    wave_left = librosa.istft(spec_left, hop_length=hop_length)\n    wave_right = librosa.istft(spec_right, hop_length=hop_length)\n    if reverse:\n        return np.asfortranarray([np.flip(wave_left), np.flip(wave_right)])\n    elif mid_side:\n        return np.asfortranarray(\n            [np.add(wave_left, wave_right / 2), np.subtract(wave_left, wave_right / 2)]",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "spectrogram_to_wave_mt",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def spectrogram_to_wave_mt(spec, hop_length, mid_side, reverse, mid_side_b2):\n    import threading\n    spec_left = np.asfortranarray(spec[0])\n    spec_right = np.asfortranarray(spec[1])\n    def run_thread(**kwargs):\n        global wave_left\n        wave_left = librosa.istft(**kwargs)\n    thread = threading.Thread(\n        target=run_thread, kwargs={\"stft_matrix\": spec_left, \"hop_length\": hop_length}\n    )",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "cmb_spectrogram_to_wave",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def cmb_spectrogram_to_wave(spec_m, mp, extra_bins_h=None, extra_bins=None):\n    wave_band = {}\n    bands_n = len(mp.param[\"band\"])\n    offset = 0\n    for d in range(1, bands_n + 1):\n        bp = mp.param[\"band\"][d]\n        spec_s = np.ndarray(\n            shape=(2, bp[\"n_fft\"] // 2 + 1, spec_m.shape[2]), dtype=complex\n        )\n        h = bp[\"crop_stop\"] - bp[\"crop_start\"]",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "fft_lp_filter",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def fft_lp_filter(spec, bin_start, bin_stop):\n    g = 1.0\n    for b in range(bin_start, bin_stop):\n        g -= 1 / (bin_stop - bin_start)\n        spec[:, b, :] = g * spec[:, b, :]\n    spec[:, bin_stop:, :] *= 0\n    return spec\ndef fft_hp_filter(spec, bin_start, bin_stop):\n    g = 1.0\n    for b in range(bin_start, bin_stop, -1):",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "fft_hp_filter",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def fft_hp_filter(spec, bin_start, bin_stop):\n    g = 1.0\n    for b in range(bin_start, bin_stop, -1):\n        g -= 1 / (bin_start - bin_stop)\n        spec[:, b, :] = g * spec[:, b, :]\n    spec[:, 0 : bin_stop + 1, :] *= 0\n    return spec\ndef mirroring(a, spec_m, input_high_end, mp):\n    if \"mirroring\" == a:\n        mirror = np.flip(",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "mirroring",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def mirroring(a, spec_m, input_high_end, mp):\n    if \"mirroring\" == a:\n        mirror = np.flip(\n            np.abs(\n                spec_m[\n                    :,\n                    mp.param[\"pre_filter_start\"]\n                    - 10\n                    - input_high_end.shape[1] : mp.param[\"pre_filter_start\"]\n                    - 10,",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "ensembling",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def ensembling(a, specs):\n    for i in range(1, len(specs)):\n        if i == 1:\n            spec = specs[0]\n        ln = min([spec.shape[2], specs[i].shape[2]])\n        spec = spec[:, :, :ln]\n        specs[i] = specs[i][:, :, :ln]\n        if \"min_mag\" == a:\n            spec = np.where(np.abs(specs[i]) <= np.abs(spec), specs[i], spec)\n        if \"max_mag\" == a:",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "stft",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def stft(wave, nfft, hl):\n    wave_left = np.asfortranarray(wave[0])\n    wave_right = np.asfortranarray(wave[1])\n    spec_left = librosa.stft(wave_left, n_fft=nfft, hop_length=hl)\n    spec_right = librosa.stft(wave_right, n_fft=nfft, hop_length=hl)\n    spec = np.asfortranarray([spec_left, spec_right])\n    return spec\ndef istft(spec, hl):\n    spec_left = np.asfortranarray(spec[0])\n    spec_right = np.asfortranarray(spec[1])",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "istft",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "peekOfCode": "def istft(spec, hl):\n    spec_left = np.asfortranarray(spec[0])\n    spec_right = np.asfortranarray(spec[1])\n    wave_left = librosa.istft(spec_left, hop_length=hl)\n    wave_right = librosa.istft(spec_right, hop_length=hl)\n    wave = np.asfortranarray([wave_left, wave_right])\nif __name__ == \"__main__\":\n    import argparse\n    import sys\n    import time",
        "detail": "Models.Sound.infer.lib.uvr5_pack.lib_v5.spec_utils",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.utils",
        "peekOfCode": "def load_data(file_name: str = \"./infer/lib/uvr5_pack/name_params.json\") -> dict:\n    with open(file_name, \"r\") as f:\n        data = json.load(f)\n    return data\ndef make_padding(width, cropsize, offset):\n    left = offset\n    roi_size = cropsize - left * 2\n    if roi_size == 0:\n        roi_size = cropsize\n    right = roi_size - (width % roi_size) + left",
        "detail": "Models.Sound.infer.lib.uvr5_pack.utils",
        "documentation": {}
    },
    {
        "label": "make_padding",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.utils",
        "peekOfCode": "def make_padding(width, cropsize, offset):\n    left = offset\n    roi_size = cropsize - left * 2\n    if roi_size == 0:\n        roi_size = cropsize\n    right = roi_size - (width % roi_size) + left\n    return left, right, roi_size\ndef inference(X_spec, device, model, aggressiveness, data):\n    \"\"\"\n    data ： dic configs",
        "detail": "Models.Sound.infer.lib.uvr5_pack.utils",
        "documentation": {}
    },
    {
        "label": "inference",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.uvr5_pack.utils",
        "description": "Models.Sound.infer.lib.uvr5_pack.utils",
        "peekOfCode": "def inference(X_spec, device, model, aggressiveness, data):\n    \"\"\"\n    data ： dic configs\n    \"\"\"\n    def _execute(\n        X_mag_pad, roi_size, n_window, device, model, aggressiveness, is_half=True\n    ):\n        model.eval()\n        with torch.no_grad():\n            preds = []",
        "detail": "Models.Sound.infer.lib.uvr5_pack.utils",
        "documentation": {}
    },
    {
        "label": "wav2",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.audio",
        "description": "Models.Sound.infer.lib.audio",
        "peekOfCode": "def wav2(i, o, format):\n    inp = av.open(i, \"rb\")\n    if format == \"m4a\":\n        format = \"mp4\"\n    out = av.open(o, \"wb\", format=format)\n    if format == \"ogg\":\n        format = \"libvorbis\"\n    if format == \"mp4\":\n        format = \"aac\"\n    ostream = out.add_stream(format)",
        "detail": "Models.Sound.infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.audio",
        "description": "Models.Sound.infer.lib.audio",
        "peekOfCode": "def load_audio(file, sr):\n    try:\n        # https://github.com/openai/whisper/blob/main/whisper/audio.py#L26\n        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n        file = clean_path(file)  # 防止小白拷路径头尾带了空格和\"和回车\n        if os.path.exists(file) == False:\n            raise RuntimeError(\n                \"You input a wrong audio path that does not exists, please fix it!\"\n            )",
        "detail": "Models.Sound.infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "clean_path",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.audio",
        "description": "Models.Sound.infer.lib.audio",
        "peekOfCode": "def clean_path(path_str):\n    if platform.system() == \"Windows\":\n        path_str = path_str.replace(\"/\", \"\\\\\")\n    path_str = re.sub(r'[\\u202a\\u202b\\u202c\\u202d\\u202e]', '', path_str)  # 移除 Unicode 控制字符\n    return path_str.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")",
        "detail": "Models.Sound.infer.lib.audio",
        "documentation": {}
    },
    {
        "label": "STFT",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class STFT(torch.nn.Module):\n    def __init__(\n        self, filter_length=1024, hop_length=512, win_length=None, window=\"hann\"\n    ):\n        \"\"\"\n        This module implements an STFT using 1D convolution and 1D transpose convolutions.\n        This is a bit tricky so there are some cases that probably won't work as working\n        out the same sizes before and after in all overlap add setups is tough. Right now,\n        this code should work with hop lengths that are half the filter length (50% overlap\n        between frames).",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "BiGRU",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class BiGRU(nn.Module):\n    def __init__(self, input_features, hidden_features, num_layers):\n        super(BiGRU, self).__init__()\n        self.gru = nn.GRU(\n            input_features,\n            hidden_features,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n        )",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "ConvBlockRes",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class ConvBlockRes(nn.Module):\n    def __init__(self, in_channels, out_channels, momentum=0.01):\n        super(ConvBlockRes, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=(3, 3),\n                stride=(1, 1),\n                padding=(1, 1),",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        in_size,\n        n_encoders,\n        kernel_size,\n        n_blocks,\n        out_channels=16,\n        momentum=0.01,",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "ResEncoderBlock",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class ResEncoderBlock(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, n_blocks=1, momentum=0.01\n    ):\n        super(ResEncoderBlock, self).__init__()\n        self.n_blocks = n_blocks\n        self.conv = nn.ModuleList()\n        self.conv.append(ConvBlockRes(in_channels, out_channels, momentum))\n        for i in range(n_blocks - 1):\n            self.conv.append(ConvBlockRes(out_channels, out_channels, momentum))",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "Intermediate",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class Intermediate(nn.Module):  #\n    def __init__(self, in_channels, out_channels, n_inters, n_blocks, momentum=0.01):\n        super(Intermediate, self).__init__()\n        self.n_inters = n_inters\n        self.layers = nn.ModuleList()\n        self.layers.append(\n            ResEncoderBlock(in_channels, out_channels, None, n_blocks, momentum)\n        )\n        for i in range(self.n_inters - 1):\n            self.layers.append(",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "ResDecoderBlock",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class ResDecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, n_blocks=1, momentum=0.01):\n        super(ResDecoderBlock, self).__init__()\n        out_padding = (0, 1) if stride == (1, 2) else (1, 1)\n        self.n_blocks = n_blocks\n        self.conv1 = nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=(3, 3),",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, in_channels, n_decoders, stride, n_blocks, momentum=0.01):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList()\n        self.n_decoders = n_decoders\n        for i in range(self.n_decoders):\n            out_channels = in_channels // 2\n            self.layers.append(\n                ResDecoderBlock(in_channels, out_channels, stride, n_blocks, momentum)\n            )",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "DeepUnet",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class DeepUnet(nn.Module):\n    def __init__(\n        self,\n        kernel_size,\n        n_blocks,\n        en_de_layers=5,\n        inter_layers=4,\n        in_channels=1,\n        en_out_channels=16,\n    ):",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "E2E",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class E2E(nn.Module):\n    def __init__(\n        self,\n        n_blocks,\n        n_gru,\n        kernel_size,\n        en_de_layers=5,\n        inter_layers=4,\n        in_channels=1,\n        en_out_channels=16,",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "MelSpectrogram",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class MelSpectrogram(torch.nn.Module):\n    def __init__(\n        self,\n        is_half,\n        n_mel_channels,\n        sampling_rate,\n        win_length,\n        hop_length,\n        n_fft=None,\n        mel_fmin=0,",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "RMVPE",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "class RMVPE:\n    def __init__(self, model_path: str, is_half, device=None, use_jit=False):\n        self.resample_kernel = {}\n        self.resample_kernel = {}\n        self.is_half = is_half\n        if device is None:\n            device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n        self.device = device\n        self.mel_extractor = MelSpectrogram(\n            is_half, 128, 16000, 1024, 160, None, 30, 8000",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.rmvpe",
        "description": "Models.Sound.infer.lib.rmvpe",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass STFT(torch.nn.Module):\n    def __init__(\n        self, filter_length=1024, hop_length=512, win_length=None, window=\"hann\"\n    ):\n        \"\"\"\n        This module implements an STFT using 1D convolution and 1D transpose convolutions.\n        This is a bit tricky so there are some cases that probably won't work as working\n        out the same sizes before and after in all overlap add setups is tough. Right now,\n        this code should work with hop lengths that are half the filter length (50% overlap",
        "detail": "Models.Sound.infer.lib.rmvpe",
        "documentation": {}
    },
    {
        "label": "RVC",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.rtrvc",
        "description": "Models.Sound.infer.lib.rtrvc",
        "peekOfCode": "class RVC:\n    def __init__(\n        self,\n        key,\n        formant,\n        pth_path,\n        index_path,\n        index_rate,\n        n_cpu,\n        inp_q,",
        "detail": "Models.Sound.infer.lib.rtrvc",
        "documentation": {}
    },
    {
        "label": "printt",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.rtrvc",
        "description": "Models.Sound.infer.lib.rtrvc",
        "peekOfCode": "def printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:\n        print(strr % args)\n# config.device=torch.device(\"cpu\")########强制cpu测试\n# config.is_half=False########强制cpu测试\nclass RVC:\n    def __init__(\n        self,",
        "detail": "Models.Sound.infer.lib.rtrvc",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.rtrvc",
        "description": "Models.Sound.infer.lib.rtrvc",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nfrom multiprocessing import Manager as M\nfrom configs.config import Config\n# config = Config()\nmm = M()\ndef printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:",
        "detail": "Models.Sound.infer.lib.rtrvc",
        "documentation": {}
    },
    {
        "label": "mm",
        "kind": 5,
        "importPath": "Models.Sound.infer.lib.rtrvc",
        "description": "Models.Sound.infer.lib.rtrvc",
        "peekOfCode": "mm = M()\ndef printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:\n        print(strr % args)\n# config.device=torch.device(\"cpu\")########强制cpu测试\n# config.is_half=False########强制cpu测试\nclass RVC:\n    def __init__(",
        "detail": "Models.Sound.infer.lib.rtrvc",
        "documentation": {}
    },
    {
        "label": "Slicer",
        "kind": 6,
        "importPath": "Models.Sound.infer.lib.slicer2",
        "description": "Models.Sound.infer.lib.slicer2",
        "peekOfCode": "class Slicer:\n    def __init__(\n        self,\n        sr: int,\n        threshold: float = -40.0,\n        min_length: int = 5000,\n        min_interval: int = 300,\n        hop_size: int = 20,\n        max_sil_kept: int = 5000,\n    ):",
        "detail": "Models.Sound.infer.lib.slicer2",
        "documentation": {}
    },
    {
        "label": "get_rms",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.slicer2",
        "description": "Models.Sound.infer.lib.slicer2",
        "peekOfCode": "def get_rms(\n    y,\n    frame_length=2048,\n    hop_length=512,\n    pad_mode=\"constant\",\n):\n    padding = (int(frame_length // 2), int(frame_length // 2))\n    y = np.pad(y, padding, mode=pad_mode)\n    axis = -1\n    # put our new within-frame axis at the end for now",
        "detail": "Models.Sound.infer.lib.slicer2",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Models.Sound.infer.lib.slicer2",
        "description": "Models.Sound.infer.lib.slicer2",
        "peekOfCode": "def main():\n    import os.path\n    from argparse import ArgumentParser\n    import librosa\n    import soundfile\n    parser = ArgumentParser()\n    parser.add_argument(\"audio\", type=str, help=\"The audio to be sliced\")\n    parser.add_argument(\n        \"--out\", type=str, help=\"Output directory of the sliced audio clips\"\n    )",
        "detail": "Models.Sound.infer.lib.slicer2",
        "documentation": {}
    },
    {
        "label": "torch_bmm",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.attention",
        "description": "Models.Sound.infer.modules.ipex.attention",
        "peekOfCode": "def torch_bmm(input, mat2, *, out=None):\n    if input.dtype != mat2.dtype:\n        mat2 = mat2.to(input.dtype)\n    # ARC GPUs can't allocate more than 4GB to a single block, Slice it:\n    batch_size_attention, input_tokens, mat2_shape = (\n        input.shape[0],\n        input.shape[1],\n        mat2.shape[2],\n    )\n    block_multiply = input.element_size()",
        "detail": "Models.Sound.infer.modules.ipex.attention",
        "documentation": {}
    },
    {
        "label": "scaled_dot_product_attention",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.attention",
        "description": "Models.Sound.infer.modules.ipex.attention",
        "peekOfCode": "def scaled_dot_product_attention(\n    query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False\n):\n    # ARC GPUs can't allocate more than 4GB to a single block, Slice it:\n    if len(query.shape) == 3:\n        batch_size_attention, query_tokens, shape_four = query.shape\n        shape_one = 1\n        no_shape_one = True\n    else:\n        shape_one, batch_size_attention, query_tokens, shape_four = query.shape",
        "detail": "Models.Sound.infer.modules.ipex.attention",
        "documentation": {}
    },
    {
        "label": "attention_init",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.attention",
        "description": "Models.Sound.infer.modules.ipex.attention",
        "peekOfCode": "def attention_init():\n    # ARC GPUs can't allocate more than 4GB to a single block:\n    torch.bmm = torch_bmm\n    torch.nn.functional.scaled_dot_product_attention = scaled_dot_product_attention",
        "detail": "Models.Sound.infer.modules.ipex.attention",
        "documentation": {}
    },
    {
        "label": "original_torch_bmm",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.attention",
        "description": "Models.Sound.infer.modules.ipex.attention",
        "peekOfCode": "original_torch_bmm = torch.bmm\ndef torch_bmm(input, mat2, *, out=None):\n    if input.dtype != mat2.dtype:\n        mat2 = mat2.to(input.dtype)\n    # ARC GPUs can't allocate more than 4GB to a single block, Slice it:\n    batch_size_attention, input_tokens, mat2_shape = (\n        input.shape[0],\n        input.shape[1],\n        mat2.shape[2],\n    )",
        "detail": "Models.Sound.infer.modules.ipex.attention",
        "documentation": {}
    },
    {
        "label": "original_scaled_dot_product_attention",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.attention",
        "description": "Models.Sound.infer.modules.ipex.attention",
        "peekOfCode": "original_scaled_dot_product_attention = torch.nn.functional.scaled_dot_product_attention\ndef scaled_dot_product_attention(\n    query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False\n):\n    # ARC GPUs can't allocate more than 4GB to a single block, Slice it:\n    if len(query.shape) == 3:\n        batch_size_attention, query_tokens, shape_four = query.shape\n        shape_one = 1\n        no_shape_one = True\n    else:",
        "detail": "Models.Sound.infer.modules.ipex.attention",
        "documentation": {}
    },
    {
        "label": "unscale_",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.gradscaler",
        "description": "Models.Sound.infer.modules.ipex.gradscaler",
        "peekOfCode": "def unscale_(self, optimizer):\n    \"\"\"\n    Divides (\"unscales\") the optimizer's gradient tensors by the scale factor.\n    :meth:`unscale_` is optional, serving cases where you need to\n    :ref:`modify or inspect gradients<working-with-unscaled-gradients>`\n    between the backward pass(es) and :meth:`step`.\n    If :meth:`unscale_` is not called explicitly,  gradients will be unscaled  automatically during :meth:`step`.\n    Simple example, using :meth:`unscale_` to enable clipping of unscaled gradients::\n        ...\n        scaler.scale(loss).backward()",
        "detail": "Models.Sound.infer.modules.ipex.gradscaler",
        "documentation": {}
    },
    {
        "label": "update",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.gradscaler",
        "description": "Models.Sound.infer.modules.ipex.gradscaler",
        "peekOfCode": "def update(self, new_scale=None):\n    \"\"\"\n    Updates the scale factor.\n    If any optimizer steps were skipped the scale is multiplied by ``backoff_factor``\n    to reduce it. If ``growth_interval`` unskipped iterations occurred consecutively,\n    the scale is multiplied by ``growth_factor`` to increase it.\n    Passing ``new_scale`` sets the new scale value manually. (``new_scale`` is not\n    used directly, it's used to fill GradScaler's internal scale tensor. So if\n    ``new_scale`` was a tensor, later in-place changes to that tensor will not further\n    affect the scale GradScaler uses internally.)",
        "detail": "Models.Sound.infer.modules.ipex.gradscaler",
        "documentation": {}
    },
    {
        "label": "gradscaler_init",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.gradscaler",
        "description": "Models.Sound.infer.modules.ipex.gradscaler",
        "peekOfCode": "def gradscaler_init():\n    torch.xpu.amp.GradScaler = ipex.cpu.autocast._grad_scaler.GradScaler\n    torch.xpu.amp.GradScaler._unscale_grads_ = _unscale_grads_\n    torch.xpu.amp.GradScaler.unscale_ = unscale_\n    torch.xpu.amp.GradScaler.update = update\n    return torch.xpu.amp.GradScaler",
        "detail": "Models.Sound.infer.modules.ipex.gradscaler",
        "documentation": {}
    },
    {
        "label": "OptState",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.gradscaler",
        "description": "Models.Sound.infer.modules.ipex.gradscaler",
        "peekOfCode": "OptState = ipex.cpu.autocast._grad_scaler.OptState\n_MultiDeviceReplicator = ipex.cpu.autocast._grad_scaler._MultiDeviceReplicator\n_refresh_per_optimizer_state = (\n    ipex.cpu.autocast._grad_scaler._refresh_per_optimizer_state\n)\ndef _unscale_grads_(\n    self, optimizer, inv_scale, found_inf, allow_fp16\n):  # pylint: disable=unused-argument\n    per_device_inv_scale = _MultiDeviceReplicator(inv_scale)\n    per_device_found_inf = _MultiDeviceReplicator(found_inf)",
        "detail": "Models.Sound.infer.modules.ipex.gradscaler",
        "documentation": {}
    },
    {
        "label": "_MultiDeviceReplicator",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.gradscaler",
        "description": "Models.Sound.infer.modules.ipex.gradscaler",
        "peekOfCode": "_MultiDeviceReplicator = ipex.cpu.autocast._grad_scaler._MultiDeviceReplicator\n_refresh_per_optimizer_state = (\n    ipex.cpu.autocast._grad_scaler._refresh_per_optimizer_state\n)\ndef _unscale_grads_(\n    self, optimizer, inv_scale, found_inf, allow_fp16\n):  # pylint: disable=unused-argument\n    per_device_inv_scale = _MultiDeviceReplicator(inv_scale)\n    per_device_found_inf = _MultiDeviceReplicator(found_inf)\n    # To set up _amp_foreach_non_finite_check_and_unscale_, split grads by device and dtype.",
        "detail": "Models.Sound.infer.modules.ipex.gradscaler",
        "documentation": {}
    },
    {
        "label": "_refresh_per_optimizer_state",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.gradscaler",
        "description": "Models.Sound.infer.modules.ipex.gradscaler",
        "peekOfCode": "_refresh_per_optimizer_state = (\n    ipex.cpu.autocast._grad_scaler._refresh_per_optimizer_state\n)\ndef _unscale_grads_(\n    self, optimizer, inv_scale, found_inf, allow_fp16\n):  # pylint: disable=unused-argument\n    per_device_inv_scale = _MultiDeviceReplicator(inv_scale)\n    per_device_found_inf = _MultiDeviceReplicator(found_inf)\n    # To set up _amp_foreach_non_finite_check_and_unscale_, split grads by device and dtype.\n    # There could be hundreds of grads, so we'd like to iterate through them just once.",
        "detail": "Models.Sound.infer.modules.ipex.gradscaler",
        "documentation": {}
    },
    {
        "label": "CondFunc",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "class CondFunc:  # pylint: disable=missing-class-docstring\n    def __new__(cls, orig_func, sub_func, cond_func):\n        self = super(CondFunc, cls).__new__(cls)\n        if isinstance(orig_func, str):\n            func_path = orig_func.split(\".\")\n            for i in range(len(func_path) - 1, -1, -1):\n                try:\n                    resolved_obj = importlib.import_module(\".\".join(func_path[:i]))\n                    break\n                except ImportError:",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "DummyDataParallel",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "class DummyDataParallel(\n    torch.nn.Module\n):  # pylint: disable=missing-class-docstring, unused-argument, too-few-public-methods\n    def __new__(\n        cls, module, device_ids=None, output_device=None, dim=0\n    ):  # pylint: disable=unused-argument\n        if isinstance(device_ids, list) and len(device_ids) > 1:\n            print(\"IPEX backend doesn't support DataParallel on multiple XPU devices\")\n        return module.to(\"xpu\")\ndef return_null_context(*args, **kwargs):  # pylint: disable=unused-argument",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "return_null_context",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def return_null_context(*args, **kwargs):  # pylint: disable=unused-argument\n    return contextlib.nullcontext()\ndef check_device(device):\n    return bool(\n        (isinstance(device, torch.device) and device.type == \"cuda\")\n        or (isinstance(device, str) and \"cuda\" in device)\n        or isinstance(device, int)\n    )\ndef return_xpu(device):\n    return (",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "check_device",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def check_device(device):\n    return bool(\n        (isinstance(device, torch.device) and device.type == \"cuda\")\n        or (isinstance(device, str) and \"cuda\" in device)\n        or isinstance(device, int)\n    )\ndef return_xpu(device):\n    return (\n        f\"xpu:{device[-1]}\"\n        if isinstance(device, str) and \":\" in device",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "return_xpu",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def return_xpu(device):\n    return (\n        f\"xpu:{device[-1]}\"\n        if isinstance(device, str) and \":\" in device\n        else (\n            f\"xpu:{device}\"\n            if isinstance(device, int)\n            else torch.device(\"xpu\") if isinstance(device, torch.device) else \"xpu\"\n        )\n    )",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "ipex_no_cuda",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def ipex_no_cuda(orig_func, *args, **kwargs):\n    torch.cuda.is_available = lambda: False\n    orig_func(*args, **kwargs)\n    torch.cuda.is_available = torch.xpu.is_available\noriginal_autocast = torch.autocast\ndef ipex_autocast(*args, **kwargs):\n    if len(args) > 0 and args[0] == \"cuda\":\n        return original_autocast(\"xpu\", *args[1:], **kwargs)\n    else:\n        return original_autocast(*args, **kwargs)",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "ipex_autocast",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def ipex_autocast(*args, **kwargs):\n    if len(args) > 0 and args[0] == \"cuda\":\n        return original_autocast(\"xpu\", *args[1:], **kwargs)\n    else:\n        return original_autocast(*args, **kwargs)\noriginal_torch_cat = torch.cat\ndef torch_cat(tensor, *args, **kwargs):\n    if len(tensor) == 3 and (\n        tensor[0].dtype != tensor[1].dtype or tensor[2].dtype != tensor[1].dtype\n    ):",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "torch_cat",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def torch_cat(tensor, *args, **kwargs):\n    if len(tensor) == 3 and (\n        tensor[0].dtype != tensor[1].dtype or tensor[2].dtype != tensor[1].dtype\n    ):\n        return original_torch_cat(\n            [tensor[0].to(tensor[1].dtype), tensor[1], tensor[2].to(tensor[1].dtype)],\n            *args,\n            **kwargs,\n        )\n    else:",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def interpolate(\n    tensor,\n    size=None,\n    scale_factor=None,\n    mode=\"nearest\",\n    align_corners=None,\n    recompute_scale_factor=None,\n    antialias=False,\n):  # pylint: disable=too-many-arguments\n    if antialias or align_corners is not None:",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "linalg_solve",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def linalg_solve(A, B, *args, **kwargs):  # pylint: disable=invalid-name\n    if A.device != torch.device(\"cpu\") or B.device != torch.device(\"cpu\"):\n        return_device = A.device\n        return original_linalg_solve(A.to(\"cpu\"), B.to(\"cpu\"), *args, **kwargs).to(\n            return_device\n        )\n    else:\n        return original_linalg_solve(A, B, *args, **kwargs)\ndef ipex_hijacks():\n    CondFunc(",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "ipex_hijacks",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "def ipex_hijacks():\n    CondFunc(\n        \"torch.Tensor.to\",\n        lambda orig_func, self, device=None, *args, **kwargs: orig_func(\n            self, return_xpu(device), *args, **kwargs\n        ),\n        lambda orig_func, self, device=None, *args, **kwargs: check_device(device),\n    )\n    CondFunc(\n        \"torch.Tensor.cuda\",",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "_utils",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "_utils = torch.utils.data._utils\ndef _shutdown_workers(self):\n    if (\n        torch.utils.data._utils is None\n        or torch.utils.data._utils.python_exit_status is True\n        or torch.utils.data._utils.python_exit_status is None\n    ):\n        return\n    if hasattr(self, \"_shutdown\") and not self._shutdown:\n        self._shutdown = True",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "original_autocast",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "original_autocast = torch.autocast\ndef ipex_autocast(*args, **kwargs):\n    if len(args) > 0 and args[0] == \"cuda\":\n        return original_autocast(\"xpu\", *args[1:], **kwargs)\n    else:\n        return original_autocast(*args, **kwargs)\noriginal_torch_cat = torch.cat\ndef torch_cat(tensor, *args, **kwargs):\n    if len(tensor) == 3 and (\n        tensor[0].dtype != tensor[1].dtype or tensor[2].dtype != tensor[1].dtype",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "original_torch_cat",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "original_torch_cat = torch.cat\ndef torch_cat(tensor, *args, **kwargs):\n    if len(tensor) == 3 and (\n        tensor[0].dtype != tensor[1].dtype or tensor[2].dtype != tensor[1].dtype\n    ):\n        return original_torch_cat(\n            [tensor[0].to(tensor[1].dtype), tensor[1], tensor[2].to(tensor[1].dtype)],\n            *args,\n            **kwargs,\n        )",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "original_interpolate",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "original_interpolate = torch.nn.functional.interpolate\ndef interpolate(\n    tensor,\n    size=None,\n    scale_factor=None,\n    mode=\"nearest\",\n    align_corners=None,\n    recompute_scale_factor=None,\n    antialias=False,\n):  # pylint: disable=too-many-arguments",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "original_linalg_solve",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.ipex.hijacks",
        "description": "Models.Sound.infer.modules.ipex.hijacks",
        "peekOfCode": "original_linalg_solve = torch.linalg.solve\ndef linalg_solve(A, B, *args, **kwargs):  # pylint: disable=invalid-name\n    if A.device != torch.device(\"cpu\") or B.device != torch.device(\"cpu\"):\n        return_device = A.device\n        return original_linalg_solve(A.to(\"cpu\"), B.to(\"cpu\"), *args, **kwargs).to(\n            return_device\n        )\n    else:\n        return original_linalg_solve(A, B, *args, **kwargs)\ndef ipex_hijacks():",
        "detail": "Models.Sound.infer.modules.ipex.hijacks",
        "documentation": {}
    },
    {
        "label": "export_onnx",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.onnx.export",
        "description": "Models.Sound.infer.modules.onnx.export",
        "peekOfCode": "def export_onnx(ModelPath, ExportedPath):\n    cpt = torch.load(ModelPath, map_location=\"cpu\")\n    cpt[\"config\"][-3] = cpt[\"weight\"][\"emb_g.weight\"].shape[0]\n    vec_channels = 256 if cpt.get(\"version\", \"v1\") == \"v1\" else 768\n    test_phone = torch.rand(1, 200, vec_channels)  # hidden unit\n    test_phone_lengths = torch.tensor([200]).long()  # hidden unit 长度（貌似没啥用）\n    test_pitch = torch.randint(size=(1, 200), low=5, high=255)  # 基频（单位赫兹）\n    test_pitchf = torch.rand(1, 200)  # nsf基频\n    test_ds = torch.LongTensor([0])  # 说话人ID\n    test_rnd = torch.rand(1, 192, 200)  # 噪声（加入随机因子）",
        "detail": "Models.Sound.infer.modules.onnx.export",
        "documentation": {}
    },
    {
        "label": "FeatureInput",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "peekOfCode": "class FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256\n        self.f0_max = 1100.0\n        self.f0_min = 50.0\n        self.f0_mel_min = 1127 * np.log(1 + self.f0_min / 700)\n        self.f0_mel_max = 1127 * np.log(1 + self.f0_max / 700)\n    def compute_f0(self, path, f0_method):",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "documentation": {}
    },
    {
        "label": "printt",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "peekOfCode": "def printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nn_p = int(sys.argv[2])\nf0method = sys.argv[3]\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nimport logging\nimport numpy as np\nimport pyworld\nfrom infer.lib.audio import load_audio\nlogging.getLogger(\"numba\").setLevel(logging.WARNING)\nfrom multiprocessing import Process\nexp_dir = sys.argv[1]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "documentation": {}
    },
    {
        "label": "exp_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "peekOfCode": "exp_dir = sys.argv[1]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nn_p = int(sys.argv[2])\nf0method = sys.argv[3]\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "documentation": {}
    },
    {
        "label": "f",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "peekOfCode": "f = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nn_p = int(sys.argv[2])\nf0method = sys.argv[3]\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "documentation": {}
    },
    {
        "label": "n_p",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "peekOfCode": "n_p = int(sys.argv[2])\nf0method = sys.argv[3]\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256\n        self.f0_max = 1100.0\n        self.f0_min = 50.0\n        self.f0_mel_min = 1127 * np.log(1 + self.f0_min / 700)",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "documentation": {}
    },
    {
        "label": "f0method",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "peekOfCode": "f0method = sys.argv[3]\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256\n        self.f0_max = 1100.0\n        self.f0_min = 50.0\n        self.f0_mel_min = 1127 * np.log(1 + self.f0_min / 700)\n        self.f0_mel_max = 1127 * np.log(1 + self.f0_max / 700)",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_print",
        "documentation": {}
    },
    {
        "label": "FeatureInput",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "class FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256\n        self.f0_max = 1100.0\n        self.f0_min = 50.0\n        self.f0_mel_min = 1127 * np.log(1 + self.f0_min / 700)\n        self.f0_mel_max = 1127 * np.log(1 + self.f0_max / 700)\n    def compute_f0(self, path, f0_method):",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "printt",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "def printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256\n        self.f0_max = 1100.0",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nimport logging\nimport numpy as np\nimport pyworld\nfrom infer.lib.audio import load_audio\nlogging.getLogger(\"numba\").setLevel(logging.WARNING)\nn_part = int(sys.argv[1])\ni_part = int(sys.argv[2])\ni_gpu = sys.argv[3]",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "n_part",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "n_part = int(sys.argv[1])\ni_part = int(sys.argv[2])\ni_gpu = sys.argv[3]\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i_gpu)\nexp_dir = sys.argv[4]\nis_half = sys.argv[5]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "i_part",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "i_part = int(sys.argv[2])\ni_gpu = sys.argv[3]\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i_gpu)\nexp_dir = sys.argv[4]\nis_half = sys.argv[5]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "i_gpu",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "i_gpu = sys.argv[3]\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i_gpu)\nexp_dir = sys.argv[4]\nis_half = sys.argv[5]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i_gpu)\nexp_dir = sys.argv[4]\nis_half = sys.argv[5]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "exp_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "exp_dir = sys.argv[4]\nis_half = sys.argv[5]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "is_half",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "is_half = sys.argv[5]\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "f",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "peekOfCode": "f = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe",
        "documentation": {}
    },
    {
        "label": "FeatureInput",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "peekOfCode": "class FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256\n        self.f0_max = 1100.0\n        self.f0_min = 50.0\n        self.f0_mel_min = 1127 * np.log(1 + self.f0_min / 700)\n        self.f0_mel_max = 1127 * np.log(1 + self.f0_max / 700)\n    def compute_f0(self, path, f0_method):",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "documentation": {}
    },
    {
        "label": "printt",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "peekOfCode": "def printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256\n        self.f0_max = 1100.0",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nimport logging\nimport numpy as np\nimport pyworld\nfrom infer.lib.audio import load_audio\nlogging.getLogger(\"numba\").setLevel(logging.WARNING)\nexp_dir = sys.argv[1]\nimport torch_directml\ndevice = torch_directml.device(torch_directml.default_device())",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "documentation": {}
    },
    {
        "label": "exp_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "peekOfCode": "exp_dir = sys.argv[1]\nimport torch_directml\ndevice = torch_directml.device(torch_directml.default_device())\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "peekOfCode": "device = torch_directml.device(torch_directml.default_device())\nf = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "documentation": {}
    },
    {
        "label": "f",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "description": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "peekOfCode": "f = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass FeatureInput(object):\n    def __init__(self, samplerate=16000, hop_size=160):\n        self.fs = samplerate\n        self.hop = hop_size\n        self.f0_bin = 256",
        "detail": "Models.Sound.infer.modules.train.extract.extract_f0_rmvpe_dml",
        "documentation": {}
    },
    {
        "label": "printt",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "def printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nprintt(\" \".join(sys.argv))\nmodel_path = \"assets/hubert/hubert_base.pt\"\nprintt(\"exp_dir: \" + exp_dir)\nwavPath = \"%s/1_16k_wavs\" % exp_dir\noutPath = (\n    \"%s/3_feature256\" % exp_dir if version == \"v1\" else \"%s/3_feature768\" % exp_dir",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "readwave",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "def readwave(wav_path, normalize=False):\n    wav, sr = sf.read(wav_path)\n    assert sr == 16000\n    feats = torch.from_numpy(wav).float()\n    if feats.dim() == 2:  # double channels\n        feats = feats.mean(-1)\n    assert feats.dim() == 1, feats.dim()\n    if normalize:\n        with torch.no_grad():\n            feats = F.layer_norm(feats, feats.shape)",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\ndevice = sys.argv[1]\nn_part = int(sys.argv[2])\ni_part = int(sys.argv[3])\nif len(sys.argv) == 7:\n    exp_dir = sys.argv[4]\n    version = sys.argv[5]\n    is_half = sys.argv[6].lower() == \"true\"\nelse:",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\ndevice = sys.argv[1]\nn_part = int(sys.argv[2])\ni_part = int(sys.argv[3])\nif len(sys.argv) == 7:\n    exp_dir = sys.argv[4]\n    version = sys.argv[5]\n    is_half = sys.argv[6].lower() == \"true\"\nelse:\n    i_gpu = sys.argv[4]",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "device = sys.argv[1]\nn_part = int(sys.argv[2])\ni_part = int(sys.argv[3])\nif len(sys.argv) == 7:\n    exp_dir = sys.argv[4]\n    version = sys.argv[5]\n    is_half = sys.argv[6].lower() == \"true\"\nelse:\n    i_gpu = sys.argv[4]\n    exp_dir = sys.argv[5]",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "n_part",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "n_part = int(sys.argv[2])\ni_part = int(sys.argv[3])\nif len(sys.argv) == 7:\n    exp_dir = sys.argv[4]\n    version = sys.argv[5]\n    is_half = sys.argv[6].lower() == \"true\"\nelse:\n    i_gpu = sys.argv[4]\n    exp_dir = sys.argv[5]\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i_gpu)",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "i_part",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "i_part = int(sys.argv[3])\nif len(sys.argv) == 7:\n    exp_dir = sys.argv[4]\n    version = sys.argv[5]\n    is_half = sys.argv[6].lower() == \"true\"\nelse:\n    i_gpu = sys.argv[4]\n    exp_dir = sys.argv[5]\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i_gpu)\n    version = sys.argv[6]",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "f",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "f = open(\"%s/extract_f0_feature.log\" % exp_dir, \"a+\")\ndef printt(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nprintt(\" \".join(sys.argv))\nmodel_path = \"assets/hubert/hubert_base.pt\"\nprintt(\"exp_dir: \" + exp_dir)\nwavPath = \"%s/1_16k_wavs\" % exp_dir\noutPath = (",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "model_path = \"assets/hubert/hubert_base.pt\"\nprintt(\"exp_dir: \" + exp_dir)\nwavPath = \"%s/1_16k_wavs\" % exp_dir\noutPath = (\n    \"%s/3_feature256\" % exp_dir if version == \"v1\" else \"%s/3_feature768\" % exp_dir\n)\nos.makedirs(outPath, exist_ok=True)\n# wave must be 16k, hop_size=320\ndef readwave(wav_path, normalize=False):\n    wav, sr = sf.read(wav_path)",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "wavPath",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "wavPath = \"%s/1_16k_wavs\" % exp_dir\noutPath = (\n    \"%s/3_feature256\" % exp_dir if version == \"v1\" else \"%s/3_feature768\" % exp_dir\n)\nos.makedirs(outPath, exist_ok=True)\n# wave must be 16k, hop_size=320\ndef readwave(wav_path, normalize=False):\n    wav, sr = sf.read(wav_path)\n    assert sr == 16000\n    feats = torch.from_numpy(wav).float()",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "outPath",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "outPath = (\n    \"%s/3_feature256\" % exp_dir if version == \"v1\" else \"%s/3_feature768\" % exp_dir\n)\nos.makedirs(outPath, exist_ok=True)\n# wave must be 16k, hop_size=320\ndef readwave(wav_path, normalize=False):\n    wav, sr = sf.read(wav_path)\n    assert sr == 16000\n    feats = torch.from_numpy(wav).float()\n    if feats.dim() == 2:  # double channels",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "model = models[0]\nmodel = model.to(device)\nprintt(\"move model to %s\" % device)\nif is_half:\n    if device not in [\"mps\", \"cpu\"]:\n        model = model.half()\nmodel.eval()\ntodo = sorted(list(os.listdir(wavPath)))[i_part::n_part]\nn = max(1, len(todo) // 10)  # 最多打印十条\nif len(todo) == 0:",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "model = model.to(device)\nprintt(\"move model to %s\" % device)\nif is_half:\n    if device not in [\"mps\", \"cpu\"]:\n        model = model.half()\nmodel.eval()\ntodo = sorted(list(os.listdir(wavPath)))[i_part::n_part]\nn = max(1, len(todo) // 10)  # 最多打印十条\nif len(todo) == 0:\n    printt(\"no-feature-todo\")",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "todo",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "todo = sorted(list(os.listdir(wavPath)))[i_part::n_part]\nn = max(1, len(todo) // 10)  # 最多打印十条\nif len(todo) == 0:\n    printt(\"no-feature-todo\")\nelse:\n    printt(\"all-feature-%s\" % len(todo))\n    for idx, file in enumerate(todo):\n        try:\n            if file.endswith(\".wav\"):\n                wav_path = \"%s/%s\" % (wavPath, file)",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.extract_feature_print",
        "description": "Models.Sound.infer.modules.train.extract_feature_print",
        "peekOfCode": "n = max(1, len(todo) // 10)  # 最多打印十条\nif len(todo) == 0:\n    printt(\"no-feature-todo\")\nelse:\n    printt(\"all-feature-%s\" % len(todo))\n    for idx, file in enumerate(todo):\n        try:\n            if file.endswith(\".wav\"):\n                wav_path = \"%s/%s\" % (wavPath, file)\n                out_path = \"%s/%s\" % (outPath, file.replace(\"wav\", \"npy\"))",
        "detail": "Models.Sound.infer.modules.train.extract_feature_print",
        "documentation": {}
    },
    {
        "label": "PreProcess",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "class PreProcess:\n    def __init__(self, sr, exp_dir, per=3.7):\n        self.slicer = Slicer(\n            sr=sr,\n            threshold=-42,\n            min_length=1500,\n            min_interval=400,\n            hop_size=15,\n            max_sil_kept=500,\n        )",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "println",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "def println(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass PreProcess:\n    def __init__(self, sr, exp_dir, per=3.7):\n        self.slicer = Slicer(\n            sr=sr,\n            threshold=-42,\n            min_length=1500,",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "preprocess_trainset",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "def preprocess_trainset(inp_root, sr, n_p, exp_dir, per):\n    pp = PreProcess(sr, exp_dir, per)\n    println(\"start preprocess\")\n    pp.pipeline_mp_inp_dir(inp_root, n_p)\n    println(\"end preprocess\")\nif __name__ == \"__main__\":\n    preprocess_trainset(inp_root, sr, n_p, exp_dir, per)",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nprint(*sys.argv[1:])\ninp_root = sys.argv[1]\nsr = int(sys.argv[2])\nn_p = int(sys.argv[3])\nexp_dir = sys.argv[4]\nnoparallel = sys.argv[5] == \"True\"\nper = float(sys.argv[6])\nimport os",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "inp_root",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "inp_root = sys.argv[1]\nsr = int(sys.argv[2])\nn_p = int(sys.argv[3])\nexp_dir = sys.argv[4]\nnoparallel = sys.argv[5] == \"True\"\nper = float(sys.argv[6])\nimport os\nimport traceback\nimport librosa\nimport numpy as np",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "sr",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "sr = int(sys.argv[2])\nn_p = int(sys.argv[3])\nexp_dir = sys.argv[4]\nnoparallel = sys.argv[5] == \"True\"\nper = float(sys.argv[6])\nimport os\nimport traceback\nimport librosa\nimport numpy as np\nfrom scipy.io import wavfile",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "n_p",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "n_p = int(sys.argv[3])\nexp_dir = sys.argv[4]\nnoparallel = sys.argv[5] == \"True\"\nper = float(sys.argv[6])\nimport os\nimport traceback\nimport librosa\nimport numpy as np\nfrom scipy.io import wavfile\nfrom infer.lib.audio import load_audio",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "exp_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "exp_dir = sys.argv[4]\nnoparallel = sys.argv[5] == \"True\"\nper = float(sys.argv[6])\nimport os\nimport traceback\nimport librosa\nimport numpy as np\nfrom scipy.io import wavfile\nfrom infer.lib.audio import load_audio\nfrom infer.lib.slicer2 import Slicer",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "noparallel",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "noparallel = sys.argv[5] == \"True\"\nper = float(sys.argv[6])\nimport os\nimport traceback\nimport librosa\nimport numpy as np\nfrom scipy.io import wavfile\nfrom infer.lib.audio import load_audio\nfrom infer.lib.slicer2 import Slicer\nf = open(\"%s/preprocess.log\" % exp_dir, \"a+\")",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "per",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "per = float(sys.argv[6])\nimport os\nimport traceback\nimport librosa\nimport numpy as np\nfrom scipy.io import wavfile\nfrom infer.lib.audio import load_audio\nfrom infer.lib.slicer2 import Slicer\nf = open(\"%s/preprocess.log\" % exp_dir, \"a+\")\ndef println(strr):",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "f",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.preprocess",
        "description": "Models.Sound.infer.modules.train.preprocess",
        "peekOfCode": "f = open(\"%s/preprocess.log\" % exp_dir, \"a+\")\ndef println(strr):\n    print(strr)\n    f.write(\"%s\\n\" % strr)\n    f.flush()\nclass PreProcess:\n    def __init__(self, sr, exp_dir, per=3.7):\n        self.slicer = Slicer(\n            sr=sr,\n            threshold=-42,",
        "detail": "Models.Sound.infer.modules.train.preprocess",
        "documentation": {}
    },
    {
        "label": "EpochRecorder",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "class EpochRecorder:\n    def __init__(self):\n        self.last_time = ttime()\n    def record(self):\n        now_time = ttime()\n        elapsed_time = now_time - self.last_time\n        self.last_time = now_time\n        elapsed_time_str = str(datetime.timedelta(seconds=elapsed_time))\n        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        return f\"[{current_time}] | ({elapsed_time_str})\"",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "def main():\n    n_gpus = torch.cuda.device_count()\n    if torch.cuda.is_available() == False and torch.backends.mps.is_available() == True:\n        n_gpus = 1\n    if n_gpus < 1:\n        # patch to unblock people without gpus. there is probably a better way.\n        print(\"NO GPU DETECTED: falling back to CPU - this may take a while\")\n        n_gpus = 1\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = str(randint(20000, 55555))",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "def run(rank, n_gpus, hps, logger: logging.Logger):\n    global global_step\n    if rank == 0:\n        # logger = utils.get_logger(hps.model_dir)\n        logger.info(hps)\n        # utils.check_git_hash(hps.model_dir)\n        writer = SummaryWriter(log_dir=hps.model_dir)\n        writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n    dist.init_process_group(\n        backend=\"gloo\", init_method=\"env://\", world_size=n_gpus, rank=rank",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "train_and_evaluate",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "def train_and_evaluate(\n    rank, epoch, hps, nets, optims, schedulers, scaler, loaders, logger, writers, cache\n):\n    net_g, net_d = nets\n    optim_g, optim_d = optims\n    train_loader, eval_loader = loaders\n    if writers is not None:\n        writer, writer_eval = writers\n    train_loader.batch_sampler.set_epoch(epoch)\n    global global_step",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "logger = logging.getLogger(__name__)\nnow_dir = os.getcwd()\nsys.path.append(os.path.join(now_dir))\nimport datetime\nfrom infer.lib.train import utils\nhps = utils.get_hparams()\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = hps.gpus.replace(\"-\", \",\")\nn_gpus = len(hps.gpus.split(\"-\"))\nfrom random import randint, shuffle\nimport torch",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(os.path.join(now_dir))\nimport datetime\nfrom infer.lib.train import utils\nhps = utils.get_hparams()\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = hps.gpus.replace(\"-\", \",\")\nn_gpus = len(hps.gpus.split(\"-\"))\nfrom random import randint, shuffle\nimport torch\ntry:",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "hps",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "hps = utils.get_hparams()\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = hps.gpus.replace(\"-\", \",\")\nn_gpus = len(hps.gpus.split(\"-\"))\nfrom random import randint, shuffle\nimport torch\ntry:\n    import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import\n    if torch.xpu.is_available():\n        from infer.modules.ipex import ipex_init\n        from infer.modules.ipex.gradscaler import gradscaler_init",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = hps.gpus.replace(\"-\", \",\")\nn_gpus = len(hps.gpus.split(\"-\"))\nfrom random import randint, shuffle\nimport torch\ntry:\n    import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import\n    if torch.xpu.is_available():\n        from infer.modules.ipex import ipex_init\n        from infer.modules.ipex.gradscaler import gradscaler_init\n        from torch.xpu.amp import autocast",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "n_gpus",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "n_gpus = len(hps.gpus.split(\"-\"))\nfrom random import randint, shuffle\nimport torch\ntry:\n    import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import\n    if torch.xpu.is_available():\n        from infer.modules.ipex import ipex_init\n        from infer.modules.ipex.gradscaler import gradscaler_init\n        from torch.xpu.amp import autocast\n        GradScaler = gradscaler_init()",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.deterministic",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "torch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = False\nfrom time import sleep\nfrom time import time as ttime\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn import functional as F\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.benchmark",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "torch.backends.cudnn.benchmark = False\nfrom time import sleep\nfrom time import time as ttime\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn import functional as F\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom infer.lib.infer_pack import commons",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.train.train",
        "description": "Models.Sound.infer.modules.train.train",
        "peekOfCode": "global_step = 0\nclass EpochRecorder:\n    def __init__(self):\n        self.last_time = ttime()\n    def record(self):\n        now_time = ttime()\n        elapsed_time = now_time - self.last_time\n        self.last_time = now_time\n        elapsed_time_str = str(datetime.timedelta(seconds=elapsed_time))\n        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")",
        "detail": "Models.Sound.infer.modules.train.train",
        "documentation": {}
    },
    {
        "label": "ConvTDFNetTrim",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.uvr5.mdxnet",
        "description": "Models.Sound.infer.modules.uvr5.mdxnet",
        "peekOfCode": "class ConvTDFNetTrim:\n    def __init__(\n        self, device, model_name, target_name, L, dim_f, dim_t, n_fft, hop=1024\n    ):\n        super(ConvTDFNetTrim, self).__init__()\n        self.dim_f = dim_f\n        self.dim_t = 2**dim_t\n        self.n_fft = n_fft\n        self.hop = hop\n        self.n_bins = self.n_fft // 2 + 1",
        "detail": "Models.Sound.infer.modules.uvr5.mdxnet",
        "documentation": {}
    },
    {
        "label": "Predictor",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.uvr5.mdxnet",
        "description": "Models.Sound.infer.modules.uvr5.mdxnet",
        "peekOfCode": "class Predictor:\n    def __init__(self, args):\n        import onnxruntime as ort\n        logger.info(ort.get_available_providers())\n        self.args = args\n        self.model_ = get_models(\n            device=cpu, dim_f=args.dim_f, dim_t=args.dim_t, n_fft=args.n_fft\n        )\n        self.model = ort.InferenceSession(\n            os.path.join(args.onnx, self.model_.target_name + \".onnx\"),",
        "detail": "Models.Sound.infer.modules.uvr5.mdxnet",
        "documentation": {}
    },
    {
        "label": "MDXNetDereverb",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.uvr5.mdxnet",
        "description": "Models.Sound.infer.modules.uvr5.mdxnet",
        "peekOfCode": "class MDXNetDereverb:\n    def __init__(self, chunks, device):\n        self.onnx = \"assets/uvr5_weights/onnx_dereverb_By_FoxJoy\"\n        self.shifts = 10  # 'Predict with randomised equivariant stabilisation'\n        self.mixing = \"min_mag\"  # ['default','min_mag','max_mag']\n        self.chunks = chunks\n        self.margin = 44100\n        self.dim_t = 9\n        self.dim_f = 3072\n        self.n_fft = 6144",
        "detail": "Models.Sound.infer.modules.uvr5.mdxnet",
        "documentation": {}
    },
    {
        "label": "get_models",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.uvr5.mdxnet",
        "description": "Models.Sound.infer.modules.uvr5.mdxnet",
        "peekOfCode": "def get_models(device, dim_f, dim_t, n_fft):\n    return ConvTDFNetTrim(\n        device=device,\n        model_name=\"Conv-TDF\",\n        target_name=\"vocals\",\n        L=11,\n        dim_f=dim_f,\n        dim_t=dim_t,\n        n_fft=n_fft,\n    )",
        "detail": "Models.Sound.infer.modules.uvr5.mdxnet",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.uvr5.mdxnet",
        "description": "Models.Sound.infer.modules.uvr5.mdxnet",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport torch\nfrom tqdm import tqdm\ncpu = torch.device(\"cpu\")\nclass ConvTDFNetTrim:\n    def __init__(\n        self, device, model_name, target_name, L, dim_f, dim_t, n_fft, hop=1024",
        "detail": "Models.Sound.infer.modules.uvr5.mdxnet",
        "documentation": {}
    },
    {
        "label": "cpu",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.uvr5.mdxnet",
        "description": "Models.Sound.infer.modules.uvr5.mdxnet",
        "peekOfCode": "cpu = torch.device(\"cpu\")\nclass ConvTDFNetTrim:\n    def __init__(\n        self, device, model_name, target_name, L, dim_f, dim_t, n_fft, hop=1024\n    ):\n        super(ConvTDFNetTrim, self).__init__()\n        self.dim_f = dim_f\n        self.dim_t = 2**dim_t\n        self.n_fft = n_fft\n        self.hop = hop",
        "detail": "Models.Sound.infer.modules.uvr5.mdxnet",
        "documentation": {}
    },
    {
        "label": "uvr",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.uvr5.modules",
        "description": "Models.Sound.infer.modules.uvr5.modules",
        "peekOfCode": "def uvr(model_name, inp_root, save_root_vocal, paths, save_root_ins, agg, format0):\n    infos = []\n    try:\n        inp_root = inp_root.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        save_root_vocal = (\n            save_root_vocal.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        )\n        save_root_ins = (\n            save_root_ins.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        )",
        "detail": "Models.Sound.infer.modules.uvr5.modules",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.uvr5.modules",
        "description": "Models.Sound.infer.modules.uvr5.modules",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport ffmpeg\nimport torch\nfrom configs.config import Config\nfrom infer.modules.uvr5.mdxnet import MDXNetDereverb\nfrom infer.modules.uvr5.vr import AudioPre, AudioPreDeEcho\nconfig = Config()\ndef uvr(model_name, inp_root, save_root_vocal, paths, save_root_ins, agg, format0):\n    infos = []\n    try:",
        "detail": "Models.Sound.infer.modules.uvr5.modules",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.uvr5.modules",
        "description": "Models.Sound.infer.modules.uvr5.modules",
        "peekOfCode": "config = Config()\ndef uvr(model_name, inp_root, save_root_vocal, paths, save_root_ins, agg, format0):\n    infos = []\n    try:\n        inp_root = inp_root.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        save_root_vocal = (\n            save_root_vocal.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        )\n        save_root_ins = (\n            save_root_ins.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")",
        "detail": "Models.Sound.infer.modules.uvr5.modules",
        "documentation": {}
    },
    {
        "label": "AudioPre",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.uvr5.vr",
        "description": "Models.Sound.infer.modules.uvr5.vr",
        "peekOfCode": "class AudioPre:\n    def __init__(self, agg, model_path, device, is_half, tta=False):\n        self.model_path = model_path\n        self.device = device\n        self.data = {\n            # Processing Options\n            \"postprocess\": False,\n            \"tta\": tta,\n            # Constants\n            \"window_size\": 512,",
        "detail": "Models.Sound.infer.modules.uvr5.vr",
        "documentation": {}
    },
    {
        "label": "AudioPreDeEcho",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.uvr5.vr",
        "description": "Models.Sound.infer.modules.uvr5.vr",
        "peekOfCode": "class AudioPreDeEcho:\n    def __init__(self, agg, model_path, device, is_half, tta=False):\n        self.model_path = model_path\n        self.device = device\n        self.data = {\n            # Processing Options\n            \"postprocess\": False,\n            \"tta\": tta,\n            # Constants\n            \"window_size\": 512,",
        "detail": "Models.Sound.infer.modules.uvr5.vr",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.uvr5.vr",
        "description": "Models.Sound.infer.modules.uvr5.vr",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport torch\nfrom infer.lib.uvr5_pack.lib_v5 import nets_61968KB as Nets\nfrom infer.lib.uvr5_pack.lib_v5 import spec_utils\nfrom infer.lib.uvr5_pack.lib_v5.model_param_init import ModelParameters\nfrom infer.lib.uvr5_pack.lib_v5.nets_new import CascadedNet\nfrom infer.lib.uvr5_pack.utils import inference",
        "detail": "Models.Sound.infer.modules.uvr5.vr",
        "documentation": {}
    },
    {
        "label": "VC",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.vc.modules",
        "description": "Models.Sound.infer.modules.vc.modules",
        "peekOfCode": "class VC:\n    def __init__(self, config):\n        self.n_spk = None\n        self.tgt_sr = None\n        self.net_g = None\n        self.pipeline = None\n        self.cpt = None\n        self.version = None\n        self.if_f0 = None\n        self.version = None",
        "detail": "Models.Sound.infer.modules.vc.modules",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.vc.modules",
        "description": "Models.Sound.infer.modules.vc.modules",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport numpy as np\nimport soundfile as sf\nimport torch\nfrom io import BytesIO\nfrom infer.lib.audio import load_audio, wav2\nfrom infer.lib.infer_pack.models import (\n    SynthesizerTrnMs256NSFsid,\n    SynthesizerTrnMs256NSFsid_nono,\n    SynthesizerTrnMs768NSFsid,",
        "detail": "Models.Sound.infer.modules.vc.modules",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "kind": 6,
        "importPath": "Models.Sound.infer.modules.vc.pipeline",
        "description": "Models.Sound.infer.modules.vc.pipeline",
        "peekOfCode": "class Pipeline(object):\n    def __init__(self, tgt_sr, config):\n        self.x_pad, self.x_query, self.x_center, self.x_max, self.is_half = (\n            config.x_pad,\n            config.x_query,\n            config.x_center,\n            config.x_max,\n            config.is_half,\n        )\n        self.sr = 16000  # hubert输入采样率",
        "detail": "Models.Sound.infer.modules.vc.pipeline",
        "documentation": {}
    },
    {
        "label": "cache_harvest_f0",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.vc.pipeline",
        "description": "Models.Sound.infer.modules.vc.pipeline",
        "peekOfCode": "def cache_harvest_f0(input_audio_path, fs, f0max, f0min, frame_period):\n    audio = input_audio_path2wav[input_audio_path]\n    f0, t = pyworld.harvest(\n        audio,\n        fs=fs,\n        f0_ceil=f0max,\n        f0_floor=f0min,\n        frame_period=frame_period,\n    )\n    f0 = pyworld.stonemask(audio, f0, t, fs)",
        "detail": "Models.Sound.infer.modules.vc.pipeline",
        "documentation": {}
    },
    {
        "label": "change_rms",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.vc.pipeline",
        "description": "Models.Sound.infer.modules.vc.pipeline",
        "peekOfCode": "def change_rms(data1, sr1, data2, sr2, rate):  # 1是输入音频，2是输出音频,rate是2的占比\n    # print(data1.max(),data2.max())\n    rms1 = librosa.feature.rms(\n        y=data1, frame_length=sr1 // 2 * 2, hop_length=sr1 // 2\n    )  # 每半秒一个点\n    rms2 = librosa.feature.rms(y=data2, frame_length=sr2 // 2 * 2, hop_length=sr2 // 2)\n    rms1 = torch.from_numpy(rms1)\n    rms1 = F.interpolate(\n        rms1.unsqueeze(0), size=data2.shape[0], mode=\"linear\"\n    ).squeeze()",
        "detail": "Models.Sound.infer.modules.vc.pipeline",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.vc.pipeline",
        "description": "Models.Sound.infer.modules.vc.pipeline",
        "peekOfCode": "logger = logging.getLogger(__name__)\nfrom functools import lru_cache\nfrom time import time as ttime\nimport faiss\nimport librosa\nimport numpy as np\nimport parselmouth\nimport pyworld\nimport torch\nimport torch.nn.functional as F",
        "detail": "Models.Sound.infer.modules.vc.pipeline",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.vc.pipeline",
        "description": "Models.Sound.infer.modules.vc.pipeline",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nbh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=16000)\ninput_audio_path2wav = {}\n@lru_cache\ndef cache_harvest_f0(input_audio_path, fs, f0max, f0min, frame_period):\n    audio = input_audio_path2wav[input_audio_path]\n    f0, t = pyworld.harvest(\n        audio,\n        fs=fs,",
        "detail": "Models.Sound.infer.modules.vc.pipeline",
        "documentation": {}
    },
    {
        "label": "input_audio_path2wav",
        "kind": 5,
        "importPath": "Models.Sound.infer.modules.vc.pipeline",
        "description": "Models.Sound.infer.modules.vc.pipeline",
        "peekOfCode": "input_audio_path2wav = {}\n@lru_cache\ndef cache_harvest_f0(input_audio_path, fs, f0max, f0min, frame_period):\n    audio = input_audio_path2wav[input_audio_path]\n    f0, t = pyworld.harvest(\n        audio,\n        fs=fs,\n        f0_ceil=f0max,\n        f0_floor=f0min,\n        frame_period=frame_period,",
        "detail": "Models.Sound.infer.modules.vc.pipeline",
        "documentation": {}
    },
    {
        "label": "get_index_path_from_model",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.vc.utils",
        "description": "Models.Sound.infer.modules.vc.utils",
        "peekOfCode": "def get_index_path_from_model(sid):\n    return next(\n        (\n            f\n            for f in [\n                os.path.join(root, name)\n                for root, _, files in os.walk(os.getenv(\"index_root\"), topdown=False)\n                for name in files\n                if name.endswith(\".index\") and \"trained\" not in name\n            ]",
        "detail": "Models.Sound.infer.modules.vc.utils",
        "documentation": {}
    },
    {
        "label": "load_hubert",
        "kind": 2,
        "importPath": "Models.Sound.infer.modules.vc.utils",
        "description": "Models.Sound.infer.modules.vc.utils",
        "peekOfCode": "def load_hubert(config):\n    models, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n        [\"assets/hubert/hubert_base.pt\"],\n        suffix=\"\",\n    )\n    hubert_model = models[0]\n    hubert_model = hubert_model.to(config.device)\n    if config.is_half:\n        hubert_model = hubert_model.half()\n    else:",
        "detail": "Models.Sound.infer.modules.vc.utils",
        "documentation": {}
    },
    {
        "label": "get_f0",
        "kind": 2,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "def get_f0(x, p_len, f0_up_key=0):\n    time_step = 160 / 16000 * 1000\n    f0_min = 50\n    f0_max = 1100\n    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n    f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n    f0 = (\n        parselmouth.Sound(x, 16000)\n        .to_pitch_ac(\n            time_step=time_step / 1000,",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport parselmouth\nimport torch\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# import torchcrepe\nfrom time import time as ttime\n# import pyworld\nimport librosa\nimport numpy as np\nimport soundfile as sf",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# import torchcrepe\nfrom time import time as ttime\n# import pyworld\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport torch.nn.functional as F\nfrom fairseq import checkpoint_utils\n# from models import SynthesizerTrn256#hifigan_nonsf",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_path = r\"E:\\codes\\py39\\vits_vc_gpu_train\\assets\\hubert\\hubert_base.pt\"  #\nlogger.info(\"Load model(s) from {}\".format(model_path))\nmodels, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task(\n    [model_path],\n    suffix=\"\",\n)\nmodel = models[0]\nmodel = model.to(device)\nmodel = model.half()",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "model_path = r\"E:\\codes\\py39\\vits_vc_gpu_train\\assets\\hubert\\hubert_base.pt\"  #\nlogger.info(\"Load model(s) from {}\".format(model_path))\nmodels, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task(\n    [model_path],\n    suffix=\"\",\n)\nmodel = models[0]\nmodel = model.to(device)\nmodel = model.half()\nmodel.eval()",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "model = models[0]\nmodel = model.to(device)\nmodel = model.half()\nmodel.eval()\n# net_g = SynthesizerTrn256(1025,32,192,192,768,2,6,3,0.1,\"1\", [3,7,11],[[1,3,5], [1,3,5], [1,3,5]],[10,10,2,2],512,[16,16,4,4],183,256,is_half=True)#hifigan#512#256\n# net_g = SynthesizerTrn256(1025,32,192,192,768,2,6,3,0.1,\"1\", [3,7,11],[[1,3,5], [1,3,5], [1,3,5]],[10,10,2,2],512,[16,16,4,4],109,256,is_half=True)#hifigan#512#256\nnet_g = SynthesizerTrn256(\n    1025,\n    32,\n    192,",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "model = model.to(device)\nmodel = model.half()\nmodel.eval()\n# net_g = SynthesizerTrn256(1025,32,192,192,768,2,6,3,0.1,\"1\", [3,7,11],[[1,3,5], [1,3,5], [1,3,5]],[10,10,2,2],512,[16,16,4,4],183,256,is_half=True)#hifigan#512#256\n# net_g = SynthesizerTrn256(1025,32,192,192,768,2,6,3,0.1,\"1\", [3,7,11],[[1,3,5], [1,3,5], [1,3,5]],[10,10,2,2],512,[16,16,4,4],109,256,is_half=True)#hifigan#512#256\nnet_g = SynthesizerTrn256(\n    1025,\n    32,\n    192,\n    192,",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "model = model.half()\nmodel.eval()\n# net_g = SynthesizerTrn256(1025,32,192,192,768,2,6,3,0.1,\"1\", [3,7,11],[[1,3,5], [1,3,5], [1,3,5]],[10,10,2,2],512,[16,16,4,4],183,256,is_half=True)#hifigan#512#256\n# net_g = SynthesizerTrn256(1025,32,192,192,768,2,6,3,0.1,\"1\", [3,7,11],[[1,3,5], [1,3,5], [1,3,5]],[10,10,2,2],512,[16,16,4,4],109,256,is_half=True)#hifigan#512#256\nnet_g = SynthesizerTrn256(\n    1025,\n    32,\n    192,\n    192,\n    768,",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "net_g",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "net_g = SynthesizerTrn256(\n    1025,\n    32,\n    192,\n    192,\n    768,\n    2,\n    6,\n    3,\n    0,",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "weights",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "weights = torch.load(\"infer/ft-mi-no_opt-no_dropout.pt\")\nlogger.debug(net_g.load_state_dict(weights, strict=True))\nnet_g.eval().to(device)\nnet_g.half()\ndef get_f0(x, p_len, f0_up_key=0):\n    time_step = 160 / 16000 * 1000\n    f0_min = 50\n    f0_max = 1100\n    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n    f0_mel_max = 1127 * np.log(1 + f0_max / 700)",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "index = faiss.read_index(\"infer/added_IVF512_Flat_mi_baseline_src_feat.index\")\nbig_npy = np.load(\"infer/big_src_feature_mi.npy\")\nta0 = ta1 = ta2 = 0\nfor idx, name in enumerate(\n    [\n        \"冬之花clip1.wav\",\n    ]\n):  ##\n    wav_path = \"todo-songs/%s\" % name  #\n    f0_up_key = -2  #",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "big_npy",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "big_npy = np.load(\"infer/big_src_feature_mi.npy\")\nta0 = ta1 = ta2 = 0\nfor idx, name in enumerate(\n    [\n        \"冬之花clip1.wav\",\n    ]\n):  ##\n    wav_path = \"todo-songs/%s\" % name  #\n    f0_up_key = -2  #\n    audio, sampling_rate = sf.read(wav_path)",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "ta0",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.infer-pm-index256",
        "description": "Models.Sound.tools.infer.infer-pm-index256",
        "peekOfCode": "ta0 = ta1 = ta2 = 0\nfor idx, name in enumerate(\n    [\n        \"冬之花clip1.wav\",\n    ]\n):  ##\n    wav_path = \"todo-songs/%s\" % name  #\n    f0_up_key = -2  #\n    audio, sampling_rate = sf.read(wav_path)\n    if len(audio.shape) > 1:",
        "detail": "Models.Sound.tools.infer.infer-pm-index256",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "logger = logging.getLogger(__name__)\nfrom multiprocessing import cpu_count\nimport faiss\nimport numpy as np\nfrom sklearn.cluster import MiniBatchKMeans\n# ###########如果是原始特征要先写save\nn_cpu = 0\nif n_cpu == 0:\n    n_cpu = cpu_count()\ninp_root = r\"./logs/anz/3_feature768\"",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "n_cpu",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "n_cpu = 0\nif n_cpu == 0:\n    n_cpu = cpu_count()\ninp_root = r\"./logs/anz/3_feature768\"\nnpys = []\nlistdir_res = list(os.listdir(inp_root))\nfor name in sorted(listdir_res):\n    phone = np.load(\"%s/%s\" % (inp_root, name))\n    npys.append(phone)\nbig_npy = np.concatenate(npys, 0)",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "inp_root",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "inp_root = r\"./logs/anz/3_feature768\"\nnpys = []\nlistdir_res = list(os.listdir(inp_root))\nfor name in sorted(listdir_res):\n    phone = np.load(\"%s/%s\" % (inp_root, name))\n    npys.append(phone)\nbig_npy = np.concatenate(npys, 0)\nbig_npy_idx = np.arange(big_npy.shape[0])\nnp.random.shuffle(big_npy_idx)\nbig_npy = big_npy[big_npy_idx]",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "npys",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "npys = []\nlistdir_res = list(os.listdir(inp_root))\nfor name in sorted(listdir_res):\n    phone = np.load(\"%s/%s\" % (inp_root, name))\n    npys.append(phone)\nbig_npy = np.concatenate(npys, 0)\nbig_npy_idx = np.arange(big_npy.shape[0])\nnp.random.shuffle(big_npy_idx)\nbig_npy = big_npy[big_npy_idx]\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "listdir_res",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "listdir_res = list(os.listdir(inp_root))\nfor name in sorted(listdir_res):\n    phone = np.load(\"%s/%s\" % (inp_root, name))\n    npys.append(phone)\nbig_npy = np.concatenate(npys, 0)\nbig_npy_idx = np.arange(big_npy.shape[0])\nnp.random.shuffle(big_npy_idx)\nbig_npy = big_npy[big_npy_idx]\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G\nif big_npy.shape[0] > 2e5:",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "big_npy",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "big_npy = np.concatenate(npys, 0)\nbig_npy_idx = np.arange(big_npy.shape[0])\nnp.random.shuffle(big_npy_idx)\nbig_npy = big_npy[big_npy_idx]\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G\nif big_npy.shape[0] > 2e5:\n    # if(1):\n    info = \"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0]\n    logger.info(info)\n    try:",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "big_npy_idx",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "big_npy_idx = np.arange(big_npy.shape[0])\nnp.random.shuffle(big_npy_idx)\nbig_npy = big_npy[big_npy_idx]\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G\nif big_npy.shape[0] > 2e5:\n    # if(1):\n    info = \"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0]\n    logger.info(info)\n    try:\n        big_npy = (",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "big_npy",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "big_npy = big_npy[big_npy_idx]\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G\nif big_npy.shape[0] > 2e5:\n    # if(1):\n    info = \"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0]\n    logger.info(info)\n    try:\n        big_npy = (\n            MiniBatchKMeans(\n                n_clusters=10000,",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "n_ivf",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "n_ivf = min(int(16 * np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)\nindex = faiss.index_factory(768, \"IVF%s,Flat\" % n_ivf)  # mi\nlogger.info(\"Training...\")\nindex_ivf = faiss.extract_index_ivf(index)  #\nindex_ivf.nprobe = 1\nindex.train(big_npy)\nfaiss.write_index(\n    index, \"tools/infer/trained_IVF%s_Flat_baseline_src_feat_v2.index\" % (n_ivf)\n)\nlogger.info(\"Adding...\")",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "index = faiss.index_factory(768, \"IVF%s,Flat\" % n_ivf)  # mi\nlogger.info(\"Training...\")\nindex_ivf = faiss.extract_index_ivf(index)  #\nindex_ivf.nprobe = 1\nindex.train(big_npy)\nfaiss.write_index(\n    index, \"tools/infer/trained_IVF%s_Flat_baseline_src_feat_v2.index\" % (n_ivf)\n)\nlogger.info(\"Adding...\")\nbatch_size_add = 8192",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "index_ivf",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "index_ivf = faiss.extract_index_ivf(index)  #\nindex_ivf.nprobe = 1\nindex.train(big_npy)\nfaiss.write_index(\n    index, \"tools/infer/trained_IVF%s_Flat_baseline_src_feat_v2.index\" % (n_ivf)\n)\nlogger.info(\"Adding...\")\nbatch_size_add = 8192\nfor i in range(0, big_npy.shape[0], batch_size_add):\n    index.add(big_npy[i : i + batch_size_add])",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "index_ivf.nprobe",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "index_ivf.nprobe = 1\nindex.train(big_npy)\nfaiss.write_index(\n    index, \"tools/infer/trained_IVF%s_Flat_baseline_src_feat_v2.index\" % (n_ivf)\n)\nlogger.info(\"Adding...\")\nbatch_size_add = 8192\nfor i in range(0, big_npy.shape[0], batch_size_add):\n    index.add(big_npy[i : i + batch_size_add])\nfaiss.write_index(",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "batch_size_add",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index-v2",
        "description": "Models.Sound.tools.infer.train-index-v2",
        "peekOfCode": "batch_size_add = 8192\nfor i in range(0, big_npy.shape[0], batch_size_add):\n    index.add(big_npy[i : i + batch_size_add])\nfaiss.write_index(\n    index, \"tools/infer/added_IVF%s_Flat_mi_baseline_src_feat.index\" % (n_ivf)\n)\n\"\"\"\n大小（都是FP32）\nbig_src_feature 2.95G\n    (3098036, 256)",
        "detail": "Models.Sound.tools.infer.train-index-v2",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index",
        "description": "Models.Sound.tools.infer.train-index",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport faiss\nimport numpy as np\n# ###########如果是原始特征要先写save\ninp_root = r\"E:\\codes\\py39\\dataset\\mi\\2-co256\"\nnpys = []\nfor name in sorted(list(os.listdir(inp_root))):\n    phone = np.load(\"%s/%s\" % (inp_root, name))\n    npys.append(phone)\nbig_npy = np.concatenate(npys, 0)",
        "detail": "Models.Sound.tools.infer.train-index",
        "documentation": {}
    },
    {
        "label": "inp_root",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index",
        "description": "Models.Sound.tools.infer.train-index",
        "peekOfCode": "inp_root = r\"E:\\codes\\py39\\dataset\\mi\\2-co256\"\nnpys = []\nfor name in sorted(list(os.listdir(inp_root))):\n    phone = np.load(\"%s/%s\" % (inp_root, name))\n    npys.append(phone)\nbig_npy = np.concatenate(npys, 0)\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G\nnp.save(\"infer/big_src_feature_mi.npy\", big_npy)\n##################train+add\n# big_npy=np.load(\"/bili-coeus/jupyter/jupyterhub-liujing04/vits_ch/inference_f0/big_src_feature_mi.npy\")",
        "detail": "Models.Sound.tools.infer.train-index",
        "documentation": {}
    },
    {
        "label": "npys",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index",
        "description": "Models.Sound.tools.infer.train-index",
        "peekOfCode": "npys = []\nfor name in sorted(list(os.listdir(inp_root))):\n    phone = np.load(\"%s/%s\" % (inp_root, name))\n    npys.append(phone)\nbig_npy = np.concatenate(npys, 0)\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G\nnp.save(\"infer/big_src_feature_mi.npy\", big_npy)\n##################train+add\n# big_npy=np.load(\"/bili-coeus/jupyter/jupyterhub-liujing04/vits_ch/inference_f0/big_src_feature_mi.npy\")\nlogger.debug(big_npy.shape)",
        "detail": "Models.Sound.tools.infer.train-index",
        "documentation": {}
    },
    {
        "label": "big_npy",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index",
        "description": "Models.Sound.tools.infer.train-index",
        "peekOfCode": "big_npy = np.concatenate(npys, 0)\nlogger.debug(big_npy.shape)  # (6196072, 192)#fp32#4.43G\nnp.save(\"infer/big_src_feature_mi.npy\", big_npy)\n##################train+add\n# big_npy=np.load(\"/bili-coeus/jupyter/jupyterhub-liujing04/vits_ch/inference_f0/big_src_feature_mi.npy\")\nlogger.debug(big_npy.shape)\nindex = faiss.index_factory(256, \"IVF512,Flat\")  # mi\nlogger.info(\"Training...\")\nindex_ivf = faiss.extract_index_ivf(index)  #\nindex_ivf.nprobe = 9",
        "detail": "Models.Sound.tools.infer.train-index",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index",
        "description": "Models.Sound.tools.infer.train-index",
        "peekOfCode": "index = faiss.index_factory(256, \"IVF512,Flat\")  # mi\nlogger.info(\"Training...\")\nindex_ivf = faiss.extract_index_ivf(index)  #\nindex_ivf.nprobe = 9\nindex.train(big_npy)\nfaiss.write_index(index, \"infer/trained_IVF512_Flat_mi_baseline_src_feat.index\")\nlogger.info(\"Adding...\")\nindex.add(big_npy)\nfaiss.write_index(index, \"infer/added_IVF512_Flat_mi_baseline_src_feat.index\")\n\"\"\"",
        "detail": "Models.Sound.tools.infer.train-index",
        "documentation": {}
    },
    {
        "label": "index_ivf",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index",
        "description": "Models.Sound.tools.infer.train-index",
        "peekOfCode": "index_ivf = faiss.extract_index_ivf(index)  #\nindex_ivf.nprobe = 9\nindex.train(big_npy)\nfaiss.write_index(index, \"infer/trained_IVF512_Flat_mi_baseline_src_feat.index\")\nlogger.info(\"Adding...\")\nindex.add(big_npy)\nfaiss.write_index(index, \"infer/added_IVF512_Flat_mi_baseline_src_feat.index\")\n\"\"\"\n大小（都是FP32）\nbig_src_feature 2.95G",
        "detail": "Models.Sound.tools.infer.train-index",
        "documentation": {}
    },
    {
        "label": "index_ivf.nprobe",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.train-index",
        "description": "Models.Sound.tools.infer.train-index",
        "peekOfCode": "index_ivf.nprobe = 9\nindex.train(big_npy)\nfaiss.write_index(index, \"infer/trained_IVF512_Flat_mi_baseline_src_feat.index\")\nlogger.info(\"Adding...\")\nindex.add(big_npy)\nfaiss.write_index(index, \"infer/added_IVF512_Flat_mi_baseline_src_feat.index\")\n\"\"\"\n大小（都是FP32）\nbig_src_feature 2.95G\n    (3098036, 256)",
        "detail": "Models.Sound.tools.infer.train-index",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer.trans_weights",
        "description": "Models.Sound.tools.infer.trans_weights",
        "peekOfCode": "a = torch.load(\n    r\"E:\\codes\\py39\\vits_vc_gpu_train\\logs\\ft-mi-no_opt-no_dropout\\G_1000.pth\"\n)[\n    \"model\"\n]  # sim_nsf#\nfor key in a.keys():\n    a[key] = a[key].half()\n# torch.save(a,\"ft-mi-freeze-vocoder_true_1k.pt\")#\n# torch.save(a,\"ft-mi-sim1k.pt\")#\ntorch.save(a, \"ft-mi-no_opt-no_dropout.pt\")  #",
        "detail": "Models.Sound.tools.infer.trans_weights",
        "documentation": {}
    },
    {
        "label": "TorchGate",
        "kind": 6,
        "importPath": "Models.Sound.tools.torchgate.torchgate",
        "description": "Models.Sound.tools.torchgate.torchgate",
        "peekOfCode": "class TorchGate(torch.nn.Module):\n    \"\"\"\n    A PyTorch module that applies a spectral gate to an input signal.\n    Arguments:\n        sr {int} -- Sample rate of the input signal.\n        nonstationary {bool} -- Whether to use non-stationary or stationary masking (default: {False}).\n        n_std_thresh_stationary {float} -- Number of standard deviations above mean to threshold noise for\n                                           stationary masking (default: {1.5}).\n        n_thresh_nonstationary {float} -- Number of multiplies above smoothed magnitude spectrogram. for\n                                        non-stationary masking (default: {1.3}).",
        "detail": "Models.Sound.tools.torchgate.torchgate",
        "documentation": {}
    },
    {
        "label": "amp_to_db",
        "kind": 2,
        "importPath": "Models.Sound.tools.torchgate.utils",
        "description": "Models.Sound.tools.torchgate.utils",
        "peekOfCode": "def amp_to_db(\n    x: torch.Tensor, eps=torch.finfo(torch.float64).eps, top_db=40\n) -> torch.Tensor:\n    \"\"\"\n    Convert the input tensor from amplitude to decibel scale.\n    Arguments:\n        x {[torch.Tensor]} -- [Input tensor.]\n    Keyword Arguments:\n        eps {[float]} -- [Small value to avoid numerical instability.]\n                          (default: {torch.finfo(torch.float64).eps})",
        "detail": "Models.Sound.tools.torchgate.utils",
        "documentation": {}
    },
    {
        "label": "temperature_sigmoid",
        "kind": 2,
        "importPath": "Models.Sound.tools.torchgate.utils",
        "description": "Models.Sound.tools.torchgate.utils",
        "peekOfCode": "def temperature_sigmoid(x: torch.Tensor, x0: float, temp_coeff: float) -> torch.Tensor:\n    \"\"\"\n    Apply a sigmoid function with temperature scaling.\n    Arguments:\n        x {[torch.Tensor]} -- [Input tensor.]\n        x0 {[float]} -- [Parameter that controls the threshold of the sigmoid.]\n        temp_coeff {[float]} -- [Parameter that controls the slope of the sigmoid.]\n    Returns:\n        [torch.Tensor] -- [Output tensor after applying the sigmoid with temperature scaling.]\n    \"\"\"",
        "detail": "Models.Sound.tools.torchgate.utils",
        "documentation": {}
    },
    {
        "label": "linspace",
        "kind": 2,
        "importPath": "Models.Sound.tools.torchgate.utils",
        "description": "Models.Sound.tools.torchgate.utils",
        "peekOfCode": "def linspace(\n    start: Number, stop: Number, num: int = 50, endpoint: bool = True, **kwargs\n) -> torch.Tensor:\n    \"\"\"\n    Generate a linearly spaced 1-D tensor.\n    Arguments:\n        start {[Number]} -- [The starting value of the sequence.]\n        stop {[Number]} -- [The end value of the sequence, unless `endpoint` is set to False.\n                            In that case, the sequence consists of all but the last of ``num + 1``\n                            evenly spaced samples, so that `stop` is excluded. Note that the step",
        "detail": "Models.Sound.tools.torchgate.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "logger = logging.getLogger(__name__)\ni18n = I18nAuto()\nlogger.info(i18n)\nload_dotenv()\nconfig = Config()\nvc = VC(config)\nweight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\nnames = []",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "i18n",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "i18n = I18nAuto()\nlogger.info(i18n)\nload_dotenv()\nconfig = Config()\nvc = VC(config)\nweight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\nnames = []\nhubert_model = None",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "config = Config()\nvc = VC(config)\nweight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\nnames = []\nhubert_model = None\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "vc",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "vc = VC(config)\nweight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\nnames = []\nhubert_model = None\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "weight_root",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "weight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\nnames = []\nhubert_model = None\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\nfor root, dirs, files in os.walk(index_root, topdown=False):",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "weight_uvr5_root",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "weight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\nnames = []\nhubert_model = None\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\nfor root, dirs, files in os.walk(index_root, topdown=False):\n    for name in files:",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "index_root",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "index_root = os.getenv(\"index_root\")\nnames = []\nhubert_model = None\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\nfor root, dirs, files in os.walk(index_root, topdown=False):\n    for name in files:\n        if name.endswith(\".index\") and \"trained\" not in name:",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "names",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "names = []\nhubert_model = None\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\nfor root, dirs, files in os.walk(index_root, topdown=False):\n    for name in files:\n        if name.endswith(\".index\") and \"trained\" not in name:\n            index_paths.append(\"%s/%s\" % (root, name))",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "hubert_model",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "hubert_model = None\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\nfor root, dirs, files in os.walk(index_root, topdown=False):\n    for name in files:\n        if name.endswith(\".index\") and \"trained\" not in name:\n            index_paths.append(\"%s/%s\" % (root, name))\napp = gr.Blocks()",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "index_paths",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "index_paths = []\nfor root, dirs, files in os.walk(index_root, topdown=False):\n    for name in files:\n        if name.endswith(\".index\") and \"trained\" not in name:\n            index_paths.append(\"%s/%s\" % (root, name))\napp = gr.Blocks()\nwith app:\n    with gr.Tabs():\n        with gr.TabItem(\"在线demo\"):\n            gr.Markdown(",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Models.Sound.tools.app",
        "description": "Models.Sound.tools.app",
        "peekOfCode": "app = gr.Blocks()\nwith app:\n    with gr.Tabs():\n        with gr.TabItem(\"在线demo\"):\n            gr.Markdown(\n                value=\"\"\"\n                RVC 在线demo\n                \"\"\"\n            )\n            sid = gr.Dropdown(label=i18n(\"推理音色\"), choices=sorted(names))",
        "detail": "Models.Sound.tools.app",
        "documentation": {}
    },
    {
        "label": "cal_cross_attn",
        "kind": 2,
        "importPath": "Models.Sound.tools.calc_rvc_model_similarity",
        "description": "Models.Sound.tools.calc_rvc_model_similarity",
        "peekOfCode": "def cal_cross_attn(to_q, to_k, to_v, rand_input):\n    hidden_dim, embed_dim = to_q.shape\n    attn_to_q = nn.Linear(hidden_dim, embed_dim, bias=False)\n    attn_to_k = nn.Linear(hidden_dim, embed_dim, bias=False)\n    attn_to_v = nn.Linear(hidden_dim, embed_dim, bias=False)\n    attn_to_q.load_state_dict({\"weight\": to_q})\n    attn_to_k.load_state_dict({\"weight\": to_k})\n    attn_to_v.load_state_dict({\"weight\": to_v})\n    return torch.einsum(\n        \"ik, jk -> ik\",",
        "detail": "Models.Sound.tools.calc_rvc_model_similarity",
        "documentation": {}
    },
    {
        "label": "model_hash",
        "kind": 2,
        "importPath": "Models.Sound.tools.calc_rvc_model_similarity",
        "description": "Models.Sound.tools.calc_rvc_model_similarity",
        "peekOfCode": "def model_hash(filename):\n    try:\n        with open(filename, \"rb\") as file:\n            import hashlib\n            m = hashlib.sha256()\n            file.seek(0x100000)\n            m.update(file.read(0x10000))\n            return m.hexdigest()[0:8]\n    except FileNotFoundError:\n        return \"NOFILE\"",
        "detail": "Models.Sound.tools.calc_rvc_model_similarity",
        "documentation": {}
    },
    {
        "label": "eval",
        "kind": 2,
        "importPath": "Models.Sound.tools.calc_rvc_model_similarity",
        "description": "Models.Sound.tools.calc_rvc_model_similarity",
        "peekOfCode": "def eval(model, n, input):\n    qk = f\"enc_p.encoder.attn_layers.{n}.conv_q.weight\"\n    uk = f\"enc_p.encoder.attn_layers.{n}.conv_k.weight\"\n    vk = f\"enc_p.encoder.attn_layers.{n}.conv_v.weight\"\n    atoq, atok, atov = model[qk][:, :, 0], model[uk][:, :, 0], model[vk][:, :, 0]\n    attn = cal_cross_attn(atoq, atok, atov, input)\n    return attn\ndef main(path, root):\n    torch.manual_seed(114514)\n    model_a = torch.load(path, map_location=\"cpu\")[\"weight\"]",
        "detail": "Models.Sound.tools.calc_rvc_model_similarity",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Models.Sound.tools.calc_rvc_model_similarity",
        "description": "Models.Sound.tools.calc_rvc_model_similarity",
        "peekOfCode": "def main(path, root):\n    torch.manual_seed(114514)\n    model_a = torch.load(path, map_location=\"cpu\")[\"weight\"]\n    logger.info(\"Query:\\t\\t%s\\t%s\" % (path, model_hash(path)))\n    map_attn_a = {}\n    map_rand_input = {}\n    for n in range(6):\n        hidden_dim, embed_dim, _ = model_a[\n            f\"enc_p.encoder.attn_layers.{n}.conv_v.weight\"\n        ].shape",
        "detail": "Models.Sound.tools.calc_rvc_model_similarity",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.tools.calc_rvc_model_similarity",
        "description": "Models.Sound.tools.calc_rvc_model_similarity",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndef cal_cross_attn(to_q, to_k, to_v, rand_input):\n    hidden_dim, embed_dim = to_q.shape\n    attn_to_q = nn.Linear(hidden_dim, embed_dim, bias=False)\n    attn_to_k = nn.Linear(hidden_dim, embed_dim, bias=False)\n    attn_to_v = nn.Linear(hidden_dim, embed_dim, bias=False)\n    attn_to_q.load_state_dict({\"weight\": to_q})",
        "detail": "Models.Sound.tools.calc_rvc_model_similarity",
        "documentation": {}
    },
    {
        "label": "dl_model",
        "kind": 2,
        "importPath": "Models.Sound.tools.download_models",
        "description": "Models.Sound.tools.download_models",
        "peekOfCode": "def dl_model(link, model_name, dir_name):\n    with requests.get(f\"{link}{model_name}\") as r:\n        r.raise_for_status()\n        os.makedirs(os.path.dirname(dir_name / model_name), exist_ok=True)\n        with open(dir_name / model_name, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\nif __name__ == \"__main__\":\n    print(\"Downloading hubert_base.pt...\")\n    dl_model(RVC_DOWNLOAD_LINK, \"hubert_base.pt\", BASE_DIR / \"assets/hubert\")",
        "detail": "Models.Sound.tools.download_models",
        "documentation": {}
    },
    {
        "label": "RVC_DOWNLOAD_LINK",
        "kind": 5,
        "importPath": "Models.Sound.tools.download_models",
        "description": "Models.Sound.tools.download_models",
        "peekOfCode": "RVC_DOWNLOAD_LINK = \"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/\"\nBASE_DIR = Path(__file__).resolve().parent.parent\ndef dl_model(link, model_name, dir_name):\n    with requests.get(f\"{link}{model_name}\") as r:\n        r.raise_for_status()\n        os.makedirs(os.path.dirname(dir_name / model_name), exist_ok=True)\n        with open(dir_name / model_name, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\nif __name__ == \"__main__\":",
        "detail": "Models.Sound.tools.download_models",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "Models.Sound.tools.download_models",
        "description": "Models.Sound.tools.download_models",
        "peekOfCode": "BASE_DIR = Path(__file__).resolve().parent.parent\ndef dl_model(link, model_name, dir_name):\n    with requests.get(f\"{link}{model_name}\") as r:\n        r.raise_for_status()\n        os.makedirs(os.path.dirname(dir_name / model_name), exist_ok=True)\n        with open(dir_name / model_name, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\nif __name__ == \"__main__\":\n    print(\"Downloading hubert_base.pt...\")",
        "detail": "Models.Sound.tools.download_models",
        "documentation": {}
    },
    {
        "label": "arg_parse",
        "kind": 2,
        "importPath": "Models.Sound.tools.infer_batch_rvc",
        "description": "Models.Sound.tools.infer_batch_rvc",
        "peekOfCode": "def arg_parse() -> tuple:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--f0up_key\", type=int, default=0)\n    parser.add_argument(\"--input_path\", type=str, help=\"input path\")\n    parser.add_argument(\"--index_path\", type=str, help=\"index path\")\n    parser.add_argument(\"--f0method\", type=str, default=\"harvest\", help=\"harvest or pm\")\n    parser.add_argument(\"--opt_path\", type=str, help=\"opt path\")\n    parser.add_argument(\"--model_name\", type=str, help=\"store in assets/weight_root\")\n    parser.add_argument(\"--index_rate\", type=float, default=0.66, help=\"index rate\")\n    parser.add_argument(\"--device\", type=str, help=\"device\")",
        "detail": "Models.Sound.tools.infer_batch_rvc",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Models.Sound.tools.infer_batch_rvc",
        "description": "Models.Sound.tools.infer_batch_rvc",
        "peekOfCode": "def main():\n    load_dotenv()\n    args = arg_parse()\n    config = Config()\n    config.device = args.device if args.device else config.device\n    config.is_half = args.is_half if args.is_half else config.is_half\n    vc = VC(config)\n    vc.get_vc(args.model_name)\n    audios = os.listdir(args.input_path)\n    for file in tq.tqdm(audios):",
        "detail": "Models.Sound.tools.infer_batch_rvc",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer_batch_rvc",
        "description": "Models.Sound.tools.infer_batch_rvc",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nimport sys\nimport tqdm as tq\nfrom dotenv import load_dotenv\nfrom scipy.io import wavfile\nfrom configs.config import Config\nfrom infer.modules.vc.modules import VC\ndef arg_parse() -> tuple:\n    parser = argparse.ArgumentParser()",
        "detail": "Models.Sound.tools.infer_batch_rvc",
        "documentation": {}
    },
    {
        "label": "arg_parse",
        "kind": 2,
        "importPath": "Models.Sound.tools.infer_cli",
        "description": "Models.Sound.tools.infer_cli",
        "peekOfCode": "def arg_parse() -> tuple:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--f0up_key\", type=int, default=0)\n    parser.add_argument(\"--input_path\", type=str, help=\"input path\")\n    parser.add_argument(\"--index_path\", type=str, help=\"index path\")\n    parser.add_argument(\"--f0method\", type=str, default=\"harvest\", help=\"harvest or pm\")\n    parser.add_argument(\"--opt_path\", type=str, help=\"opt path\")\n    parser.add_argument(\"--model_name\", type=str, help=\"store in assets/weight_root\")\n    parser.add_argument(\"--index_rate\", type=float, default=0.66, help=\"index rate\")\n    parser.add_argument(\"--device\", type=str, help=\"device\")",
        "detail": "Models.Sound.tools.infer_cli",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Models.Sound.tools.infer_cli",
        "description": "Models.Sound.tools.infer_cli",
        "peekOfCode": "def main():\n    load_dotenv()\n    args = arg_parse()\n    config = Config()\n    config.device = args.device if args.device else config.device\n    config.is_half = args.is_half if args.is_half else config.is_half\n    vc = VC(config)\n    vc.get_vc(args.model_name)\n    _, wav_opt = vc.vc_single(\n        0,",
        "detail": "Models.Sound.tools.infer_cli",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.tools.infer_cli",
        "description": "Models.Sound.tools.infer_cli",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nfrom dotenv import load_dotenv\nfrom scipy.io import wavfile\nfrom configs.config import Config\nfrom infer.modules.vc.modules import VC\n####\n# USAGE\n#\n# In your Terminal or CMD or whatever",
        "detail": "Models.Sound.tools.infer_cli",
        "documentation": {}
    },
    {
        "label": "hop_size",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "hop_size = 512\nsampling_rate = 40000  # 采样率\nf0_up_key = 0  # 升降调\nsid = 0  # 角色ID\nf0_method = \"dio\"  # F0提取算法\nmodel_path = \"ShirohaRVC.onnx\"  # 模型的完整路径\nvec_name = (\n    \"vec-256-layer-9\"  # 内部自动补齐为 f\"pretrained/{vec_name}.onnx\" 需要onnx的vec模型\n)\nwav_path = \"123.wav\"  # 输入路径或ByteIO实例",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "sampling_rate",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "sampling_rate = 40000  # 采样率\nf0_up_key = 0  # 升降调\nsid = 0  # 角色ID\nf0_method = \"dio\"  # F0提取算法\nmodel_path = \"ShirohaRVC.onnx\"  # 模型的完整路径\nvec_name = (\n    \"vec-256-layer-9\"  # 内部自动补齐为 f\"pretrained/{vec_name}.onnx\" 需要onnx的vec模型\n)\nwav_path = \"123.wav\"  # 输入路径或ByteIO实例\nout_path = \"out.wav\"  # 输出路径或ByteIO实例",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "f0_up_key",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "f0_up_key = 0  # 升降调\nsid = 0  # 角色ID\nf0_method = \"dio\"  # F0提取算法\nmodel_path = \"ShirohaRVC.onnx\"  # 模型的完整路径\nvec_name = (\n    \"vec-256-layer-9\"  # 内部自动补齐为 f\"pretrained/{vec_name}.onnx\" 需要onnx的vec模型\n)\nwav_path = \"123.wav\"  # 输入路径或ByteIO实例\nout_path = \"out.wav\"  # 输出路径或ByteIO实例\nmodel = OnnxRVC(",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "sid",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "sid = 0  # 角色ID\nf0_method = \"dio\"  # F0提取算法\nmodel_path = \"ShirohaRVC.onnx\"  # 模型的完整路径\nvec_name = (\n    \"vec-256-layer-9\"  # 内部自动补齐为 f\"pretrained/{vec_name}.onnx\" 需要onnx的vec模型\n)\nwav_path = \"123.wav\"  # 输入路径或ByteIO实例\nout_path = \"out.wav\"  # 输出路径或ByteIO实例\nmodel = OnnxRVC(\n    model_path, vec_path=vec_name, sr=sampling_rate, hop_size=hop_size, device=\"cuda\"",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "f0_method",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "f0_method = \"dio\"  # F0提取算法\nmodel_path = \"ShirohaRVC.onnx\"  # 模型的完整路径\nvec_name = (\n    \"vec-256-layer-9\"  # 内部自动补齐为 f\"pretrained/{vec_name}.onnx\" 需要onnx的vec模型\n)\nwav_path = \"123.wav\"  # 输入路径或ByteIO实例\nout_path = \"out.wav\"  # 输出路径或ByteIO实例\nmodel = OnnxRVC(\n    model_path, vec_path=vec_name, sr=sampling_rate, hop_size=hop_size, device=\"cuda\"\n)",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "model_path = \"ShirohaRVC.onnx\"  # 模型的完整路径\nvec_name = (\n    \"vec-256-layer-9\"  # 内部自动补齐为 f\"pretrained/{vec_name}.onnx\" 需要onnx的vec模型\n)\nwav_path = \"123.wav\"  # 输入路径或ByteIO实例\nout_path = \"out.wav\"  # 输出路径或ByteIO实例\nmodel = OnnxRVC(\n    model_path, vec_path=vec_name, sr=sampling_rate, hop_size=hop_size, device=\"cuda\"\n)\naudio = model.inference(wav_path, sid, f0_method=f0_method, f0_up_key=f0_up_key)",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "vec_name",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "vec_name = (\n    \"vec-256-layer-9\"  # 内部自动补齐为 f\"pretrained/{vec_name}.onnx\" 需要onnx的vec模型\n)\nwav_path = \"123.wav\"  # 输入路径或ByteIO实例\nout_path = \"out.wav\"  # 输出路径或ByteIO实例\nmodel = OnnxRVC(\n    model_path, vec_path=vec_name, sr=sampling_rate, hop_size=hop_size, device=\"cuda\"\n)\naudio = model.inference(wav_path, sid, f0_method=f0_method, f0_up_key=f0_up_key)\nsoundfile.write(out_path, audio, sampling_rate)",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "wav_path",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "wav_path = \"123.wav\"  # 输入路径或ByteIO实例\nout_path = \"out.wav\"  # 输出路径或ByteIO实例\nmodel = OnnxRVC(\n    model_path, vec_path=vec_name, sr=sampling_rate, hop_size=hop_size, device=\"cuda\"\n)\naudio = model.inference(wav_path, sid, f0_method=f0_method, f0_up_key=f0_up_key)\nsoundfile.write(out_path, audio, sampling_rate)",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "out_path",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "out_path = \"out.wav\"  # 输出路径或ByteIO实例\nmodel = OnnxRVC(\n    model_path, vec_path=vec_name, sr=sampling_rate, hop_size=hop_size, device=\"cuda\"\n)\naudio = model.inference(wav_path, sid, f0_method=f0_method, f0_up_key=f0_up_key)\nsoundfile.write(out_path, audio, sampling_rate)",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "model = OnnxRVC(\n    model_path, vec_path=vec_name, sr=sampling_rate, hop_size=hop_size, device=\"cuda\"\n)\naudio = model.inference(wav_path, sid, f0_method=f0_method, f0_up_key=f0_up_key)\nsoundfile.write(out_path, audio, sampling_rate)",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "audio",
        "kind": 5,
        "importPath": "Models.Sound.tools.onnx_inference_demo",
        "description": "Models.Sound.tools.onnx_inference_demo",
        "peekOfCode": "audio = model.inference(wav_path, sid, f0_method=f0_method, f0_up_key=f0_up_key)\nsoundfile.write(out_path, audio, sampling_rate)",
        "detail": "Models.Sound.tools.onnx_inference_demo",
        "documentation": {}
    },
    {
        "label": "RVC",
        "kind": 6,
        "importPath": "Models.Sound.tools.rvc_for_realtime",
        "description": "Models.Sound.tools.rvc_for_realtime",
        "peekOfCode": "class RVC:\n    def __init__(\n        self,\n        key,\n        pth_path,\n        index_path,\n        index_rate,\n        n_cpu,\n        inp_q,\n        opt_q,",
        "detail": "Models.Sound.tools.rvc_for_realtime",
        "documentation": {}
    },
    {
        "label": "printt",
        "kind": 2,
        "importPath": "Models.Sound.tools.rvc_for_realtime",
        "description": "Models.Sound.tools.rvc_for_realtime",
        "peekOfCode": "def printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:\n        print(strr % args)\n# config.device=torch.device(\"cpu\")########强制cpu测试\n# config.is_half=False########强制cpu测试\nclass RVC:\n    def __init__(\n        self,",
        "detail": "Models.Sound.tools.rvc_for_realtime",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.tools.rvc_for_realtime",
        "description": "Models.Sound.tools.rvc_for_realtime",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nfrom multiprocessing import Manager as M\nfrom configs.config import Config\n# config = Config()\nmm = M()\ndef printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:",
        "detail": "Models.Sound.tools.rvc_for_realtime",
        "documentation": {}
    },
    {
        "label": "mm",
        "kind": 5,
        "importPath": "Models.Sound.tools.rvc_for_realtime",
        "description": "Models.Sound.tools.rvc_for_realtime",
        "peekOfCode": "mm = M()\ndef printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:\n        print(strr % args)\n# config.device=torch.device(\"cpu\")########强制cpu测试\n# config.is_half=False########强制cpu测试\nclass RVC:\n    def __init__(",
        "detail": "Models.Sound.tools.rvc_for_realtime",
        "documentation": {}
    },
    {
        "label": "GUIConfig",
        "kind": 6,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "class GUIConfig:\n    def __init__(self) -> None:\n        self.pth_path: str = \"\"\n        self.index_path: str = \"\"\n        self.pitch: int = 0\n        self.samplerate: int = 40000\n        self.block_time: float = 1.0  # s\n        self.buffer_num: int = 1\n        self.threhold: int = -60\n        self.crossfade_time: float = 0.05",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "ConfigData",
        "kind": 6,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "class ConfigData(BaseModel):\n    pth_path: str\n    index_path: str\n    sg_input_device: str\n    sg_output_device: str\n    threhold: int = -60\n    pitch: int = 0\n    index_rate: float = 0.3\n    rms_mix_rate: float = 0.0\n    block_time: float = 0.25",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "AudioAPI",
        "kind": 6,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "class AudioAPI:\n    def __init__(self) -> None:\n        self.gui_config = GUIConfig()\n        self.config = None  # Initialize Config object as None\n        self.flag_vc = False\n        self.function = \"vc\"\n        self.delay_time = 0\n        self.rvc = None  # Initialize RVC object as None\n    def load(self):\n        input_devices, output_devices, _, _ = self.get_devices()",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "get_input_devices",
        "kind": 2,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "def get_input_devices():\n    try:\n        input_devices, _, _, _ = audio_api.get_devices()\n        return input_devices\n    except Exception as e:\n        logger.error(f\"Failed to get input devices: {e}\")\n        raise HTTPException(status_code=500, detail=\"Failed to get input devices\")\n@app.get(\"/outputDevices\", response_model=list)\ndef get_output_devices():\n    try:",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "get_output_devices",
        "kind": 2,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "def get_output_devices():\n    try:\n        _, output_devices, _, _ = audio_api.get_devices()\n        return output_devices\n    except Exception as e:\n        logger.error(f\"Failed to get output devices: {e}\")\n        raise HTTPException(status_code=500, detail=\"Failed to get output devices\")\n@app.post(\"/config\")\ndef configure_audio(config_data: ConfigData):\n    try:",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "configure_audio",
        "kind": 2,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "def configure_audio(config_data: ConfigData):\n    try:\n        logger.info(f\"Configuring audio with data: {config_data}\")\n        if audio_api.set_values(config_data):\n            settings = config_data.dict()\n            settings[\"use_jit\"] = False\n            settings[\"f0method\"] = \"rmvpe\"\n            with open(\"configs/config.json\", \"w\", encoding='utf-8') as j:\n                json.dump(settings, j, ensure_ascii=False)\n            logger.info(\"Configuration set successfully\")",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "start_conversion",
        "kind": 2,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "def start_conversion():\n    try:\n        if not audio_api.flag_vc:\n            audio_api.start_vc()\n            return {\"message\": \"Audio conversion started\"}\n        else:\n            logger.warning(\"Audio conversion already running\")\n            raise HTTPException(status_code=400, detail=\"Audio conversion already running\")\n    except HTTPException as e:\n        logger.error(f\"Start conversion error: {e.detail}\")",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "stop_conversion",
        "kind": 2,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "def stop_conversion():\n    try:\n        if audio_api.flag_vc:\n            audio_api.flag_vc = False\n            global stream_latency\n            stream_latency = -1\n            return {\"message\": \"Audio conversion stopped\"}\n        else:\n            logger.warning(\"Audio conversion not running\")\n            raise HTTPException(status_code=400, detail=\"Audio conversion not running\")",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# Define FastAPI app\napp = FastAPI()\nclass GUIConfig:\n    def __init__(self) -> None:\n        self.pth_path: str = \"\"\n        self.index_path: str = \"\"\n        self.pitch: int = 0\n        self.samplerate: int = 40000\n        self.block_time: float = 1.0  # s",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "app = FastAPI()\nclass GUIConfig:\n    def __init__(self) -> None:\n        self.pth_path: str = \"\"\n        self.index_path: str = \"\"\n        self.pitch: int = 0\n        self.samplerate: int = 40000\n        self.block_time: float = 1.0  # s\n        self.buffer_num: int = 1\n        self.threhold: int = -60",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "audio_api",
        "kind": 5,
        "importPath": "Models.Sound.api_231006",
        "description": "Models.Sound.api_231006",
        "peekOfCode": "audio_api = AudioAPI()\n@app.get(\"/inputDevices\", response_model=list)\ndef get_input_devices():\n    try:\n        input_devices, _, _, _ = audio_api.get_devices()\n        return input_devices\n    except Exception as e:\n        logger.error(f\"Failed to get input devices: {e}\")\n        raise HTTPException(status_code=500, detail=\"Failed to get input devices\")\n@app.get(\"/outputDevices\", response_model=list)",
        "detail": "Models.Sound.api_231006",
        "documentation": {}
    },
    {
        "label": "GUIConfig",
        "kind": 6,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "class GUIConfig:\n    def __init__(self) -> None:\n        self.pth_path: str = \"\"\n        self.index_path: str = \"\"\n        self.pitch: int = 0\n        self.formant: float = 0.0\n        self.sr_type: str = \"sr_model\"\n        self.block_time: float = 0.25  # s\n        self.threhold: int = -60\n        self.crossfade_time: float = 0.05",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "ConfigData",
        "kind": 6,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "class ConfigData(BaseModel):\n    pth_path: str\n    index_path: str\n    sg_input_device: str\n    sg_output_device: str\n    threhold: int = -60\n    pitch: int = 0\n    formant: float = 0.0\n    index_rate: float = 0.3\n    rms_mix_rate: float = 0.0",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "Harvest",
        "kind": 6,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "class Harvest(Process):\n    def __init__(self, inp_q, opt_q):\n        super(Harvest, self).__init__()\n        self.inp_q = inp_q\n        self.opt_q = opt_q\n    def run(self):\n        import numpy as np\n        import pyworld\n        while True:\n            idx, x, res_f0, n_cpu, ts = self.inp_q.get()",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "AudioAPI",
        "kind": 6,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "class AudioAPI:\n    def __init__(self) -> None:\n        self.gui_config = GUIConfig()\n        self.config = None  # Initialize Config object as None\n        self.flag_vc = False\n        self.function = \"vc\"\n        self.delay_time = 0\n        self.rvc = None  # Initialize RVC object as None\n        self.inp_q = None\n        self.opt_q = None",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "get_input_devices",
        "kind": 2,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "def get_input_devices():\n    try:\n        input_devices, _, _, _ = audio_api.get_devices()\n        return input_devices\n    except Exception as e:\n        logger.error(f\"Failed to get input devices: {e}\")\n        raise HTTPException(status_code=500, detail=\"Failed to get input devices\")\n@app.get(\"/outputDevices\", response_model=list)\ndef get_output_devices():\n    try:",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "get_output_devices",
        "kind": 2,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "def get_output_devices():\n    try:\n        _, output_devices, _, _ = audio_api.get_devices()\n        return output_devices\n    except Exception as e:\n        logger.error(f\"Failed to get output devices: {e}\")\n        raise HTTPException(status_code=500, detail=\"Failed to get output devices\")\n@app.post(\"/config\")\ndef configure_audio(config_data: ConfigData):\n    try:",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "configure_audio",
        "kind": 2,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "def configure_audio(config_data: ConfigData):\n    try:\n        logger.info(f\"Configuring audio with data: {config_data}\")\n        if audio_api.set_values(config_data):\n            settings = config_data.dict()\n            settings[\"use_jit\"] = False\n            with open(\"configs/config.json\", \"w\", encoding='utf-8') as j:\n                json.dump(settings, j, ensure_ascii=False)\n            logger.info(\"Configuration set successfully\")\n            return {\"message\": \"Configuration set successfully\"}",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "start_conversion",
        "kind": 2,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "def start_conversion():\n    try:\n        if not audio_api.flag_vc:\n            audio_api.start_vc()\n            return {\"message\": \"Audio conversion started\"}\n        else:\n            logger.warning(\"Audio conversion already running\")\n            raise HTTPException(status_code=400, detail=\"Audio conversion already running\")\n    except HTTPException as e:\n        logger.error(f\"Start conversion error: {e.detail}\")",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "stop_conversion",
        "kind": 2,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "def stop_conversion():\n    try:\n        if audio_api.flag_vc:\n            audio_api.flag_vc = False\n            global stream_latency\n            stream_latency = -1\n            return {\"message\": \"Audio conversion stopped\"}\n        else:\n            logger.warning(\"Audio conversion not running\")\n            raise HTTPException(status_code=400, detail=\"Audio conversion not running\")",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# Define FastAPI app\napp = FastAPI()\nclass GUIConfig:\n    def __init__(self) -> None:\n        self.pth_path: str = \"\"\n        self.index_path: str = \"\"\n        self.pitch: int = 0\n        self.formant: float = 0.0\n        self.sr_type: str = \"sr_model\"",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "app = FastAPI()\nclass GUIConfig:\n    def __init__(self) -> None:\n        self.pth_path: str = \"\"\n        self.index_path: str = \"\"\n        self.pitch: int = 0\n        self.formant: float = 0.0\n        self.sr_type: str = \"sr_model\"\n        self.block_time: float = 0.25  # s\n        self.threhold: int = -60",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "audio_api",
        "kind": 5,
        "importPath": "Models.Sound.api_240604",
        "description": "Models.Sound.api_240604",
        "peekOfCode": "audio_api = AudioAPI()\n@app.get(\"/inputDevices\", response_model=list)\ndef get_input_devices():\n    try:\n        input_devices, _, _, _ = audio_api.get_devices()\n        return input_devices\n    except Exception as e:\n        logger.error(f\"Failed to get input devices: {e}\")\n        raise HTTPException(status_code=500, detail=\"Failed to get input devices\")\n@app.get(\"/outputDevices\", response_model=list)",
        "detail": "Models.Sound.api_240604",
        "documentation": {}
    },
    {
        "label": "Harvest",
        "kind": 6,
        "importPath": "Models.Sound.gui_v1",
        "description": "Models.Sound.gui_v1",
        "peekOfCode": "class Harvest(multiprocessing.Process):\n    def __init__(self, inp_q, opt_q):\n        multiprocessing.Process.__init__(self)\n        self.inp_q = inp_q\n        self.opt_q = opt_q\n    def run(self):\n        import numpy as np\n        import pyworld\n        while 1:\n            idx, x, res_f0, n_cpu, ts = self.inp_q.get()",
        "detail": "Models.Sound.gui_v1",
        "documentation": {}
    },
    {
        "label": "printt",
        "kind": 2,
        "importPath": "Models.Sound.gui_v1",
        "description": "Models.Sound.gui_v1",
        "peekOfCode": "def printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:\n        print(strr % args)\ndef phase_vocoder(a, b, fade_out, fade_in):\n    window = torch.sqrt(fade_out * fade_in)\n    fa = torch.fft.rfft(a * window)\n    fb = torch.fft.rfft(b * window)\n    absab = torch.abs(fa) + torch.abs(fb)",
        "detail": "Models.Sound.gui_v1",
        "documentation": {}
    },
    {
        "label": "phase_vocoder",
        "kind": 2,
        "importPath": "Models.Sound.gui_v1",
        "description": "Models.Sound.gui_v1",
        "peekOfCode": "def phase_vocoder(a, b, fade_out, fade_in):\n    window = torch.sqrt(fade_out * fade_in)\n    fa = torch.fft.rfft(a * window)\n    fb = torch.fft.rfft(b * window)\n    absab = torch.abs(fa) + torch.abs(fb)\n    n = a.shape[0]\n    if n % 2 == 0:\n        absab[1:-1] *= 2\n    else:\n        absab[1:] *= 2",
        "detail": "Models.Sound.gui_v1",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OMP_NUM_THREADS\"]",
        "kind": 5,
        "importPath": "Models.Sound.gui_v1",
        "description": "Models.Sound.gui_v1",
        "peekOfCode": "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\nimport multiprocessing\nflag_vc = False\ndef printt(strr, *args):\n    if len(args) == 0:\n        print(strr)",
        "detail": "Models.Sound.gui_v1",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.gui_v1",
        "description": "Models.Sound.gui_v1",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nimport multiprocessing\nflag_vc = False\ndef printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:\n        print(strr % args)\ndef phase_vocoder(a, b, fade_out, fade_in):",
        "detail": "Models.Sound.gui_v1",
        "documentation": {}
    },
    {
        "label": "flag_vc",
        "kind": 5,
        "importPath": "Models.Sound.gui_v1",
        "description": "Models.Sound.gui_v1",
        "peekOfCode": "flag_vc = False\ndef printt(strr, *args):\n    if len(args) == 0:\n        print(strr)\n    else:\n        print(strr % args)\ndef phase_vocoder(a, b, fade_out, fade_in):\n    window = torch.sqrt(fade_out * fade_in)\n    fa = torch.fft.rfft(a * window)\n    fb = torch.fft.rfft(b * window)",
        "detail": "Models.Sound.gui_v1",
        "documentation": {}
    },
    {
        "label": "ToolButton",
        "kind": 6,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "class ToolButton(gr.Button, gr.components.FormComponent):\n    \"\"\"Small button with single emoji as text, fits inside gradio forms\"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(variant=\"tool\", **kwargs)\n    def get_block_name(self):\n        return \"button\"\nweight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\noutside_index_root = os.getenv(\"outside_index_root\")",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "lookup_indices",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def lookup_indices(index_root):\n    global index_paths\n    for root, dirs, files in os.walk(index_root, topdown=False):\n        for name in files:\n            if name.endswith(\".index\") and \"trained\" not in name:\n                index_paths.append(\"%s/%s\" % (root, name))\nlookup_indices(index_root)\nlookup_indices(outside_index_root)\nuvr5_names = []\nfor name in os.listdir(weight_uvr5_root):",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "change_choices",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def change_choices():\n    names = []\n    for name in os.listdir(weight_root):\n        if name.endswith(\".pth\"):\n            names.append(name)\n    index_paths = []\n    for root, dirs, files in os.walk(index_root, topdown=False):\n        for name in files:\n            if name.endswith(\".index\") and \"trained\" not in name:\n                index_paths.append(\"%s/%s\" % (root, name))",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "clean",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def clean():\n    return {\"value\": \"\", \"__type__\": \"update\"}\ndef export_onnx(ModelPath, ExportedPath):\n    from infer.modules.onnx.export import export_onnx as eo\n    eo(ModelPath, ExportedPath)\nsr_dict = {\n    \"32k\": 32000,\n    \"40k\": 40000,\n    \"48k\": 48000,\n}",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "export_onnx",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def export_onnx(ModelPath, ExportedPath):\n    from infer.modules.onnx.export import export_onnx as eo\n    eo(ModelPath, ExportedPath)\nsr_dict = {\n    \"32k\": 32000,\n    \"40k\": 40000,\n    \"48k\": 48000,\n}\ndef if_done(done, p):\n    while 1:",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "if_done",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def if_done(done, p):\n    while 1:\n        if p.poll() is None:\n            sleep(0.5)\n        else:\n            break\n    done[0] = True\ndef if_done_multi(done, ps):\n    while 1:\n        # poll==None代表进程未结束",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "if_done_multi",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def if_done_multi(done, ps):\n    while 1:\n        # poll==None代表进程未结束\n        # 只要有一个进程未结束都不停\n        flag = 1\n        for p in ps:\n            if p.poll() is None:\n                flag = 0\n                sleep(0.5)\n                break",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "preprocess_dataset",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def preprocess_dataset(trainset_dir, exp_dir, sr, n_p):\n    sr = sr_dict[sr]\n    os.makedirs(\"%s/logs/%s\" % (now_dir, exp_dir), exist_ok=True)\n    f = open(\"%s/logs/%s/preprocess.log\" % (now_dir, exp_dir), \"w\")\n    f.close()\n    cmd = '\"%s\" infer/modules/train/preprocess.py \"%s\" %s %s \"%s/logs/%s\" %s %.1f' % (\n        config.python_cmd,\n        trainset_dir,\n        sr,\n        n_p,",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "extract_f0_feature",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def extract_f0_feature(gpus, n_p, f0method, if_f0, exp_dir, version19, gpus_rmvpe):\n    gpus = gpus.split(\"-\")\n    os.makedirs(\"%s/logs/%s\" % (now_dir, exp_dir), exist_ok=True)\n    f = open(\"%s/logs/%s/extract_f0_feature.log\" % (now_dir, exp_dir), \"w\")\n    f.close()\n    if if_f0:\n        if f0method != \"rmvpe_gpu\":\n            cmd = (\n                '\"%s\" infer/modules/train/extract/extract_f0_print.py \"%s/logs/%s\" %s %s'\n                % (",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "get_pretrained_models",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def get_pretrained_models(path_str, f0_str, sr2):\n    if_pretrained_generator_exist = os.access(\n        \"assets/pretrained%s/%sG%s.pth\" % (path_str, f0_str, sr2), os.F_OK\n    )\n    if_pretrained_discriminator_exist = os.access(\n        \"assets/pretrained%s/%sD%s.pth\" % (path_str, f0_str, sr2), os.F_OK\n    )\n    if not if_pretrained_generator_exist:\n        logger.warning(\n            \"assets/pretrained%s/%sG%s.pth not exist, will not use pretrained model\",",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "change_sr2",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def change_sr2(sr2, if_f0_3, version19):\n    path_str = \"\" if version19 == \"v1\" else \"_v2\"\n    f0_str = \"f0\" if if_f0_3 else \"\"\n    return get_pretrained_models(path_str, f0_str, sr2)\ndef change_version19(sr2, if_f0_3, version19):\n    path_str = \"\" if version19 == \"v1\" else \"_v2\"\n    if sr2 == \"32k\" and version19 == \"v1\":\n        sr2 = \"40k\"\n    to_return_sr2 = (\n        {\"choices\": [\"40k\", \"48k\"], \"__type__\": \"update\", \"value\": sr2}",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "change_version19",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def change_version19(sr2, if_f0_3, version19):\n    path_str = \"\" if version19 == \"v1\" else \"_v2\"\n    if sr2 == \"32k\" and version19 == \"v1\":\n        sr2 = \"40k\"\n    to_return_sr2 = (\n        {\"choices\": [\"40k\", \"48k\"], \"__type__\": \"update\", \"value\": sr2}\n        if version19 == \"v1\"\n        else {\"choices\": [\"40k\", \"48k\", \"32k\"], \"__type__\": \"update\", \"value\": sr2}\n    )\n    f0_str = \"f0\" if if_f0_3 else \"\"",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "change_f0",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def change_f0(if_f0_3, sr2, version19):  # f0method8,pretrained_G14,pretrained_D15\n    path_str = \"\" if version19 == \"v1\" else \"_v2\"\n    return (\n        {\"visible\": if_f0_3, \"__type__\": \"update\"},\n        {\"visible\": if_f0_3, \"__type__\": \"update\"},\n        *get_pretrained_models(path_str, \"f0\" if if_f0_3 == True else \"\", sr2),\n    )\n# but3.click(click_train,[exp_dir1,sr2,if_f0_3,save_epoch10,total_epoch11,batch_size12,if_save_latest13,pretrained_G14,pretrained_D15,gpus16])\ndef click_train(\n    exp_dir1,",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "click_train",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def click_train(\n    exp_dir1,\n    sr2,\n    if_f0_3,\n    spk_id5,\n    save_epoch10,\n    total_epoch11,\n    batch_size12,\n    if_save_latest13,\n    pretrained_G14,",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "train_index",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def train_index(exp_dir1, version19):\n    # exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)\n    exp_dir = \"logs/%s\" % (exp_dir1)\n    os.makedirs(exp_dir, exist_ok=True)\n    feature_dir = (\n        \"%s/3_feature256\" % (exp_dir)\n        if version19 == \"v1\"\n        else \"%s/3_feature768\" % (exp_dir)\n    )\n    if not os.path.exists(feature_dir):",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "train1key",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def train1key(\n    exp_dir1,\n    sr2,\n    if_f0_3,\n    trainset_dir4,\n    spk_id5,\n    np7,\n    f0method8,\n    save_epoch10,\n    total_epoch11,",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "change_info_",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def change_info_(ckpt_path):\n    if not os.path.exists(ckpt_path.replace(os.path.basename(ckpt_path), \"train.log\")):\n        return {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\n    try:\n        with open(\n            ckpt_path.replace(os.path.basename(ckpt_path), \"train.log\"), \"r\"\n        ) as f:\n            info = eval(f.read().strip(\"\\n\").split(\"\\n\")[0].split(\"\\t\")[-1])\n            sr, f0 = info[\"sample_rate\"], info[\"if_f0\"]\n            version = \"v2\" if (\"version\" in info and info[\"version\"] == \"v2\") else \"v1\"",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "change_f0_method",
        "kind": 2,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "def change_f0_method(f0method8):\n    if f0method8 == \"rmvpe_gpu\":\n        visible = F0GPUVisible\n    else:\n        visible = False\n    return {\"visible\": visible, \"__type__\": \"update\"}\nwith gr.Blocks(title=\"RVC WebUI\") as app:\n    gr.Markdown(\"## RVC WebUI\")\n    gr.Markdown(\n        value=i18n(",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "now_dir",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "now_dir = os.getcwd()\nsys.path.append(now_dir)\nload_dotenv()\nfrom infer.modules.vc.modules import VC\nfrom infer.modules.uvr5.modules import uvr\nfrom infer.lib.train.process_ckpt import (\n    change_info,\n    extract_small_model,\n    merge,\n    show_info,",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "logger = logging.getLogger(__name__)\ntmp = os.path.join(now_dir, \"TEMP\")\nshutil.rmtree(tmp, ignore_errors=True)\nshutil.rmtree(\"%s/runtime/Lib/site-packages/infer_pack\" % (now_dir), ignore_errors=True)\nshutil.rmtree(\"%s/runtime/Lib/site-packages/uvr5_pack\" % (now_dir), ignore_errors=True)\nos.makedirs(tmp, exist_ok=True)\nos.makedirs(os.path.join(now_dir, \"logs\"), exist_ok=True)\nos.makedirs(os.path.join(now_dir, \"assets/weights\"), exist_ok=True)\nos.environ[\"TEMP\"] = tmp\nwarnings.filterwarnings(\"ignore\")",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "tmp",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "tmp = os.path.join(now_dir, \"TEMP\")\nshutil.rmtree(tmp, ignore_errors=True)\nshutil.rmtree(\"%s/runtime/Lib/site-packages/infer_pack\" % (now_dir), ignore_errors=True)\nshutil.rmtree(\"%s/runtime/Lib/site-packages/uvr5_pack\" % (now_dir), ignore_errors=True)\nos.makedirs(tmp, exist_ok=True)\nos.makedirs(os.path.join(now_dir, \"logs\"), exist_ok=True)\nos.makedirs(os.path.join(now_dir, \"assets/weights\"), exist_ok=True)\nos.environ[\"TEMP\"] = tmp\nwarnings.filterwarnings(\"ignore\")\ntorch.manual_seed(114514)",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TEMP\"]",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "os.environ[\"TEMP\"] = tmp\nwarnings.filterwarnings(\"ignore\")\ntorch.manual_seed(114514)\nconfig = Config()\nvc = VC(config)\nif config.dml == True:\n    def forward_dml(ctx, x, scale):\n        ctx.scale = scale\n        res = x.clone().detach()\n        return res",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "config = Config()\nvc = VC(config)\nif config.dml == True:\n    def forward_dml(ctx, x, scale):\n        ctx.scale = scale\n        res = x.clone().detach()\n        return res\n    fairseq.modules.grad_multiply.GradMultiply.forward = forward_dml\ni18n = I18nAuto()\nlogger.info(i18n)",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "vc",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "vc = VC(config)\nif config.dml == True:\n    def forward_dml(ctx, x, scale):\n        ctx.scale = scale\n        res = x.clone().detach()\n        return res\n    fairseq.modules.grad_multiply.GradMultiply.forward = forward_dml\ni18n = I18nAuto()\nlogger.info(i18n)\n# 判断是否有能用来训练和加速推理的N卡",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "i18n",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "i18n = I18nAuto()\nlogger.info(i18n)\n# 判断是否有能用来训练和加速推理的N卡\nngpu = torch.cuda.device_count()\ngpu_infos = []\nmem = []\nif_gpu_ok = False\nif torch.cuda.is_available() or ngpu != 0:\n    for i in range(ngpu):\n        gpu_name = torch.cuda.get_device_name(i)",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "ngpu",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "ngpu = torch.cuda.device_count()\ngpu_infos = []\nmem = []\nif_gpu_ok = False\nif torch.cuda.is_available() or ngpu != 0:\n    for i in range(ngpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        if any(\n            value in gpu_name.upper()\n            for value in [",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "gpu_infos",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "gpu_infos = []\nmem = []\nif_gpu_ok = False\nif torch.cuda.is_available() or ngpu != 0:\n    for i in range(ngpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        if any(\n            value in gpu_name.upper()\n            for value in [\n                \"10\",",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "mem",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "mem = []\nif_gpu_ok = False\nif torch.cuda.is_available() or ngpu != 0:\n    for i in range(ngpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        if any(\n            value in gpu_name.upper()\n            for value in [\n                \"10\",\n                \"16\",",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "if_gpu_ok",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "if_gpu_ok = False\nif torch.cuda.is_available() or ngpu != 0:\n    for i in range(ngpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        if any(\n            value in gpu_name.upper()\n            for value in [\n                \"10\",\n                \"16\",\n                \"20\",",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "gpus",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "gpus = \"-\".join([i[0] for i in gpu_infos])\nclass ToolButton(gr.Button, gr.components.FormComponent):\n    \"\"\"Small button with single emoji as text, fits inside gradio forms\"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(variant=\"tool\", **kwargs)\n    def get_block_name(self):\n        return \"button\"\nweight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "weight_root",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "weight_root = os.getenv(\"weight_root\")\nweight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\noutside_index_root = os.getenv(\"outside_index_root\")\nnames = []\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\ndef lookup_indices(index_root):",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "weight_uvr5_root",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "weight_uvr5_root = os.getenv(\"weight_uvr5_root\")\nindex_root = os.getenv(\"index_root\")\noutside_index_root = os.getenv(\"outside_index_root\")\nnames = []\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\ndef lookup_indices(index_root):\n    global index_paths",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "index_root",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "index_root = os.getenv(\"index_root\")\noutside_index_root = os.getenv(\"outside_index_root\")\nnames = []\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\ndef lookup_indices(index_root):\n    global index_paths\n    for root, dirs, files in os.walk(index_root, topdown=False):",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "outside_index_root",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "outside_index_root = os.getenv(\"outside_index_root\")\nnames = []\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\ndef lookup_indices(index_root):\n    global index_paths\n    for root, dirs, files in os.walk(index_root, topdown=False):\n        for name in files:",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "names",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "names = []\nfor name in os.listdir(weight_root):\n    if name.endswith(\".pth\"):\n        names.append(name)\nindex_paths = []\ndef lookup_indices(index_root):\n    global index_paths\n    for root, dirs, files in os.walk(index_root, topdown=False):\n        for name in files:\n            if name.endswith(\".index\") and \"trained\" not in name:",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "index_paths",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "index_paths = []\ndef lookup_indices(index_root):\n    global index_paths\n    for root, dirs, files in os.walk(index_root, topdown=False):\n        for name in files:\n            if name.endswith(\".index\") and \"trained\" not in name:\n                index_paths.append(\"%s/%s\" % (root, name))\nlookup_indices(index_root)\nlookup_indices(outside_index_root)\nuvr5_names = []",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "uvr5_names",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "uvr5_names = []\nfor name in os.listdir(weight_uvr5_root):\n    if name.endswith(\".pth\") or \"onnx\" in name:\n        uvr5_names.append(name.replace(\".pth\", \"\"))\ndef change_choices():\n    names = []\n    for name in os.listdir(weight_root):\n        if name.endswith(\".pth\"):\n            names.append(name)\n    index_paths = []",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "sr_dict",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "sr_dict = {\n    \"32k\": 32000,\n    \"40k\": 40000,\n    \"48k\": 48000,\n}\ndef if_done(done, p):\n    while 1:\n        if p.poll() is None:\n            sleep(0.5)\n        else:",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "F0GPUVisible",
        "kind": 5,
        "importPath": "Models.Sound.infer-web",
        "description": "Models.Sound.infer-web",
        "peekOfCode": "F0GPUVisible = config.dml == False\ndef change_f0_method(f0method8):\n    if f0method8 == \"rmvpe_gpu\":\n        visible = F0GPUVisible\n    else:\n        visible = False\n    return {\"visible\": visible, \"__type__\": \"update\"}\nwith gr.Blocks(title=\"RVC WebUI\") as app:\n    gr.Markdown(\"## RVC WebUI\")\n    gr.Markdown(",
        "detail": "Models.Sound.infer-web",
        "documentation": {}
    },
    {
        "label": "FaceAligner",
        "kind": 6,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "class FaceAligner:\n    def __init__(self):\n        self.FACIAL_LANDMARKS_68_IDXS = {\n            \"mouth\": (48, 68),\n            \"inner_mouth\": (60, 68),\n            \"right_eyebrow\": (17, 22),\n            \"left_eyebrow\": (22, 27),\n            \"right_eye\": (36, 42),\n            \"left_eye\": (42, 48),\n            \"nose\": (27, 36),",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "PerformanceMonitor",
        "kind": 6,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "class PerformanceMonitor:\n    def __init__(self):\n        self.timers = {}\n        self.stats = deque(maxlen=100)  \n    def start_timer(self, name: str):\n        self.timers[name] = time.time()\n    def end_timer(self, name: str) -> float:\n        if name in self.timers:\n            duration = time.time() - self.timers[name]\n            self.stats.append(duration)",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "PresetManager",
        "kind": 6,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "class PresetManager:\n    def __init__(self, preset_dir: str = \"presets\"):\n        self.preset_dir = preset_dir\n        os.makedirs(preset_dir, exist_ok=True)\n        self.default_presets = {\n            \"Natural\": {\n                'brightness': 105, 'contrast': 110, 'saturation': 105,\n                'gamma': 95, 'hue_shift': 0, 'vibrance': 10,\n                'temperature': 5, 'sharpen': 20\n            },",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "AdvancedOpenCVControls",
        "kind": 6,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "class AdvancedOpenCVControls(QWidget):\n    adjustment_changed = Signal(dict)\n    def __init__(self):\n        super().__init__()\n        self.preset_manager = PresetManager()\n        self.setup_ui()\n        self.connect_signals()\n    def setup_ui(self):\n        layout = QVBoxLayout(self)\n        self.enabled_cb = QCheckBox(\"Enable OpenCV Adjustments\")",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "EnhancedFaceDetector",
        "kind": 6,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "class EnhancedFaceDetector:\n    def __init__(self):\n        self.detectors = {}\n        self.current_detector = \"opencv\"\n        self.init_detectors()\n    def init_detectors(self):\n        try:\n            self.detectors[\"opencv\"] = cv2.CascadeClassifier(\n                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n            )",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "AdvancedImageProcessor",
        "kind": 6,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "class AdvancedImageProcessor:\n    @staticmethod\n    def adjust_gamma(image: np.ndarray, gamma: float) -> np.ndarray:\n        inv_gamma = 1.0 / gamma\n        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n        return cv2.LUT(image, table)\n    @staticmethod\n    def adjust_temperature(image: np.ndarray, temp: float) -> np.ndarray:\n        result = image.astype(np.float32)\n        if temp > 0:  ",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "FaceadjustmentsDialog",
        "kind": 6,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "class FaceadjustmentsDialog(QDialog):\n    image_processed = Signal(np.ndarray)  \n    processing_stats = Signal(dict)  \n    def __init__(self, parent=None, initial_image: np.ndarray = None, target_face_embedding: np.ndarray = None):\n        super().__init__(parent)\n        self.original_image = initial_image\n        self.processed_image = None\n        self.current_faces = []\n        self.target_face_embedding = target_face_embedding\n        self.selected_face_index = -1",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "create_smooth_mask",
        "kind": 2,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "def create_smooth_mask(shape: Tuple[int, ...], center: Tuple[float, float],\n                       size: Tuple[float, float], feather: int = 20) -> np.ndarray:\n    try:\n        h, w = shape[:2]\n        mask = np.zeros((h, w), dtype=np.float32)\n        center_x, center_y = center\n        radius_x, radius_y = size[0] / 2, size[1] / 2\n        y, x = np.ogrid[:h, :w]\n        distance = np.sqrt(((x - center_x) / radius_x) ** 2 + ((y - center_y) / radius_y) ** 2)\n        mask = np.clip(1.0 - distance, 0.0, 1.0)",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "apply_enhanced_opencv_adjustments",
        "kind": 2,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "def apply_enhanced_opencv_adjustments(image: np.ndarray, adjustments: Dict[str, Any]) -> np.ndarray:\n    try:\n        result = image.copy()\n        processor = AdvancedImageProcessor()\n        brightness = adjustments.get('brightness', 100) / 100.0\n        contrast = adjustments.get('contrast', 100) / 100.0\n        saturation = adjustments.get('saturation', 100) / 100.0\n        gamma = adjustments.get('gamma', 100) / 100.0\n        hue_shift = adjustments.get('hue_shift', 0)\n        vibrance = adjustments.get('vibrance', 0)",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "apply_opencv_to_aligned_face",
        "kind": 2,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "def apply_opencv_to_aligned_face(image: np.ndarray, face: Any,\n                                 adjustments: Dict[str, Any], aligner: FaceAligner) -> np.ndarray:\n    try:\n        if hasattr(face, 'bbox'):\n            bbox = face.bbox\n        elif hasattr(face, 'box'):\n            bbox = face.box\n        else:\n            return image\n        x1, y1, x2, y2 = map(int, bbox)",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "test_face_alignment_dialog",
        "kind": 2,
        "importPath": "ui.adjustments",
        "description": "ui.adjustments",
        "peekOfCode": "def test_face_alignment_dialog():\n    import sys\n    app = QApplication(sys.argv)\n    dialog = FaceadjustmentsDialog()\n    result = dialog.exec()\n    def on_image_processed(image):\n        print(f\"Image processed! Shape: {image.shape}\")\n    def on_processing_stats(stats):\n        print(f\"Processing stats: {stats}\")\n    dialog.image_processed.connect(on_image_processed)",
        "detail": "ui.adjustments",
        "documentation": {}
    },
    {
        "label": "AgingWorker",
        "kind": 6,
        "importPath": "ui.aging",
        "description": "ui.aging",
        "peekOfCode": "class AgingWorker(QThread):\n    finished = Signal(object)\n    error = Signal(str)\n    progress = Signal(str)\n    def __init__(self, settings):\n        super().__init__()\n        self.settings = settings\n    def run(self):\n        try:\n            face_crop = self.settings['face_crop']",
        "detail": "ui.aging",
        "documentation": {}
    },
    {
        "label": "FaceAgingWindow",
        "kind": 6,
        "importPath": "ui.aging",
        "description": "ui.aging",
        "peekOfCode": "class FaceAgingWindow(QDialog): \n    log_message = Signal(str)\n    settings_accepted = Signal(dict) \n    def __init__(self, processor, initial_frame, stylegan_session, parent=None):\n        super().__init__(parent) \n        self.setWindowTitle(\"Advanced Face Aging\")\n        self.setMinimumSize(1200, 800)\n        self.processor = processor\n        self.original_frame = initial_frame.copy()\n        self.stylegan_session = stylegan_session ",
        "detail": "ui.aging",
        "documentation": {}
    },
    {
        "label": "find_model_path",
        "kind": 2,
        "importPath": "ui.aging",
        "description": "ui.aging",
        "peekOfCode": "def find_model_path(model_name: str) -> Path | None:\n    search_paths = [\n        Path(__file__).parent / \"Models\" / model_name,\n        Path.cwd() / \"Models\" / model_name,\n        Path(__file__).parent.parent / \"Models\" / model_name,\n        Path(__file__).parent / \"models\" / model_name,\n        Path.cwd() / \"models\" / model_name,\n        Path(__file__).parent.parent / \"models\" / model_name,\n    ]\n    for path in search_paths:",
        "detail": "ui.aging",
        "documentation": {}
    },
    {
        "label": "detect_faces_opencv",
        "kind": 2,
        "importPath": "ui.aging",
        "description": "ui.aging",
        "peekOfCode": "def detect_faces_opencv(detector, frame):\n    try:\n        faces = detector.detect_faces(frame)\n        return faces if faces else []\n    except Exception as e:\n        print(f\"[ERROR] OpenCV face detection failed: {e}\")\n        return []\nclass AgingWorker(QThread):\n    finished = Signal(object)\n    error = Signal(str)",
        "detail": "ui.aging",
        "documentation": {}
    },
    {
        "label": "apply_aging_to_video",
        "kind": 2,
        "importPath": "ui.aging",
        "description": "ui.aging",
        "peekOfCode": "def apply_aging_to_video(processor, aging_session, input_video_path, output_video_path, settings, progress_callback=None):\n    try:\n        if progress_callback:\n            progress_callback(\"Starting video aging process...\")\n        cap = cv2.VideoCapture(str(input_video_path))\n        if not cap.isOpened():\n            raise ValueError(f\"Cannot open video: {input_video_path}\")\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))",
        "detail": "ui.aging",
        "documentation": {}
    },
    {
        "label": "DEBUG_MODE",
        "kind": 5,
        "importPath": "ui.aging",
        "description": "ui.aging",
        "peekOfCode": "DEBUG_MODE = True\ndef find_model_path(model_name: str) -> Path | None:\n    search_paths = [\n        Path(__file__).parent / \"Models\" / model_name,\n        Path.cwd() / \"Models\" / model_name,\n        Path(__file__).parent.parent / \"Models\" / model_name,\n        Path(__file__).parent / \"models\" / model_name,\n        Path.cwd() / \"models\" / model_name,\n        Path(__file__).parent.parent / \"models\" / model_name,\n    ]",
        "detail": "ui.aging",
        "documentation": {}
    },
    {
        "label": "FullVideoProcessor",
        "kind": 6,
        "importPath": "ui.auto",
        "description": "ui.auto",
        "peekOfCode": "class FullVideoProcessor(QThread):\n    progress_updated = Signal(int, int, str)\n    processing_completed = Signal(dict)\n    error_occurred = Signal(str)\n    def __init__(self, video_path: str, detector, predictor, selected_face_embedding: Optional[np.ndarray] = None):\n        super().__init__()\n        self.video_path = video_path\n        self.detector = detector\n        self.predictor = predictor\n        self.selected_face_embedding = selected_face_embedding",
        "detail": "ui.auto",
        "documentation": {}
    },
    {
        "label": "AutoAlignmentDialog",
        "kind": 6,
        "importPath": "ui.auto",
        "description": "ui.auto",
        "peekOfCode": "class AutoAlignmentDialog(QDialog):\n    def __init__(self, video_path: str, selected_face_embedding: Optional[np.ndarray] = None, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Automatic Landmark Alignment\")\n        self.setGeometry(150, 150, 800, 650)\n        self.setModal(True)\n        self.video_path = video_path\n        self.frames = []\n        self.preview_frame = None\n        self.preview_landmarks = None",
        "detail": "ui.auto",
        "documentation": {}
    },
    {
        "label": "EnhancementWorker",
        "kind": 6,
        "importPath": "ui.enhancement",
        "description": "ui.enhancement",
        "peekOfCode": "class EnhancementWorker(QThread):\n    finished = Signal(np.ndarray)  \n    error = Signal(str)\n    progress = Signal(int)\n    def __init__(self, image, enhancer, settings):\n        super().__init__()\n        self.image = image\n        self.enhancer = enhancer\n        self.settings = settings\n    def run(self):",
        "detail": "ui.enhancement",
        "documentation": {}
    },
    {
        "label": "FaceEnhancementDialog",
        "kind": 6,
        "importPath": "ui.enhancement",
        "description": "ui.enhancement",
        "peekOfCode": "class FaceEnhancementDialog(QDialog):\n    def __init__(self, processor=None, initial_frame=None, parent=None):\n        super().__init__(parent)\n        self.processor = processor\n        self.original_frame = initial_frame\n        self.enhanced_frame = None\n        self.enhancer = None\n        self.enhancement_worker = None\n        self.settings = {}\n        self.setWindowTitle(\"Face Enhancement\")",
        "detail": "ui.enhancement",
        "documentation": {}
    },
    {
        "label": "open_enhancement_dialog",
        "kind": 2,
        "importPath": "ui.enhancement",
        "description": "ui.enhancement",
        "peekOfCode": "def open_enhancement_dialog(self):\n    if not self.cached_swapped_video and not self.target_video_path:\n        QMessageBox.warning(self, \"Warning\", \"Please load a video first!\")\n        return\n    if not self.processor:\n        QMessageBox.critical(self, \"Error\", \"Face processor is not available.\")\n        return\n    try:\n        current_frame = self.get_current_preview_frame()\n        if current_frame is None:",
        "detail": "ui.enhancement",
        "documentation": {}
    },
    {
        "label": "apply_enhancement_to_video",
        "kind": 2,
        "importPath": "ui.enhancement",
        "description": "ui.enhancement",
        "peekOfCode": "def apply_enhancement_to_video(self, input_path, output_path, settings):\n    if not GFPGAN_AVAILABLE:\n        self.update_progress(\"❌ Error: GFPGAN library not found.\")\n        return\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        self.update_progress(f\"❌ Error: Could not open video file {input_path}\")\n        return\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))",
        "detail": "ui.enhancement",
        "documentation": {}
    },
    {
        "label": "VideoPreviewWidget",
        "kind": 6,
        "importPath": "ui.handler",
        "description": "ui.handler",
        "peekOfCode": "class VideoPreviewWidget(QWidget):\n    \"\"\"Enhanced video preview with controls\"\"\"\n    position_changed = Signal(int)\n    duration_changed = Signal(int)\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setup_ui()\n        self.setup_media_player()\n        self.is_playing = False\n        self.current_frame_cache = {'position': 0, 'frame': None}",
        "detail": "ui.handler",
        "documentation": {}
    },
    {
        "label": "DetectedFacesWidget",
        "kind": 6,
        "importPath": "ui.handler",
        "description": "ui.handler",
        "peekOfCode": "class DetectedFacesWidget(QWidget):\n    face_clicked = Signal(int)  \n    from PySide6.QtWidgets import QScrollArea, QHBoxLayout, QSplitter\n    from PySide6.QtCore import Qt, Signal\n    from PySide6.QtGui import QPixmap, QImage\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setup_ui()\n        self.face_images = []\n    def setup_ui(self):",
        "detail": "ui.handler",
        "documentation": {}
    },
    {
        "label": "ClickableLabel",
        "kind": 6,
        "importPath": "ui.handler",
        "description": "ui.handler",
        "peekOfCode": "class ClickableLabel(QLabel):\n    \"\"\"Clickable QLabel that emits signal when clicked\"\"\"\n    clicked = Signal(int)\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.face_index = 0\n    def mousePressEvent(self, event):\n        if event.button() == Qt.MouseButton.LeftButton:\n            self.clicked.emit(self.face_index)\n        super().mousePressEvent(event)",
        "detail": "ui.handler",
        "documentation": {}
    },
    {
        "label": "CollapsibleSplitter",
        "kind": 6,
        "importPath": "ui.handler",
        "description": "ui.handler",
        "peekOfCode": "class CollapsibleSplitter(QSplitter):\n    \"\"\"Custom splitter with collapsible functionality\"\"\"\n    def __init__(self, orientation=Qt.Orientation.Horizontal, parent=None):\n        super().__init__(orientation, parent)\n        self.setChildrenCollapsible(True)\n        self.setHandleWidth(12)\n        self.left_collapsed = False\n        self.right_collapsed = False\n        self.saved_sizes = []\n    def addCollapsibleWidget(self, widget):",
        "detail": "ui.handler",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ui.handler",
        "description": "ui.handler",
        "peekOfCode": "def main():\n    from ui_logic import EnhancedMainWindow\n    \"\"\"Main application entry point with enhanced error handling\"\"\"\n    try:\n        app = QApplication(sys.argv)\n        app.setApplicationName(\"Face Swapping Studio\")\n        app.setApplicationVersion(\"2.0.0\")\n        app.setOrganizationName(\"AI Studio Pro\")\n        try:\n            if os.path.exists(\"assets/icon.png\"):",
        "detail": "ui.handler",
        "documentation": {}
    },
    {
        "label": "LandmarkTrackingThread",
        "kind": 6,
        "importPath": "ui.landmarksEditor",
        "description": "ui.landmarksEditor",
        "peekOfCode": "class LandmarkTrackingThread(QThread):\n    \"\"\"Tracks landmarks across all video frames.\"\"\"\n    progress_updated = Signal(int, int)\n    tracking_complete = Signal(dict)\n    def __init__(self, frames, detector, predictor, initial_landmarks):\n        super().__init__()\n        self.frames = frames\n        self.detector = detector\n        self.predictor = predictor\n        self.initial_landmarks = initial_landmarks",
        "detail": "ui.landmarksEditor",
        "documentation": {}
    },
    {
        "label": "LandmarkEditorApp",
        "kind": 6,
        "importPath": "ui.landmarksEditor",
        "description": "ui.landmarksEditor",
        "peekOfCode": "class LandmarkEditorApp(QDialog):\n    \"\"\"A QDialog for editing and tracking facial landmarks on a video.\"\"\"\n    def __init__(self, video_path, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Manual Landmark Editor & Tracker\")\n        self.setGeometry(100, 100, 1200, 800)\n        self.setModal(True)\n        self.video_path = video_path\n        self.frames = []\n        self.current_frame_index = 0",
        "detail": "ui.landmarksEditor",
        "documentation": {}
    },
    {
        "label": "Ui_MainWindow",
        "kind": 6,
        "importPath": "ui.main_window",
        "description": "ui.main_window",
        "peekOfCode": "class Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        if not MainWindow.objectName():\n            MainWindow.setObjectName(u\"MainWindow\")\n        MainWindow.resize(1400, 900)\n        MainWindow.setMinimumSize(QSize(1400, 900))\n        MainWindow.setStyleSheet(u\"\"\"\"\"\")\n        self.actionsave = QAction(MainWindow)\n        self.actionsave.setObjectName(u\"actionsave\")\n        self.actionimport_image = QAction(MainWindow)",
        "detail": "ui.main_window",
        "documentation": {}
    },
    {
        "label": "MaskEditorWindow",
        "kind": 6,
        "importPath": "ui.mask_app",
        "description": "ui.mask_app",
        "peekOfCode": "class MaskEditorWindow(QDialog):\n    tracking_complete = Signal()\n    def __init__(self, parent, video_path, controller):\n        super().__init__(parent)\n        self.parent = parent\n        self.video_path = video_path\n        self.controller = controller\n        self.setWindowTitle(\"Manual Mask Editor\")\n        self.resize(1000, 700)\n        self.setStyleSheet(\"background-color: #212529;\")",
        "detail": "ui.mask_app",
        "documentation": {}
    },
    {
        "label": "TrackingThread",
        "kind": 6,
        "importPath": "ui.mask_app",
        "description": "ui.mask_app",
        "peekOfCode": "class TrackingThread(QThread):\n    progress_updated = Signal(int, object)  \n    tracking_complete = Signal()\n    def __init__(self, frames, detector, progress_bar):\n        super().__init__()\n        self.frames = frames\n        self.detector = detector\n        self.progress_bar = progress_bar\n        self.cancel_flag = False\n    def run(self):",
        "detail": "ui.mask_app",
        "documentation": {}
    },
    {
        "label": "ProgressDialog",
        "kind": 6,
        "importPath": "ui.sound_handler",
        "description": "ui.sound_handler",
        "peekOfCode": "class ProgressDialog(QDialog):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Voice Conversion Progress\")\n        self.setModal(True)\n        self.resize(600, 400)\n        self.setup_ui()\n        self.is_closed = False\n    def setup_ui(self):\n        layout = QVBoxLayout(self)",
        "detail": "ui.sound_handler",
        "documentation": {}
    },
    {
        "label": "ConversionWorker",
        "kind": 6,
        "importPath": "ui.sound_handler",
        "description": "ui.sound_handler",
        "peekOfCode": "class ConversionWorker(QObject):\n    progress = Signal(str)\n    finished = Signal(dict)\n    def __init__(self, converter: RVCConverter, params: Dict[str, Any]):\n        super().__init__()\n        self.converter = converter\n        self.params = params\n        self.is_canceled = False\n    def run(self):\n        try:",
        "detail": "ui.sound_handler",
        "documentation": {}
    },
    {
        "label": "VideoMergeWorker",
        "kind": 6,
        "importPath": "ui.sound_handler",
        "description": "ui.sound_handler",
        "peekOfCode": "class VideoMergeWorker(QObject):\n    progress = Signal(str)\n    finished = Signal(dict)\n    def __init__(self, converter: RVCConverter, params: Dict[str, Any]):\n        super().__init__()\n        self.converter = converter\n        self.params = params\n        self.is_canceled = False\n    def run(self):\n        try:",
        "detail": "ui.sound_handler",
        "documentation": {}
    },
    {
        "label": "AudioPlayerWidget",
        "kind": 6,
        "importPath": "ui.sound_handler",
        "description": "ui.sound_handler",
        "peekOfCode": "class AudioPlayerWidget(QWidget):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.current_audio_path = None\n        self.setup_ui()\n    def setup_ui(self):\n        layout = QVBoxLayout(self)\n        layout.setContentsMargins(0, 0, 0, 0)\n        controls_layout = QHBoxLayout()\n        self.play_button = QPushButton(\"▶️ Play\")",
        "detail": "ui.sound_handler",
        "documentation": {}
    },
    {
        "label": "RVCConverterGUI",
        "kind": 6,
        "importPath": "ui.sound_handler",
        "description": "ui.sound_handler",
        "peekOfCode": "class RVCConverterGUI(QDialog):\n    conversion_successful = Signal(str)\n    def __init__(self, parent=None, target_video_path: Optional[str] = None, stylesheet: str = \"\"):\n        super().__init__(parent)\n        self.converter = None\n        self.conversion_thread = None\n        self.merge_thread = None\n        self.conversion_worker = None\n        self.merge_worker = None\n        self.progress_dialog = None",
        "detail": "ui.sound_handler",
        "documentation": {}
    },
    {
        "label": "CollapsibleTabBar",
        "kind": 6,
        "importPath": "ui.Splash_Screen",
        "description": "ui.Splash_Screen",
        "peekOfCode": "class CollapsibleTabBar(QWidget):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setFixedWidth(50)\n        self.setStyleSheet(\"\"\"\n            background-color: #22262b;\n            border-right: 1px solid #444;\n        \"\"\")\n        layout = QVBoxLayout()\n        layout.setContentsMargins(0, 10, 0, 10)",
        "detail": "ui.Splash_Screen",
        "documentation": {}
    },
    {
        "label": "FaceSwappingApp",
        "kind": 6,
        "importPath": "ui.Splash_Screen",
        "description": "ui.Splash_Screen",
        "peekOfCode": "class FaceSwappingApp(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setCentralWidget(self.main_window)\n        self.setWindowTitle(\"Face Swapping App\")\n        self.setGeometry(100, 100, 1200, 800)\n        self.theme_combo = self.findChild(QComboBox, \"combo_theme\")\n        if self.theme_combo:\n            self.theme_combo.currentTextChanged.connect(self.change_theme)\n        self.change_theme(self.theme_combo.currentText() if self.theme_combo else \"Dark\")",
        "detail": "ui.Splash_Screen",
        "documentation": {}
    },
    {
        "label": "GifSplashScreen",
        "kind": 6,
        "importPath": "ui.Splash_Screen",
        "description": "ui.Splash_Screen",
        "peekOfCode": "class GifSplashScreen(QWidget):\n    def __init__(self, gif_path, duration=15000, parent=None):\n        super().__init__(parent)\n        self.setWindowFlags(Qt.WindowType.SplashScreen | Qt.WindowType.FramelessWindowHint)\n        self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground)\n        self.setFixedSize(600, 400)\n        self.setStyleSheet(\"background: transparent;\")\n        self.label = QLabel(self)\n        self.label.setGeometry(0, 0, 600, 400)\n        self.label.setAlignment(Qt.AlignmentFlag.AlignCenter)",
        "detail": "ui.Splash_Screen",
        "documentation": {}
    },
    {
        "label": "update_gradient",
        "kind": 2,
        "importPath": "ui.themes",
        "description": "ui.themes",
        "peekOfCode": "def update_gradient():\n    global pos, current_colors\n    pos += 0.01\n    if pos > 1.0:\n        pos = 0.0\ndef apply_theme(self, theme: str):\n        \"\"\"Apply theme to application\"\"\"\n        self.current_theme = theme\n        if theme == \"dark\":\n            stylesheet = \"\"\"",
        "detail": "ui.themes",
        "documentation": {}
    },
    {
        "label": "apply_theme",
        "kind": 2,
        "importPath": "ui.themes",
        "description": "ui.themes",
        "peekOfCode": "def apply_theme(self, theme: str):\n        \"\"\"Apply theme to application\"\"\"\n        self.current_theme = theme\n        if theme == \"dark\":\n            stylesheet = \"\"\"\n            {\n    selection-background-color: #00c896;\n    selection-color: #ffffff;\n    alternate-background-color: #2a2930;\n}",
        "detail": "ui.themes",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "ui.themes",
        "description": "ui.themes",
        "peekOfCode": "colors = [\n    [\"#3498db\", \"#518142\", \"#ff0000\"],\n    [\"#04b97f\", \"#007b50\", \"#37efba\"]\n]\ncurrent_colors = random.choice(colors)\npos = 0.0\ndef update_gradient():\n    global pos, current_colors\n    pos += 0.01\n    if pos > 1.0:",
        "detail": "ui.themes",
        "documentation": {}
    },
    {
        "label": "current_colors",
        "kind": 5,
        "importPath": "ui.themes",
        "description": "ui.themes",
        "peekOfCode": "current_colors = random.choice(colors)\npos = 0.0\ndef update_gradient():\n    global pos, current_colors\n    pos += 0.01\n    if pos > 1.0:\n        pos = 0.0\ndef apply_theme(self, theme: str):\n        \"\"\"Apply theme to application\"\"\"\n        self.current_theme = theme",
        "detail": "ui.themes",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "ui.themes",
        "description": "ui.themes",
        "peekOfCode": "pos = 0.0\ndef update_gradient():\n    global pos, current_colors\n    pos += 0.01\n    if pos > 1.0:\n        pos = 0.0\ndef apply_theme(self, theme: str):\n        \"\"\"Apply theme to application\"\"\"\n        self.current_theme = theme\n        if theme == \"dark\":",
        "detail": "ui.themes",
        "documentation": {}
    },
    {
        "label": "StreamHandlerUTF8",
        "kind": 6,
        "importPath": "ui.uitls",
        "description": "ui.uitls",
        "peekOfCode": "class StreamHandlerUTF8(logging.StreamHandler):\n    def __init__(self, stream=None):\n        super().__init__(stream or sys.stdout)\n        self.stream = open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('face_swap_app.log', encoding='utf-8'),\n        StreamHandlerUTF8()",
        "detail": "ui.uitls",
        "documentation": {}
    },
    {
        "label": "CacheManager",
        "kind": 6,
        "importPath": "ui.uitls",
        "description": "ui.uitls",
        "peekOfCode": "class CacheManager:\n    \"\"\"Manages caching of processed frames and resources\"\"\"\n    def __init__(self, cache_dir: Path = Path(\"cache\")):\n        self.cache_dir = cache_dir\n        self.cache_dir.mkdir(exist_ok=True)\n        self.cache_mutex = QMutex()\n        self._cache_data = {}\n        logger.info(f\"Cache manager initialized at {cache_dir}\")\n    def get(self, key: str) -> Optional[Any]:\n        with QMutexLocker(self.cache_mutex):",
        "detail": "ui.uitls",
        "documentation": {}
    },
    {
        "label": "ProcessingThread",
        "kind": 6,
        "importPath": "ui.uitls",
        "description": "ui.uitls",
        "peekOfCode": "class ProcessingThread(QThread):\n    \"\"\"Thread for face swapping processing\"\"\"\n    progress = Signal(str)\n    finished = Signal(bool)\n    frame_processed = Signal(np.ndarray)\n    def __init__(self, processor: FaceSwapProcessor):\n        super().__init__()\n        self.processor = processor\n        self.source_path = None\n        self.target_path = None",
        "detail": "ui.uitls",
        "documentation": {}
    },
    {
        "label": "SettingsDialog",
        "kind": 6,
        "importPath": "ui.uitls",
        "description": "ui.uitls",
        "peekOfCode": "class SettingsDialog(QDialog):\n    \"\"\"Settings dialog for theme and preferences\"\"\"\n    theme_changed = Signal(str)\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Settings\")\n        self.setModal(True)\n        self.resize(400, 300)\n        self.setup_ui()\n    def setup_ui(self):",
        "detail": "ui.uitls",
        "documentation": {}
    },
    {
        "label": "safe_paint",
        "kind": 2,
        "importPath": "ui.uitls",
        "description": "ui.uitls",
        "peekOfCode": "def safe_paint(pixmap, paint_function):\n    \"\"\"Safely use a QPainter with proper cleanup even if exceptions occur\n    Args:\n        pixmap: The QPixmap to paint on\n        paint_function: A function that takes a QPainter as argument and performs painting\n    Returns:\n        The pixmap after painting\n    \"\"\"\n    painter = QPainter(pixmap)\n    try:",
        "detail": "ui.uitls",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "ui.uitls",
        "description": "ui.uitls",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass CacheManager:\n    \"\"\"Manages caching of processed frames and resources\"\"\"\n    def __init__(self, cache_dir: Path = Path(\"cache\")):\n        self.cache_dir = cache_dir\n        self.cache_dir.mkdir(exist_ok=True)\n        self.cache_mutex = QMutex()\n        self._cache_data = {}\n        logger.info(f\"Cache manager initialized at {cache_dir}\")\n    def get(self, key: str) -> Optional[Any]:",
        "detail": "ui.uitls",
        "documentation": {}
    },
    {
        "label": "EnhancedMainWindow",
        "kind": 6,
        "importPath": "ui.ui_logic",
        "description": "ui.ui_logic",
        "peekOfCode": "class EnhancedMainWindow(QMainWindow):\n    def __init__(self):\n        super().__init__(parent=None)\n        if Ui_MainWindow is None:\n            QMessageBox.critical(self, \"Initialization Error\",\n                                \"UI module (main_window.py) not found. The application cannot start.\")\n            sys.exit(1)\n        self.ui = Ui_MainWindow()\n        self.ui.setupUi(self)\n        self._initialize_attributes()",
        "detail": "ui.ui_logic",
        "documentation": {}
    },
    {
        "label": "RVCConverter",
        "kind": 6,
        "importPath": "ui.voice_handler",
        "description": "ui.voice_handler",
        "peekOfCode": "class RVCConverter:\n    \"\"\"\n    Unified RVC Voice Converter with workflow integration\n    \"\"\"\n    def __init__(self, rvc_root: str = None, assets_path: str = None, temp_base_dir: str = \"temp\"):\n        if rvc_root is None:\n            possible_paths = [\n                Path.cwd() / \"Models\" / \"Sound\",\n                Path.cwd().parent / \"Models\" / \"Sound\",\n                Path(__file__).parent / \"Models\" / \"Sound\",",
        "detail": "ui.voice_handler",
        "documentation": {}
    },
    {
        "label": "VoiceConversionWorkflow",
        "kind": 6,
        "importPath": "ui.voice_handler",
        "description": "ui.voice_handler",
        "peekOfCode": "class VoiceConversionWorkflow:\n    def __init__(self, rvc_root: str = None, assets_path: str = None):\n        self.converter = RVCConverter(rvc_root=rvc_root, assets_path=assets_path)\n    def convert_voice_with_progress(self, input_file: str, model_name: str, output_file: str = None, progress_callback=None, **conversion_params):\n        def progress_handler(message):\n            if progress_callback:\n                progress_callback(message)\n        conversion_params.pop('progress_callback', None)\n        return self.converter.convert_voice(\n            input_path=input_file,",
        "detail": "ui.voice_handler",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ui.voice_handler",
        "description": "ui.voice_handler",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description='RVC Voice Conversion')\n    parser.add_argument('--input_path', required=True, help='Input audio file')\n    parser.add_argument('--output_path', required=True, help='Output audio file') \n    parser.add_argument('--model_name', required=True, help='Model name')\n    parser.add_argument('--f0up_key', type=int, default=0, help='Pitch adjustment')\n    parser.add_argument('--index_rate', type=float, default=0.75, help='Index rate')\n    parser.add_argument('--protect', type=float, default=0.33, help='Protect rate')\n    parser.add_argument('--pth_path', help='Path to model weights directory')\n    # Alternative argument names",
        "detail": "ui.voice_handler",
        "documentation": {}
    },
    {
        "label": "safe_import_enhanced_main_window",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def safe_import_enhanced_main_window():\n    \"\"\"Safely import EnhancedMainWindow with error handling\"\"\"\n    try:\n        from ui.ui_logic import EnhancedMainWindow\n        return EnhancedMainWindow\n    except ImportError as e:\n        print(f\"Import error: Failed to import EnhancedMainWindow from ui.ui_logic: {e}\")\n        try:\n            from core.requirements_checker import RequirementsChecker, Colors\n            print(f\"RequirementsChecker imported successfully\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "show_splash_and_main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def show_splash_and_main():\n    \"\"\"Show splash screen and then main window\"\"\"\n    global main_window\n    try:\n        from ui.Splash_Screen import GifSplashScreen\n        gif_path = Path(\"Assets/Gif/pop art loop GIF by Dax Norman.gif\")\n        if gif_path.exists():\n            print(\"Loading splash screen...\")\n            splash = GifSplashScreen(str(gif_path), duration=3000)\n            splash.show()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def main():\n    \"\"\"Main entry point for the application\"\"\"\n    global app, main_window\n    app = QApplication(sys.argv)\n    main_window = None\n    print(\"Face Swapping Application Starting...\")\n    try:\n        from core.requirements_checker import RequirementsChecker, Colors\n        print(\"Checking requirements...\")\n        checker = RequirementsChecker()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "exception_hook",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def exception_hook(exctype, value, traceback):\n    print(exctype, value, traceback)\n    if \"QBackingStore::endPaint()\" in str(value):\n        print(\"QPainter cleanup error detected. Attempting to force cleanup...\")\n        # if QApplication.instance():\n        #     QApplication.instance().processEvents()\n    sys.__excepthook__(exctype, value, traceback)\n    QCoreApplication.quit()\nsys.excepthook = exception_hook\nif __name__ == \"__main__\":",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "sys.excepthook",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "sys.excepthook = exception_hook\nif __name__ == \"__main__\":\n    try:\n        exit_code = main()\n        sys.exit(exit_code)\n    except KeyboardInterrupt:\n        sys.exit(0)\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        sys.exit(1)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "import_obj_sequence_as_blendshapes",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def import_obj_sequence_as_blendshapes():\n    folder_path = cmds.fileDialog2(\n        dialogStyle=2, \n        fileMode=3, \n        caption=\"Select OBJ Sequence Folder\"\n    )\n    if not folder_path:\n        cmds.warning(\"No folder selected. Operation cancelled.\")\n        return\n    folder_path = folder_path[0]",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "get_obj_files",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def get_obj_files(folder_path):\n    \"\"\"\n    Get all OBJ files from the folder and sort them numerically\n    \"\"\"\n    obj_files = []\n    for file in os.listdir(folder_path):\n        if file.lower().endswith('.obj'):\n            obj_files.append(file)\n    def extract_frame_number(filename):\n        match = re.search(r'(\\d+)', filename)",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "import_base_mesh",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def import_base_mesh(folder_path, base_file):\n    \"\"\"\n    Import the base mesh (first frame)\n    \"\"\"\n    base_path = os.path.join(folder_path, base_file).replace('\\\\', '/')\n    try:\n        cmds.select(clear=True)\n        imported_nodes = cmds.file(\n            base_path,\n            i=True,  ",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "import_target_meshes",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def import_target_meshes(folder_path, target_files):\n    \"\"\"\n    Import all target meshes for blend shapes\n    \"\"\"\n    target_meshes = []\n    for i, target_file in enumerate(target_files, 1):\n        target_path = os.path.join(folder_path, target_file).replace('\\\\', '/')\n        try:\n            cmds.select(clear=True)\n            imported_nodes = cmds.file(",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "create_blend_shape",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def create_blend_shape(base_mesh, target_meshes):\n    if not target_meshes:\n        cmds.warning(\"No target meshes to create blend shapes\")\n        return None\n    base_shape = cmds.listRelatives(base_mesh, shapes=True)[0]\n    base_vertex_count = cmds.polyEvaluate(base_shape, vertex=True)\n    print(f\"Base mesh '{base_mesh}' has {base_vertex_count} vertices\")\n    topology_info = []\n    for target_mesh in target_meshes:\n        try:",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "create_blend_shape_controls",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def create_blend_shape_controls(blend_shape_node, total_frames):\n    controls = {\n        'blend_shape_node': blend_shape_node,\n        'total_frames': total_frames\n    }\n    print(\"Blend shape controls setup completed - use timeline scrubbing\")\n    return controls\ndef create_sequence_expression(main_controller, blend_shape_node, total_frames):\n    pass\ndef create_blend_shape_animation(controls, total_frames, valid_targets):",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "create_sequence_expression",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def create_sequence_expression(main_controller, blend_shape_node, total_frames):\n    pass\ndef create_blend_shape_animation(controls, total_frames, valid_targets):\n    \"\"\"\n    Create keyframe animation where timeline frame = blend shape frame\n    Frame 0 = base mesh, Frame 1 = first target, Frame 2 = second target, etc.\n    \"\"\"\n    blend_shape_node = controls['blend_shape_node']\n    cmds.playbackOptions(minTime=0, maxTime=total_frames - 1)\n    for frame_index in range(total_frames):",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "create_blend_shape_animation",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def create_blend_shape_animation(controls, total_frames, valid_targets):\n    \"\"\"\n    Create keyframe animation where timeline frame = blend shape frame\n    Frame 0 = base mesh, Frame 1 = first target, Frame 2 = second target, etc.\n    \"\"\"\n    blend_shape_node = controls['blend_shape_node']\n    cmds.playbackOptions(minTime=0, maxTime=total_frames - 1)\n    for frame_index in range(total_frames):\n        current_time = float(frame_index)  # Ensure time is float\n        for target_index, target_mesh in enumerate(valid_targets):",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "cleanup_target_meshes",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def cleanup_target_meshes(target_meshes):\n    \"\"\"\n    Hide all target meshes to keep scene clean - only base mesh visible\n    \"\"\"\n    if not target_meshes:\n        print(\"No target meshes to clean up\")\n        return\n    existing_meshes = []\n    missing_meshes = []\n    for mesh in target_meshes:",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    },
    {
        "label": "create_ui",
        "kind": 2,
        "importPath": "Maya_Animation_tool_01",
        "description": "Maya_Animation_tool_01",
        "peekOfCode": "def create_ui():\n    \"\"\"\n    Create a simple UI for the blend shape tool\n    \"\"\"\n    window_name = \"objSequenceBlendShapeUI\"\n    if cmds.window(window_name, exists=True):\n        cmds.deleteUI(window_name)\n    window = cmds.window(\n        window_name,\n        title=\"OBJ Sequence Blend Shape Tool\",",
        "detail": "Maya_Animation_tool_01",
        "documentation": {}
    }
]